running build_ext
Using cuda device
Logging to assets/out/models/exp2/PPO_1
---------------------------------
| penalty            | 0        |
| reward             | 0.232    |
| reward_distance    | 0.218    |
| reward_energy      | 0.0148   |
| reward_velocity    | 1.83e-14 |
| rollout/           |          |
|    ep_len_mean     | 96.2     |
|    ep_rew_mean     | 124      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 1        |
|    time_elapsed    | 54       |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 1.02       |
| reward_distance         | 0.328      |
| reward_energy           | 0.263      |
| reward_velocity         | 0.432      |
| rollout/                |            |
|    ep_len_mean          | 105        |
|    ep_rew_mean          | 157        |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 2          |
|    time_elapsed         | 114        |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.16236478 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4         |
|    explained_variance   | -0.000266  |
|    learning_rate        | 0.0003     |
|    loss                 | 28.6       |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0633    |
|    std                  | 0.368      |
|    value_loss           | 99.3       |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.47        |
| reward_distance         | 0.336       |
| reward_energy           | 0.491       |
| reward_velocity         | 0.639       |
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 182         |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 3           |
|    time_elapsed         | 185         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.022231922 |
|    clip_fraction        | 0.00913     |
|    clip_range           | 0.4         |
|    entropy_loss         | -3.96       |
|    explained_variance   | -0.0169     |
|    learning_rate        | 0.0003      |
|    loss                 | 58.9        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0106     |
|    std                  | 0.368       |
|    value_loss           | 166         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.55        |
| reward_distance         | 0.331       |
| reward_energy           | 0.536       |
| reward_velocity         | 0.683       |
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 189         |
| time/                   |             |
|    fps                  | 31          |
|    iterations           | 4           |
|    time_elapsed         | 256         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.054523826 |
|    clip_fraction        | 0.0434      |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.12       |
|    explained_variance   | -0.0144     |
|    learning_rate        | 0.0003      |
|    loss                 | 80.9        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00993    |
|    std                  | 0.368       |
|    value_loss           | 198         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.63        |
| reward_distance         | 0.336       |
| reward_energy           | 0.584       |
| reward_velocity         | 0.715       |
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 197         |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 5           |
|    time_elapsed         | 331         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.045913845 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.3        |
|    explained_variance   | -0.00596    |
|    learning_rate        | 0.0003      |
|    loss                 | 91.2        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00903    |
|    std                  | 0.368       |
|    value_loss           | 210         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.71        |
| reward_distance         | 0.315       |
| reward_energy           | 0.636       |
| reward_velocity         | 0.758       |
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 204         |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 6           |
|    time_elapsed         | 405         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.032437008 |
|    clip_fraction        | 0.0303      |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.47       |
|    explained_variance   | -0.00283    |
|    learning_rate        | 0.0003      |
|    loss                 | 96.9        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0128     |
|    std                  | 0.368       |
|    value_loss           | 217         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.84        |
| reward_distance         | 0.301       |
| reward_energy           | 0.714       |
| reward_velocity         | 0.824       |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 226         |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 7           |
|    time_elapsed         | 476         |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.015318963 |
|    clip_fraction        | 0.0123      |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.61       |
|    explained_variance   | -0.00127    |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.00113     |
|    std                  | 0.368       |
|    value_loss           | 235         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.94        |
| reward_distance         | 0.306       |
| reward_energy           | 0.771       |
| reward_velocity         | 0.867       |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 235         |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 8           |
|    time_elapsed         | 556         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.010494138 |
|    clip_fraction        | 0.00469     |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.69       |
|    explained_variance   | -0.00176    |
|    learning_rate        | 0.0003      |
|    loss                 | 118         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00541    |
|    std                  | 0.368       |
|    value_loss           | 259         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.92        |
| reward_distance         | 0.304       |
| reward_energy           | 0.757       |
| reward_velocity         | 0.859       |
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 237         |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 9           |
|    time_elapsed         | 636         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.012035344 |
|    clip_fraction        | 0.00645     |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.78       |
|    explained_variance   | -0.000481   |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00239    |
|    std                  | 0.368       |
|    value_loss           | 240         |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 2          |
| reward_distance         | 0.285      |
| reward_energy           | 0.813      |
| reward_velocity         | 0.899      |
| rollout/                |            |
|    ep_len_mean          | 122        |
|    ep_rew_mean          | 246        |
| time/                   |            |
|    fps                  | 28         |
|    iterations           | 10         |
|    time_elapsed         | 716        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.00806747 |
|    clip_fraction        | 0.00654    |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.86      |
|    explained_variance   | -0.000355  |
|    learning_rate        | 0.0003     |
|    loss                 | 107        |
|    n_updates            | 90         |
|    policy_gradient_loss | 0.000603   |
|    std                  | 0.368      |
|    value_loss           | 234        |
----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.97         |
| reward_distance         | 0.263        |
| reward_energy           | 0.808        |
| reward_velocity         | 0.9          |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 242          |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 11           |
|    time_elapsed         | 796          |
|    total_timesteps      | 22528        |
| train/                  |              |
|    approx_kl            | 0.0037681847 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.92        |
|    explained_variance   | -0.000223    |
|    learning_rate        | 0.0003       |
|    loss                 | 104          |
|    n_updates            | 100          |
|    policy_gradient_loss | 0.000801     |
|    std                  | 0.367        |
|    value_loss           | 227          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.98         |
| reward_distance         | 0.259        |
| reward_energy           | 0.817        |
| reward_velocity         | 0.907        |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 247          |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 12           |
|    time_elapsed         | 878          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0054088575 |
|    clip_fraction        | 0.00146      |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.96        |
|    explained_variance   | 0.000531     |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00092     |
|    std                  | 0.367        |
|    value_loss           | 226          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.97         |
| reward_distance         | 0.266        |
| reward_energy           | 0.804        |
| reward_velocity         | 0.897        |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 246          |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 13           |
|    time_elapsed         | 965          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0052448353 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.02        |
|    explained_variance   | 0.000474     |
|    learning_rate        | 0.0003       |
|    loss                 | 109          |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.000323     |
|    std                  | 0.367        |
|    value_loss           | 238          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.98         |
| reward_distance         | 0.247        |
| reward_energy           | 0.819        |
| reward_velocity         | 0.915        |
| rollout/                |              |
|    ep_len_mean          | 122          |
|    ep_rew_mean          | 252          |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 14           |
|    time_elapsed         | 1052         |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0049496433 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.08        |
|    explained_variance   | 0.000248     |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000362    |
|    std                  | 0.367        |
|    value_loss           | 242          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.99         |
| reward_distance         | 0.241        |
| reward_energy           | 0.827        |
| reward_velocity         | 0.925        |
| rollout/                |              |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | 257          |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 15           |
|    time_elapsed         | 1137         |
|    total_timesteps      | 30720        |
| train/                  |              |
|    approx_kl            | 0.0044026785 |
|    clip_fraction        | 0.00117      |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.13        |
|    explained_variance   | 0.00022      |
|    learning_rate        | 0.0003       |
|    loss                 | 116          |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00314     |
|    std                  | 0.367        |
|    value_loss           | 252          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.97         |
| reward_distance         | 0.255        |
| reward_energy           | 0.797        |
| reward_velocity         | 0.915        |
| rollout/                |              |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | 258          |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 16           |
|    time_elapsed         | 1224         |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0044405065 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.17        |
|    explained_variance   | 8.6e-05      |
|    learning_rate        | 0.0003       |
|    loss                 | 117          |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.000613    |
|    std                  | 0.367        |
|    value_loss           | 254          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2            |
| reward_distance         | 0.256        |
| reward_energy           | 0.813        |
| reward_velocity         | 0.935        |
| rollout/                |              |
|    ep_len_mean          | 127          |
|    ep_rew_mean          | 268          |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 17           |
|    time_elapsed         | 1312         |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0043142634 |
|    clip_fraction        | 0.00137      |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.21        |
|    explained_variance   | -6.56e-05    |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 160          |
|    policy_gradient_loss | 0.000477     |
|    std                  | 0.367        |
|    value_loss           | 223          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.99        |
| reward_distance         | 0.264       |
| reward_energy           | 0.808       |
| reward_velocity         | 0.922       |
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 261         |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 18          |
|    time_elapsed         | 1400        |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.009156759 |
|    clip_fraction        | 0.0043      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.26       |
|    explained_variance   | -4.45e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00595    |
|    std                  | 0.367       |
|    value_loss           | 232         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.98         |
| reward_distance         | 0.267        |
| reward_energy           | 0.8          |
| reward_velocity         | 0.908        |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 257          |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 19           |
|    time_elapsed         | 1494         |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0034919768 |
|    clip_fraction        | 0.000977     |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.32        |
|    explained_variance   | 8.76e-05     |
|    learning_rate        | 0.0003       |
|    loss                 | 104          |
|    n_updates            | 180          |
|    policy_gradient_loss | 0.00262      |
|    std                  | 0.367        |
|    value_loss           | 226          |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 1.98          |
| reward_distance         | 0.25          |
| reward_energy           | 0.822         |
| reward_velocity         | 0.908         |
| rollout/                |               |
|    ep_len_mean          | 119           |
|    ep_rew_mean          | 253           |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 20            |
|    time_elapsed         | 1585          |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00077221135 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.34         |
|    explained_variance   | 8.44e-05      |
|    learning_rate        | 0.0003        |
|    loss                 | 105           |
|    n_updates            | 190           |
|    policy_gradient_loss | -0.000349     |
|    std                  | 0.367         |
|    value_loss           | 228           |
-------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.98         |
| reward_distance         | 0.248        |
| reward_energy           | 0.824        |
| reward_velocity         | 0.907        |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 251          |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 21           |
|    time_elapsed         | 1676         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0016009794 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.35        |
|    explained_variance   | 0.000152     |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.367        |
|    value_loss           | 223          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2            |
| reward_distance         | 0.227        |
| reward_energy           | 0.853        |
| reward_velocity         | 0.917        |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 251          |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 22           |
|    time_elapsed         | 1764         |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0028857724 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.37        |
|    explained_variance   | 0.000139     |
|    learning_rate        | 0.0003       |
|    loss                 | 98.7         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000587    |
|    std                  | 0.367        |
|    value_loss           | 214          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.02         |
| reward_distance         | 0.236        |
| reward_energy           | 0.868        |
| reward_velocity         | 0.917        |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 252          |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 23           |
|    time_elapsed         | 1854         |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0045680446 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.4         |
|    explained_variance   | -2.25e-05    |
|    learning_rate        | 0.0003       |
|    loss                 | 105          |
|    n_updates            | 220          |
|    policy_gradient_loss | -0.0032      |
|    std                  | 0.367        |
|    value_loss           | 228          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.07        |
| reward_distance         | 0.24        |
| reward_energy           | 0.888       |
| reward_velocity         | 0.94        |
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | 258         |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 24          |
|    time_elapsed         | 1939        |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.004662297 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.41       |
|    explained_variance   | -4.41e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 110         |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.000801   |
|    std                  | 0.367       |
|    value_loss           | 237         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.09         |
| reward_distance         | 0.235        |
| reward_energy           | 0.905        |
| reward_velocity         | 0.954        |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | 268          |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 25           |
|    time_elapsed         | 2025         |
|    total_timesteps      | 51200        |
| train/                  |              |
|    approx_kl            | 8.398289e-06 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.41        |
|    explained_variance   | -9.89e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 108          |
|    n_updates            | 240          |
|    policy_gradient_loss | 0.000452     |
|    std                  | 0.367        |
|    value_loss           | 232          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.13         |
| reward_distance         | 0.248        |
| reward_energy           | 0.916        |
| reward_velocity         | 0.964        |
| rollout/                |              |
|    ep_len_mean          | 127          |
|    ep_rew_mean          | 271          |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 26           |
|    time_elapsed         | 2114         |
|    total_timesteps      | 53248        |
| train/                  |              |
|    approx_kl            | 0.0036053774 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.43        |
|    explained_variance   | -1.79e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 250          |
|    policy_gradient_loss | -0.00255     |
|    std                  | 0.367        |
|    value_loss           | 240          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.16         |
| reward_distance         | 0.247        |
| reward_energy           | 0.931        |
| reward_velocity         | 0.984        |
| rollout/                |              |
|    ep_len_mean          | 127          |
|    ep_rew_mean          | 272          |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 27           |
|    time_elapsed         | 2203         |
|    total_timesteps      | 55296        |
| train/                  |              |
|    approx_kl            | 0.0034591157 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.47        |
|    explained_variance   | -2.74e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 102          |
|    n_updates            | 260          |
|    policy_gradient_loss | -0.000816    |
|    std                  | 0.367        |
|    value_loss           | 220          |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.16          |
| reward_distance         | 0.264         |
| reward_energy           | 0.923         |
| reward_velocity         | 0.974         |
| rollout/                |               |
|    ep_len_mean          | 125           |
|    ep_rew_mean          | 270           |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 28            |
|    time_elapsed         | 2294          |
|    total_timesteps      | 57344         |
| train/                  |               |
|    approx_kl            | 0.00027778262 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.49         |
|    explained_variance   | -3.7e-06      |
|    learning_rate        | 0.0003        |
|    loss                 | 107           |
|    n_updates            | 270           |
|    policy_gradient_loss | 0.000158      |
|    std                  | 0.367         |
|    value_loss           | 231           |
-------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.11          |
| reward_distance         | 0.254         |
| reward_energy           | 0.905         |
| reward_velocity         | 0.954         |
| rollout/                |               |
|    ep_len_mean          | 123           |
|    ep_rew_mean          | 264           |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 29            |
|    time_elapsed         | 2382          |
|    total_timesteps      | 59392         |
| train/                  |               |
|    approx_kl            | 0.00020506498 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.49         |
|    explained_variance   | 7.03e-06      |
|    learning_rate        | 0.0003        |
|    loss                 | 102           |
|    n_updates            | 280           |
|    policy_gradient_loss | -0.000261     |
|    std                  | 0.367         |
|    value_loss           | 218           |
-------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.1          |
| reward_distance         | 0.239        |
| reward_energy           | 0.903        |
| reward_velocity         | 0.954        |
| rollout/                |              |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | 264          |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 30           |
|    time_elapsed         | 2472         |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0034478186 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 1.91e-05     |
|    learning_rate        | 0.0003       |
|    loss                 | 104          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00222     |
|    std                  | 0.367        |
|    value_loss           | 224          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.06         |
| reward_distance         | 0.23         |
| reward_energy           | 0.889        |
| reward_velocity         | 0.94         |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 257          |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 31           |
|    time_elapsed         | 2561         |
|    total_timesteps      | 63488        |
| train/                  |              |
|    approx_kl            | 0.0004367486 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.54        |
|    explained_variance   | 2.98e-07     |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 300          |
|    policy_gradient_loss | 0.000432     |
|    std                  | 0.367        |
|    value_loss           | 220          |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.03          |
| reward_distance         | 0.214         |
| reward_energy           | 0.88          |
| reward_velocity         | 0.933         |
| rollout/                |               |
|    ep_len_mean          | 120           |
|    ep_rew_mean          | 256           |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 32            |
|    time_elapsed         | 2648          |
|    total_timesteps      | 65536         |
| train/                  |               |
|    approx_kl            | 0.00094170857 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.54         |
|    explained_variance   | -6.32e-06     |
|    learning_rate        | 0.0003        |
|    loss                 | 105           |
|    n_updates            | 310           |
|    policy_gradient_loss | -0.000832     |
|    std                  | 0.367         |
|    value_loss           | 224           |
-------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.99        |
| reward_distance         | 0.216       |
| reward_energy           | 0.863       |
| reward_velocity         | 0.913       |
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 250         |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 33          |
|    time_elapsed         | 2739        |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.003632182 |
|    clip_fraction        | 0.000391    |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.57       |
|    explained_variance   | -1.96e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.000509   |
|    std                  | 0.367       |
|    value_loss           | 225         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.98        |
| reward_distance         | 0.211       |
| reward_energy           | 0.855       |
| reward_velocity         | 0.913       |
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 249         |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 34          |
|    time_elapsed         | 2825        |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.002019269 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.59       |
|    explained_variance   | 5.3e-05     |
|    learning_rate        | 0.0003      |
|    loss                 | 107         |
|    n_updates            | 330         |
|    policy_gradient_loss | -8.55e-05   |
|    std                  | 0.367       |
|    value_loss           | 228         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.02        |
| reward_distance         | 0.21        |
| reward_energy           | 0.874       |
| reward_velocity         | 0.933       |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 254         |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 35          |
|    time_elapsed         | 2913        |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.003459463 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.6        |
|    explained_variance   | 2.5e-06     |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0018     |
|    std                  | 0.367       |
|    value_loss           | 216         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.03        |
| reward_distance         | 0.216       |
| reward_energy           | 0.878       |
| reward_velocity         | 0.933       |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 255         |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 36          |
|    time_elapsed         | 2995        |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.000628136 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.62       |
|    explained_variance   | 1.79e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 110         |
|    n_updates            | 350         |
|    policy_gradient_loss | 0.000703    |
|    std                  | 0.367       |
|    value_loss           | 234         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.05        |
| reward_distance         | 0.223       |
| reward_energy           | 0.887       |
| reward_velocity         | 0.943       |
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | 260         |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 37          |
|    time_elapsed         | 3081        |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.000863511 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.61       |
|    explained_variance   | 3.1e-06     |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 360         |
|    policy_gradient_loss | -0.000786   |
|    std                  | 0.367       |
|    value_loss           | 241         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.04         |
| reward_distance         | 0.216        |
| reward_energy           | 0.889        |
| reward_velocity         | 0.94         |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 257          |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 38           |
|    time_elapsed         | 3171         |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0011842949 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.62        |
|    explained_variance   | 1.79e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 106          |
|    n_updates            | 370          |
|    policy_gradient_loss | -0.000289    |
|    std                  | 0.367        |
|    value_loss           | 225          |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.09          |
| reward_distance         | 0.222         |
| reward_energy           | 0.907         |
| reward_velocity         | 0.96          |
| rollout/                |               |
|    ep_len_mean          | 123           |
|    ep_rew_mean          | 263           |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 39            |
|    time_elapsed         | 3262          |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00086766714 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.62         |
|    explained_variance   | 1.13e-06      |
|    learning_rate        | 0.0003        |
|    loss                 | 114           |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000447     |
|    std                  | 0.367         |
|    value_loss           | 241           |
-------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.1          |
| reward_distance         | 0.231        |
| reward_energy           | 0.909        |
| reward_velocity         | 0.96         |
| rollout/                |              |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | 264          |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 40           |
|    time_elapsed         | 3354         |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0008242357 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.63        |
|    explained_variance   | 1.43e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 114          |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.000619    |
|    std                  | 0.367        |
|    value_loss           | 241          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.06         |
| reward_distance         | 0.232        |
| reward_energy           | 0.892        |
| reward_velocity         | 0.94         |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 258          |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 41           |
|    time_elapsed         | 3449         |
|    total_timesteps      | 83968        |
| train/                  |              |
|    approx_kl            | 0.0016217139 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.6         |
|    explained_variance   | -2.93e-05    |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 400          |
|    policy_gradient_loss | -0.000576    |
|    std                  | 0.367        |
|    value_loss           | 233          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2           |
| reward_distance         | 0.222       |
| reward_energy           | 0.868       |
| reward_velocity         | 0.911       |
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 252         |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 42          |
|    time_elapsed         | 3545        |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.001798104 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.59       |
|    explained_variance   | 3.1e-06     |
|    learning_rate        | 0.0003      |
|    loss                 | 115         |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.000976   |
|    std                  | 0.367       |
|    value_loss           | 242         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2           |
| reward_distance         | 0.218       |
| reward_energy           | 0.867       |
| reward_velocity         | 0.911       |
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 255         |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 43          |
|    time_elapsed         | 3643        |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.012429581 |
|    clip_fraction        | 0.00146     |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.56       |
|    explained_variance   | 9.54e-07    |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00807    |
|    std                  | 0.367       |
|    value_loss           | 256         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.01        |
| reward_distance         | 0.226       |
| reward_energy           | 0.865       |
| reward_velocity         | 0.921       |
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 256         |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 44          |
|    time_elapsed         | 3748        |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.012139238 |
|    clip_fraction        | 0.00156     |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.52       |
|    explained_variance   | 1.01e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 122         |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00422    |
|    std                  | 0.367       |
|    value_loss           | 256         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.97        |
| reward_distance         | 0.22        |
| reward_energy           | 0.846       |
| reward_velocity         | 0.901       |
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | 248         |
| time/                   |             |
|    fps                  | 23          |
|    iterations           | 45          |
|    time_elapsed         | 3857        |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.010147376 |
|    clip_fraction        | 0.00352     |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.53       |
|    explained_variance   | 1.49e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 116         |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.00236    |
|    std                  | 0.367       |
|    value_loss           | 243         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.95         |
| reward_distance         | 0.208        |
| reward_energy           | 0.843        |
| reward_velocity         | 0.901        |
| rollout/                |              |
|    ep_len_mean          | 117          |
|    ep_rew_mean          | 247          |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 46           |
|    time_elapsed         | 3968         |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0126088895 |
|    clip_fraction        | 0.00972      |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.59        |
|    explained_variance   | 8.11e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 114          |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000582    |
|    std                  | 0.367        |
|    value_loss           | 239          |
------------------------------------------
Num timesteps: 96000
Best mean reward: -inf - Last mean reward per episode: 255.17
Saving new best model to assets/out/models/exp2/best_model.zip
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2           |
| reward_distance         | 0.227       |
| reward_energy           | 0.851       |
| reward_velocity         | 0.921       |
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 257         |
| time/                   |             |
|    fps                  | 23          |
|    iterations           | 47          |
|    time_elapsed         | 4071        |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.005359381 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.63       |
|    explained_variance   | 2.79e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 134         |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.0014     |
|    std                  | 0.367       |
|    value_loss           | 280         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.08         |
| reward_distance         | 0.26         |
| reward_energy           | 0.87         |
| reward_velocity         | 0.95         |
| rollout/                |              |
|    ep_len_mean          | 122          |
|    ep_rew_mean          | 264          |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 48           |
|    time_elapsed         | 4180         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0011767058 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.62        |
|    explained_variance   | 1.07e-05     |
|    learning_rate        | 0.0003       |
|    loss                 | 138          |
|    n_updates            | 470          |
|    policy_gradient_loss | 0.000894     |
|    std                  | 0.367        |
|    value_loss           | 288          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.02         |
| reward_distance         | 0.253        |
| reward_energy           | 0.851        |
| reward_velocity         | 0.92         |
| rollout/                |              |
|    ep_len_mean          | 118          |
|    ep_rew_mean          | 255          |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 49           |
|    time_elapsed         | 4293         |
|    total_timesteps      | 100352       |
| train/                  |              |
|    approx_kl            | 0.0007306787 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.61        |
|    explained_variance   | 8.52e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 124          |
|    n_updates            | 480          |
|    policy_gradient_loss | 0.000262     |
|    std                  | 0.367        |
|    value_loss           | 259          |
------------------------------------------
Traceback (most recent call last):
  File "train.py", line 399, in <module>
    print(stable_baselines3.common.evaluation.evaluate_policy(model, env, render=True))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/evaluation.py", line 123, in evaluate_policy
    env.render()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 87, in render
    return super().render(mode=mode)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 178, in render
    imgs = self.get_images()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 70, in get_images
    return [env.render(mode="rgb_array") for env in self.envs]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 70, in <listcomp>
    return [env.render(mode="rgb_array") for env in self.envs]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/core.py", line 240, in render
    return self.env.render(mode, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/core.py", line 240, in render
    return self.env.render(mode, **kwargs)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 516, in render
    self._get_viewer(mode).render(width, height, camera_id=camera_id)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 545, in _get_viewer
    self.viewer = mujoco_py.MjRenderContextOffscreen(self.sim, -1)
AttributeError: module 'mujoco_py' has no attribute 'MjRenderContextOffscreen'
