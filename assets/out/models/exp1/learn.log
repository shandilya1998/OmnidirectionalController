running build_ext
GLFW error (code %d): %s 65544 b'X11: The DISPLAY environment variable is missing'
GLFW error (code %d): %s 65544 b'X11: The DISPLAY environment variable is missing'
Traceback (most recent call last):
  File "train.py", line 201, in <module>
    'info_keywords' : info_kwargs
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 105, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 80, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 60, in make
    env = cls(**_kwargs)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 104, in __init__
    observation, _reward, done, _info = self.step(action)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 305, in step
    reward, done, info = self.do_simulation(action, n_frames = self._frame_skip)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 461, in do_simulation
    self.render()
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 521, in render
    self._get_viewer(mode).render()
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 533, in _get_viewer
    self.viewer = mujoco_py.MjViewer(self.sim)
  File "/usr/local/lib/python3.6/dist-packages/mujoco_py-1.50.1.0-py3.6.egg/mujoco_py/mjviewer.py", line 130, in __init__
    super().__init__(sim)
  File "/usr/local/lib/python3.6/dist-packages/mujoco_py-1.50.1.0-py3.6.egg/mujoco_py/mjviewer.py", line 25, in __init__
    super().__init__(sim)
  File "mjrendercontext.pyx", line 244, in mujoco_py.cymj.MjRenderContextWindow.__init__
  File "mjrendercontext.pyx", line 43, in mujoco_py.cymj.MjRenderContext.__init__
  File "mjrendercontext.pyx", line 92, in mujoco_py.cymj.MjRenderContext._setup_opengl_context
  File "opengl_context.pyx", line 44, in mujoco_py.cymj.GlfwContext.__init__
  File "opengl_context.pyx", line 64, in mujoco_py.cymj.GlfwContext._init_glfw
mujoco_py.cymj.GlfwError: Failed to initialize GLFW
Traceback (most recent call last):
  File "train.py", line 7, in <module>
    import stable_baselines3
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/__init__.py", line 3, in <module>
    from stable_baselines3.a2c import A2C
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/a2c/__init__.py", line 1, in <module>
    from stable_baselines3.a2c.a2c import A2C
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/a2c/a2c.py", line 3, in <module>
    import torch as th
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/__init__.py", line 190, in <module>
    from torch._C import *
RuntimeError: KeyboardInterrupt: 
running build_ext
Using cuda device
[Quadruped] Command is `[0.007 0.    0.    0.    0.    0.   ]` with gait `trot` in task `straight` and direction `forward`
[Quadruped] Command is `[0.053 0.    0.    0.    0.    0.   ]` with gait `trot` in task `straight` and direction `forward`
[Quadruped] Command is `[0.047 0.    0.    0.    0.    0.   ]` with gait `trot` in task `straight` and direction `forward`
[Quadruped] Command is `[0.041 0.    0.    0.    0.    0.   ]` with gait `trot` in task `straight` and direction `forward`
Logging to assets/out/models/exp1/PPO_10
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp1/PPO_38
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 23       |
| time/              |          |
|    fps             | 36       |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 128      |
---------------------------------
---------------------------------------
| penalty                 | 0         |
| reward                  | 0.498     |
| reward_distance         | 0.5       |
| reward_energy           | -0.00126  |
| reward_velocity         | -0.00106  |
| rollout/                |           |
|    ep_len_mean          | 128       |
|    ep_rew_mean          | 33.5      |
| time/                   |           |
|    fps                  | 36        |
|    iterations           | 2         |
|    time_elapsed         | 6         |
|    total_timesteps      | 256       |
| train/                  |           |
|    approx_kl            | 0.0806283 |
|    clip_fraction        | 0.203     |
|    clip_range           | 0.4       |
|    entropy_loss         | -3.96     |
|    explained_variance   | 0.00434   |
|    learning_rate        | 0.0003    |
|    loss                 | 1.54      |
|    n_updates            | 10        |
|    policy_gradient_loss | -0.0726   |
|    std                  | 0.368     |
|    value_loss           | 3.99      |
---------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.499      |
| reward_distance         | 0.509      |
| reward_energy           | -0.00216   |
| reward_velocity         | -0.00795   |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 26         |
| time/                   |            |
|    fps                  | 37         |
|    iterations           | 3          |
|    time_elapsed         | 10         |
|    total_timesteps      | 384        |
| train/                  |            |
|    approx_kl            | 0.02864236 |
|    clip_fraction        | 0.0203     |
|    clip_range           | 0.4        |
|    entropy_loss         | -3.85      |
|    explained_variance   | -0.0236    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19       |
|    n_updates            | 20         |
|    policy_gradient_loss | 0.00464    |
|    std                  | 0.368      |
|    value_loss           | 6.74       |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.364      |
| reward_distance         | 0.388      |
| reward_energy           | -0.0117    |
| reward_velocity         | -0.0124    |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 20.6       |
| time/                   |            |
|    fps                  | 32         |
|    iterations           | 4          |
|    time_elapsed         | 15         |
|    total_timesteps      | 512        |
| train/                  |            |
|    approx_kl            | 0.24585913 |
|    clip_fraction        | 0.314      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.03      |
|    explained_variance   | -0.238     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.594      |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0259    |
|    std                  | 0.368      |
|    value_loss           | 1.37       |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.301      |
| reward_distance         | 0.319      |
| reward_energy           | -0.00888   |
| reward_velocity         | -0.00927   |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 21.6       |
| time/                   |            |
|    fps                  | 30         |
|    iterations           | 5          |
|    time_elapsed         | 20         |
|    total_timesteps      | 640        |
| train/                  |            |
|    approx_kl            | 0.07212226 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.37      |
|    explained_variance   | 0.0305     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.505      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.042     |
|    std                  | 0.368      |
|    value_loss           | 1.18       |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.304      |
| reward_distance         | 0.319      |
| reward_energy           | -0.00728   |
| reward_velocity         | -0.00742   |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 19.5       |
| time/                   |            |
|    fps                  | 29         |
|    iterations           | 6          |
|    time_elapsed         | 25         |
|    total_timesteps      | 768        |
| train/                  |            |
|    approx_kl            | 0.22695191 |
|    clip_fraction        | 0.298      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.24      |
|    explained_variance   | -0.0654    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.658      |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0261    |
|    std                  | 0.368      |
|    value_loss           | 1.62       |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.269      |
| reward_distance         | 0.281      |
| reward_energy           | -0.00621   |
| reward_velocity         | -0.00618   |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 20.2       |
| time/                   |            |
|    fps                  | 30         |
|    iterations           | 7          |
|    time_elapsed         | 29         |
|    total_timesteps      | 896        |
| train/                  |            |
|    approx_kl            | 0.23119017 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.42      |
|    explained_variance   | 0.0417     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.581      |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0371    |
|    std                  | 0.368      |
|    value_loss           | 1.39       |
----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 0.263        |
| reward_distance         | 0.274        |
| reward_energy           | -0.00533     |
| reward_velocity         | -0.0053      |
| rollout/                |              |
|    ep_len_mean          | 128          |
|    ep_rew_mean          | 19.9         |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 8            |
|    time_elapsed         | 32           |
|    total_timesteps      | 1024         |
| train/                  |              |
|    approx_kl            | 0.0103480145 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.48        |
|    explained_variance   | -0.0302      |
|    learning_rate        | 0.0003       |
|    loss                 | 0.869        |
|    n_updates            | 70           |
|    policy_gradient_loss | 0.00836      |
|    std                  | 0.368        |
|    value_loss           | 1.72         |
------------------------------------------
--------------------------------------
| penalty                 | 0        |
| reward                  | 0.256    |
| reward_distance         | 0.266    |
| reward_energy           | -0.00467 |
| reward_velocity         | -0.00464 |
| rollout/                |          |
|    ep_len_mean          | 128      |
|    ep_rew_mean          | 18.8     |
| time/                   |          |
|    fps                  | 33       |
|    iterations           | 9        |
|    time_elapsed         | 34       |
|    total_timesteps      | 1152     |
| train/                  |          |
|    approx_kl            | 0.990806 |
|    clip_fraction        | 0.456    |
|    clip_range           | 0.4      |
|    entropy_loss         | -4.08    |
|    explained_variance   | -0.00562 |
|    learning_rate        | 0.0003   |
|    loss                 | 0.758    |
|    n_updates            | 80       |
|    policy_gradient_loss | -0.0323  |
|    std                  | 0.369    |
|    value_loss           | 1.58     |
--------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.261      |
| reward_distance         | 0.269      |
| reward_energy           | -0.00434   |
| reward_velocity         | -0.00412   |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 20         |
| time/                   |            |
|    fps                  | 33         |
|    iterations           | 10         |
|    time_elapsed         | 37         |
|    total_timesteps      | 1280       |
| train/                  |            |
|    approx_kl            | 0.12154263 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.01      |
|    explained_variance   | 0.0814     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.619      |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0257    |
|    std                  | 0.369      |
|    value_loss           | 1.37       |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.277       |
| reward_distance         | 0.285       |
| reward_energy           | -0.00399    |
| reward_velocity         | -0.00371    |
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | 21.1        |
| time/                   |             |
|    fps                  | 35          |
|    iterations           | 11          |
|    time_elapsed         | 39          |
|    total_timesteps      | 1408        |
| train/                  |             |
|    approx_kl            | 0.045241594 |
|    clip_fraction        | 0.0906      |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.39       |
|    explained_variance   | 0.0261      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 100         |
|    policy_gradient_loss | 0.0564      |
|    std                  | 0.368       |
|    value_loss           | 2.56        |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.281      |
| reward_distance         | 0.296      |
| reward_energy           | -0.00941   |
| reward_velocity         | -0.00579   |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | 20.2       |
| time/                   |            |
|    fps                  | 37         |
|    iterations           | 12         |
|    time_elapsed         | 41         |
|    total_timesteps      | 1536       |
| train/                  |            |
|    approx_kl            | 0.06074789 |
|    clip_fraction        | 0.0508     |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.52      |
|    explained_variance   | -0.00414   |
|    learning_rate        | 0.0003     |
|    loss                 | 2.4        |
|    n_updates            | 110        |
|    policy_gradient_loss | 0.00917    |
|    std                  | 0.368      |
|    value_loss           | 4.92       |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.182       |
| reward_distance         | 0.284       |
| reward_energy           | -0.0114     |
| reward_velocity         | -0.0906     |
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 18.6        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 13          |
|    time_elapsed         | 43          |
|    total_timesteps      | 1664        |
| train/                  |             |
|    approx_kl            | 0.025216632 |
|    clip_fraction        | 0.0586      |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.83       |
|    explained_variance   | -0.0195     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.62        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00954    |
|    std                  | 0.368       |
|    value_loss           | 3.57        |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.191      |
| reward_distance         | 0.297      |
| reward_energy           | -0.0213    |
| reward_velocity         | -0.0841    |
| rollout/                |            |
|    ep_len_mean          | 119        |
|    ep_rew_mean          | 19.6       |
| time/                   |            |
|    fps                  | 38         |
|    iterations           | 14         |
|    time_elapsed         | 46         |
|    total_timesteps      | 1792       |
| train/                  |            |
|    approx_kl            | 0.08652544 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.73      |
|    explained_variance   | -0.125     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.999      |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.0242     |
|    std                  | 0.368      |
|    value_loss           | 3.07       |
----------------------------------------
---------------------------------------
| penalty                 | 0         |
| reward                  | 0.136     |
| reward_distance         | 0.287     |
| reward_energy           | -0.0208   |
| reward_velocity         | -0.13     |
| rollout/                |           |
|    ep_len_mean          | 112       |
|    ep_rew_mean          | 18.9      |
| time/                   |           |
|    fps                  | 38        |
|    iterations           | 15        |
|    time_elapsed         | 49        |
|    total_timesteps      | 1920      |
| train/                  |           |
|    approx_kl            | 0.0598565 |
|    clip_fraction        | 0.0398    |
|    clip_range           | 0.4       |
|    entropy_loss         | -4.69     |
|    explained_variance   | -0.0124   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.677     |
|    n_updates            | 140       |
|    policy_gradient_loss | -0.0239   |
|    std                  | 0.367     |
|    value_loss           | 1.47      |
---------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.142       |
| reward_distance         | 0.285       |
| reward_energy           | -0.0206     |
| reward_velocity         | -0.122      |
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 19          |
| time/                   |             |
|    fps                  | 39          |
|    iterations           | 16          |
|    time_elapsed         | 52          |
|    total_timesteps      | 2048        |
| train/                  |             |
|    approx_kl            | 0.049952228 |
|    clip_fraction        | 0.0406      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.01       |
|    explained_variance   | -0.025      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.13        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.00552    |
|    std                  | 0.367       |
|    value_loss           | 2.32        |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 0.144        |
| reward_distance         | 0.279        |
| reward_energy           | -0.0195      |
| reward_velocity         | -0.116       |
| rollout/                |              |
|    ep_len_mean          | 114          |
|    ep_rew_mean          | 19.3         |
| time/                   |              |
|    fps                  | 39           |
|    iterations           | 17           |
|    time_elapsed         | 55           |
|    total_timesteps      | 2176         |
| train/                  |              |
|    approx_kl            | 0.0049872957 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.16        |
|    explained_variance   | -0.00238     |
|    learning_rate        | 0.0003       |
|    loss                 | 1.03         |
|    n_updates            | 160          |
|    policy_gradient_loss | -0.00927     |
|    std                  | 0.367        |
|    value_loss           | 2.33         |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.144       |
| reward_distance         | 0.273       |
| reward_energy           | -0.0187     |
| reward_velocity         | -0.11       |
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 19.7        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 18          |
|    time_elapsed         | 59          |
|    total_timesteps      | 2304        |
| train/                  |             |
|    approx_kl            | 0.051943786 |
|    clip_fraction        | 0.0289      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.27       |
|    explained_variance   | 0.000621    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0219     |
|    std                  | 0.367       |
|    value_loss           | 2.5         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.169       |
| reward_distance         | 0.292       |
| reward_energy           | -0.0177     |
| reward_velocity         | -0.105      |
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 20.5        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 19          |
|    time_elapsed         | 63          |
|    total_timesteps      | 2432        |
| train/                  |             |
|    approx_kl            | 0.027660571 |
|    clip_fraction        | 0.0187      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.51       |
|    explained_variance   | 0.00267     |
|    learning_rate        | 0.0003      |
|    loss                 | 3.31        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.00876    |
|    std                  | 0.367       |
|    value_loss           | 7.42        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.155       |
| reward_distance         | 0.28        |
| reward_energy           | -0.018      |
| reward_velocity         | -0.106      |
| rollout/                |             |
|    ep_len_mean          | 111         |
|    ep_rew_mean          | 18.7        |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 20          |
|    time_elapsed         | 68          |
|    total_timesteps      | 2560        |
| train/                  |             |
|    approx_kl            | 0.012438038 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.66       |
|    explained_variance   | 0.00288     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.8         |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00727    |
|    std                  | 0.367       |
|    value_loss           | 4.03        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.119       |
| reward_distance         | 0.275       |
| reward_energy           | -0.0185     |
| reward_velocity         | -0.137      |
| rollout/                |             |
|    ep_len_mean          | 107         |
|    ep_rew_mean          | 18.4        |
| time/                   |             |
|    fps                  | 36          |
|    iterations           | 21          |
|    time_elapsed         | 73          |
|    total_timesteps      | 2688        |
| train/                  |             |
|    approx_kl            | 0.017886046 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.74       |
|    explained_variance   | -0.000828   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.4         |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00961    |
|    std                  | 0.367       |
|    value_loss           | 5.1         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.122       |
| reward_distance         | 0.272       |
| reward_energy           | -0.0178     |
| reward_velocity         | -0.132      |
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 18.5        |
| time/                   |             |
|    fps                  | 35          |
|    iterations           | 22          |
|    time_elapsed         | 79          |
|    total_timesteps      | 2816        |
| train/                  |             |
|    approx_kl            | 0.021734472 |
|    clip_fraction        | 0.0148      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.76       |
|    explained_variance   | -0.013      |
|    learning_rate        | 0.0003      |
|    loss                 | 2.84        |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.00732    |
|    std                  | 0.368       |
|    value_loss           | 5.75        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.144       |
| reward_distance         | 0.288       |
| reward_energy           | -0.0172     |
| reward_velocity         | -0.127      |
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | 21.1        |
| time/                   |             |
|    fps                  | 34          |
|    iterations           | 23          |
|    time_elapsed         | 85          |
|    total_timesteps      | 2944        |
| train/                  |             |
|    approx_kl            | 0.013165794 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.71       |
|    explained_variance   | -0.00315    |
|    learning_rate        | 0.0003      |
|    loss                 | 11.1        |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.0215      |
|    std                  | 0.368       |
|    value_loss           | 24.4        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.148       |
| reward_distance         | 0.286       |
| reward_energy           | -0.0165     |
| reward_velocity         | -0.122      |
| rollout/                |             |
|    ep_len_mean          | 109         |
|    ep_rew_mean          | 21.3        |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 24          |
|    time_elapsed         | 91          |
|    total_timesteps      | 3072        |
| train/                  |             |
|    approx_kl            | 0.038824145 |
|    clip_fraction        | 0.0172      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.78       |
|    explained_variance   | 0.00166     |
|    learning_rate        | 0.0003      |
|    loss                 | 2.63        |
|    n_updates            | 230         |
|    policy_gradient_loss | -0.00945    |
|    std                  | 0.367       |
|    value_loss           | 5.26        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.152       |
| reward_distance         | 0.286       |
| reward_energy           | -0.016      |
| reward_velocity         | -0.118      |
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | 21.8        |
| time/                   |             |
|    fps                  | 32          |
|    iterations           | 25          |
|    time_elapsed         | 97          |
|    total_timesteps      | 3200        |
| train/                  |             |
|    approx_kl            | 0.010432456 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.93       |
|    explained_variance   | -2.88e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 2.91        |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.00583     |
|    std                  | 0.367       |
|    value_loss           | 6.01        |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 0.151        |
| reward_distance         | 0.28         |
| reward_energy           | -0.0154      |
| reward_velocity         | -0.114       |
| rollout/                |              |
|    ep_len_mean          | 110          |
|    ep_rew_mean          | 21.4         |
| time/                   |              |
|    fps                  | 32           |
|    iterations           | 26           |
|    time_elapsed         | 102          |
|    total_timesteps      | 3328         |
| train/                  |              |
|    approx_kl            | 0.0065006413 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.98        |
|    explained_variance   | -0.000285    |
|    learning_rate        | 0.0003       |
|    loss                 | 2.98         |
|    n_updates            | 250          |
|    policy_gradient_loss | 0.00978      |
|    std                  | 0.367        |
|    value_loss           | 6.23         |
------------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.153      |
| reward_distance         | 0.278      |
| reward_energy           | -0.0149    |
| reward_velocity         | -0.11      |
| rollout/                |            |
|    ep_len_mean          | 111        |
|    ep_rew_mean          | 21.6       |
| time/                   |            |
|    fps                  | 31         |
|    iterations           | 27         |
|    time_elapsed         | 109        |
|    total_timesteps      | 3456       |
| train/                  |            |
|    approx_kl            | 0.10952699 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.4        |
|    entropy_loss         | -5.91      |
|    explained_variance   | -0.000506  |
|    learning_rate        | 0.0003     |
|    loss                 | 2.91       |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0375    |
|    std                  | 0.368      |
|    value_loss           | 5.95       |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.167       |
| reward_distance         | 0.288       |
| reward_energy           | -0.0145     |
| reward_velocity         | -0.106      |
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 23.2        |
| time/                   |             |
|    fps                  | 31          |
|    iterations           | 28          |
|    time_elapsed         | 115         |
|    total_timesteps      | 3584        |
| train/                  |             |
|    approx_kl            | 0.009930832 |
|    clip_fraction        | 0.00234     |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.9        |
|    explained_variance   | 0.000886    |
|    learning_rate        | 0.0003      |
|    loss                 | 6.38        |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.00367     |
|    std                  | 0.368       |
|    value_loss           | 15.3        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.173       |
| reward_distance         | 0.29        |
| reward_energy           | -0.014      |
| reward_velocity         | -0.103      |
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | 23.6        |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 29          |
|    time_elapsed         | 122         |
|    total_timesteps      | 3712        |
| train/                  |             |
|    approx_kl            | 0.044353828 |
|    clip_fraction        | 0.0398      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.97       |
|    explained_variance   | -0.0011     |
|    learning_rate        | 0.0003      |
|    loss                 | 3.27        |
|    n_updates            | 280         |
|    policy_gradient_loss | -0.0327     |
|    std                  | 0.367       |
|    value_loss           | 6.63        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.171       |
| reward_distance         | 0.284       |
| reward_energy           | -0.0136     |
| reward_velocity         | -0.1        |
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 23.2        |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 30          |
|    time_elapsed         | 129         |
|    total_timesteps      | 3840        |
| train/                  |             |
|    approx_kl            | 0.036419235 |
|    clip_fraction        | 0.0516      |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.17       |
|    explained_variance   | -0.00107    |
|    learning_rate        | 0.0003      |
|    loss                 | 3.24        |
|    n_updates            | 290         |
|    policy_gradient_loss | 0.00407     |
|    std                  | 0.367       |
|    value_loss           | 7.19        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.173       |
| reward_distance         | 0.283       |
| reward_energy           | -0.0133     |
| reward_velocity         | -0.0971     |
| rollout/                |             |
|    ep_len_mean          | 113         |
|    ep_rew_mean          | 23.4        |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 31          |
|    time_elapsed         | 135         |
|    total_timesteps      | 3968        |
| train/                  |             |
|    approx_kl            | 0.032431327 |
|    clip_fraction        | 0.0156      |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.14       |
|    explained_variance   | -0.000146   |
|    learning_rate        | 0.0003      |
|    loss                 | 3.47        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.0518     |
|    std                  | 0.367       |
|    value_loss           | 7.11        |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 0.17         |
| reward_distance         | 0.277        |
| reward_energy           | -0.0129      |
| reward_velocity         | -0.0943      |
| rollout/                |              |
|    ep_len_mean          | 113          |
|    ep_rew_mean          | 22.9         |
| time/                   |              |
|    fps                  | 28           |
|    iterations           | 32           |
|    time_elapsed         | 142          |
|    total_timesteps      | 4096         |
| train/                  |              |
|    approx_kl            | 0.0034689559 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.02        |
|    explained_variance   | -0.000538    |
|    learning_rate        | 0.0003       |
|    loss                 | 3.5          |
|    n_updates            | 310          |
|    policy_gradient_loss | 0.00807      |
|    std                  | 0.366        |
|    value_loss           | 7.53         |
------------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.173      |
| reward_distance         | 0.279      |
| reward_energy           | -0.0128    |
| reward_velocity         | -0.0932    |
| rollout/                |            |
|    ep_len_mean          | 114        |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 28         |
|    iterations           | 33         |
|    time_elapsed         | 149        |
|    total_timesteps      | 4224       |
| train/                  |            |
|    approx_kl            | 0.03250873 |
|    clip_fraction        | 0.0305     |
|    clip_range           | 0.4        |
|    entropy_loss         | -6.05      |
|    explained_variance   | 0.0223     |
|    learning_rate        | 0.0003     |
|    loss                 | 3.44       |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.031     |
|    std                  | 0.366      |
|    value_loss           | 7.23       |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.186       |
| reward_distance         | 0.289       |
| reward_energy           | -0.0124     |
| reward_velocity         | -0.0907     |
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 24.6        |
| time/                   |             |
|    fps                  | 27          |
|    iterations           | 34          |
|    time_elapsed         | 156         |
|    total_timesteps      | 4352        |
| train/                  |             |
|    approx_kl            | 0.015485904 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.16       |
|    explained_variance   | 0.0276      |
|    learning_rate        | 0.0003      |
|    loss                 | 7.93        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.0254     |
|    std                  | 0.366       |
|    value_loss           | 19.2        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.173       |
| reward_distance         | 0.287       |
| reward_energy           | -0.0246     |
| reward_velocity         | -0.0886     |
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 24.7        |
| time/                   |             |
|    fps                  | 27          |
|    iterations           | 35          |
|    time_elapsed         | 163         |
|    total_timesteps      | 4480        |
| train/                  |             |
|    approx_kl            | 0.008187103 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.19       |
|    explained_variance   | -0.016      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.37        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0246     |
|    std                  | 0.365       |
|    value_loss           | 6.89        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.173       |
| reward_distance         | 0.283       |
| reward_energy           | -0.0239     |
| reward_velocity         | -0.0864     |
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 24.3        |
| time/                   |             |
|    fps                  | 27          |
|    iterations           | 36          |
|    time_elapsed         | 169         |
|    total_timesteps      | 4608        |
| train/                  |             |
|    approx_kl            | 0.006314424 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.09       |
|    explained_variance   | 0.0662      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.65        |
|    n_updates            | 350         |
|    policy_gradient_loss | 0.00606     |
|    std                  | 0.365       |
|    value_loss           | 8.19        |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.175      |
| reward_distance         | 0.282      |
| reward_energy           | -0.0233    |
| reward_velocity         | -0.0842    |
| rollout/                |            |
|    ep_len_mean          | 115        |
|    ep_rew_mean          | 24.4       |
| time/                   |            |
|    fps                  | 26         |
|    iterations           | 37         |
|    time_elapsed         | 176        |
|    total_timesteps      | 4736       |
| train/                  |            |
|    approx_kl            | 0.09387481 |
|    clip_fraction        | 0.0695     |
|    clip_range           | 0.4        |
|    entropy_loss         | -5.95      |
|    explained_variance   | -0.0141    |
|    learning_rate        | 0.0003     |
|    loss                 | 3.43       |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0619    |
|    std                  | 0.365      |
|    value_loss           | 7.07       |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.174      |
| reward_distance         | 0.279      |
| reward_energy           | -0.0228    |
| reward_velocity         | -0.0822    |
| rollout/                |            |
|    ep_len_mean          | 116        |
|    ep_rew_mean          | 24.2       |
| time/                   |            |
|    fps                  | 26         |
|    iterations           | 38         |
|    time_elapsed         | 183        |
|    total_timesteps      | 4864       |
| train/                  |            |
|    approx_kl            | 0.02188083 |
|    clip_fraction        | 0.0125     |
|    clip_range           | 0.4        |
|    entropy_loss         | -5.8       |
|    explained_variance   | 7.33e-05   |
|    learning_rate        | 0.0003     |
|    loss                 | 3.01       |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0272    |
|    std                  | 0.364      |
|    value_loss           | 6.42       |
----------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp1/PPO_39
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | -313     |
| time/              |          |
|    fps             | 36       |
|    iterations      | 1        |
|    time_elapsed    | 3        |
|    total_timesteps | 128      |
---------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -15.1      |
| reward_distance         | 0.417      |
| reward_energy           | -0.449     |
| reward_velocity         | -0.151     |
| rollout/                |            |
|    ep_len_mean          | 64.5       |
|    ep_rew_mean          | -171       |
| time/                   |            |
|    fps                  | 38         |
|    iterations           | 2          |
|    time_elapsed         | 6          |
|    total_timesteps      | 256        |
| train/                  |            |
|    approx_kl            | 0.10195224 |
|    clip_fraction        | 0.255      |
|    clip_range           | 0.4        |
|    entropy_loss         | -3.89      |
|    explained_variance   | -0.000763  |
|    learning_rate        | 0.0003     |
|    loss                 | 169        |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0369    |
|    std                  | 0.368      |
|    value_loss           | 401        |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -17.8      |
| reward_distance         | 0.432      |
| reward_energy           | -0.952     |
| reward_velocity         | -0.173     |
| rollout/                |            |
|    ep_len_mean          | 85.7       |
|    ep_rew_mean          | -230       |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 3          |
|    time_elapsed         | 9          |
|    total_timesteps      | 384        |
| train/                  |            |
|    approx_kl            | 0.11673074 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.4        |
|    entropy_loss         | -3.94      |
|    explained_variance   | 0.00329    |
|    learning_rate        | 0.0003     |
|    loss                 | 268        |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0393    |
|    std                  | 0.368      |
|    value_loss           | 556        |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -14.1      |
| reward_distance         | 0.368      |
| reward_energy           | -1.17      |
| reward_velocity         | -0.133     |
| rollout/                |            |
|    ep_len_mean          | 96.2       |
|    ep_rew_mean          | -221       |
| time/                   |            |
|    fps                  | 40         |
|    iterations           | 4          |
|    time_elapsed         | 12         |
|    total_timesteps      | 512        |
| train/                  |            |
|    approx_kl            | 0.24508053 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.15      |
|    explained_variance   | 0.0126     |
|    learning_rate        | 0.0003     |
|    loss                 | 60.8       |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0696    |
|    std                  | 0.368      |
|    value_loss           | 140        |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -11.2      |
| reward_distance         | 0.351      |
| reward_energy           | -0.939     |
| reward_velocity         | -0.106     |
| rollout/                |            |
|    ep_len_mean          | 103        |
|    ep_rew_mean          | -175       |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 5          |
|    time_elapsed         | 16         |
|    total_timesteps      | 640        |
| train/                  |            |
|    approx_kl            | 0.17070964 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.52      |
|    explained_variance   | -0.0174    |
|    learning_rate        | 0.0003     |
|    loss                 | 8.04       |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0151    |
|    std                  | 0.368      |
|    value_loss           | 15.1       |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -9.35      |
| reward_distance         | 0.311      |
| reward_energy           | -0.783     |
| reward_velocity         | -0.0888    |
| rollout/                |            |
|    ep_len_mean          | 107        |
|    ep_rew_mean          | -172       |
| time/                   |            |
|    fps                  | 36         |
|    iterations           | 6          |
|    time_elapsed         | 21         |
|    total_timesteps      | 768        |
| train/                  |            |
|    approx_kl            | 0.13432813 |
|    clip_fraction        | 0.0555     |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.84      |
|    explained_variance   | 0.0052     |
|    learning_rate        | 0.0003     |
|    loss                 | 149        |
|    n_updates            | 50         |
|    policy_gradient_loss | -0.0408    |
|    std                  | 0.367      |
|    value_loss           | 302        |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -7.96       |
| reward_distance         | 0.319       |
| reward_energy           | -0.671      |
| reward_velocity         | -0.0761     |
| rollout/                |             |
|    ep_len_mean          | 110         |
|    ep_rew_mean          | -151        |
| time/                   |             |
|    fps                  | 34          |
|    iterations           | 7           |
|    time_elapsed         | 25          |
|    total_timesteps      | 896         |
| train/                  |             |
|    approx_kl            | 0.030098487 |
|    clip_fraction        | 0.0437      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.04       |
|    explained_variance   | 0.00213     |
|    learning_rate        | 0.0003      |
|    loss                 | 19.7        |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00235    |
|    std                  | 0.367       |
|    value_loss           | 40          |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -6.93       |
| reward_distance         | 0.313       |
| reward_energy           | -0.588      |
| reward_velocity         | -0.0666     |
| rollout/                |             |
|    ep_len_mean          | 112         |
|    ep_rew_mean          | -142        |
| time/                   |             |
|    fps                  | 35          |
|    iterations           | 8           |
|    time_elapsed         | 29          |
|    total_timesteps      | 1024        |
| train/                  |             |
|    approx_kl            | 0.073861435 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.13       |
|    explained_variance   | 0.00155     |
|    learning_rate        | 0.0003      |
|    loss                 | 44.4        |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0135     |
|    std                  | 0.367       |
|    value_loss           | 91.1        |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -6.14      |
| reward_distance         | 0.305      |
| reward_energy           | -0.523     |
| reward_velocity         | -0.0592    |
| rollout/                |            |
|    ep_len_mean          | 114        |
|    ep_rew_mean          | -129       |
| time/                   |            |
|    fps                  | 34         |
|    iterations           | 9          |
|    time_elapsed         | 33         |
|    total_timesteps      | 1152       |
| train/                  |            |
|    approx_kl            | 0.31005767 |
|    clip_fraction        | 0.273      |
|    clip_range           | 0.4        |
|    entropy_loss         | -5.56      |
|    explained_variance   | 0.0127     |
|    learning_rate        | 0.0003     |
|    loss                 | 12.5       |
|    n_updates            | 80         |
|    policy_gradient_loss | 0.0191     |
|    std                  | 0.367      |
|    value_loss           | 24.9       |
----------------------------------------
---------------------------------------
| penalty                 | 0         |
| reward                  | -5.5      |
| reward_distance         | 0.293     |
| reward_energy           | -0.471    |
| reward_velocity         | -0.0533   |
| rollout/                |           |
|    ep_len_mean          | 115       |
|    ep_rew_mean          | -119      |
| time/                   |           |
|    fps                  | 34        |
|    iterations           | 10        |
|    time_elapsed         | 37        |
|    total_timesteps      | 1280      |
| train/                  |           |
|    approx_kl            | 0.0928019 |
|    clip_fraction        | 0.102     |
|    clip_range           | 0.4       |
|    entropy_loss         | -6.15     |
|    explained_variance   | 0.00309   |
|    learning_rate        | 0.0003    |
|    loss                 | 10        |
|    n_updates            | 90        |
|    policy_gradient_loss | 0.00858   |
|    std                  | 0.366     |
|    value_loss           | 21.8      |
---------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -4.96        |
| reward_distance         | 0.305        |
| reward_energy           | -0.428       |
| reward_velocity         | -0.0484      |
| rollout/                |              |
|    ep_len_mean          | 116          |
|    ep_rew_mean          | -112         |
| time/                   |              |
|    fps                  | 33           |
|    iterations           | 11           |
|    time_elapsed         | 42           |
|    total_timesteps      | 1408         |
| train/                  |              |
|    approx_kl            | 0.0049987277 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.36        |
|    explained_variance   | -0.000111    |
|    learning_rate        | 0.0003       |
|    loss                 | 33.3         |
|    n_updates            | 100          |
|    policy_gradient_loss | 0.0079       |
|    std                  | 0.366        |
|    value_loss           | 72.6         |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | -4.54         |
| reward_distance         | 0.295         |
| reward_energy           | -0.393        |
| reward_velocity         | -0.0444       |
| rollout/                |               |
|    ep_len_mean          | 117           |
|    ep_rew_mean          | -115          |
| time/                   |               |
|    fps                  | 32            |
|    iterations           | 12            |
|    time_elapsed         | 46            |
|    total_timesteps      | 1536          |
| train/                  |               |
|    approx_kl            | 3.9895065e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -6.41         |
|    explained_variance   | 0.0225        |
|    learning_rate        | 0.0003        |
|    loss                 | 235           |
|    n_updates            | 110           |
|    policy_gradient_loss | 0.00131       |
|    std                  | 0.366         |
|    value_loss           | 475           |
-------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -4.18       |
| reward_distance         | 0.282       |
| reward_energy           | -0.363      |
| reward_velocity         | -0.041      |
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | -108        |
| time/                   |             |
|    fps                  | 32          |
|    iterations           | 13          |
|    time_elapsed         | 51          |
|    total_timesteps      | 1664        |
| train/                  |             |
|    approx_kl            | 0.007147508 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.44       |
|    explained_variance   | 0.0213      |
|    learning_rate        | 0.0003      |
|    loss                 | 3.86        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00631    |
|    std                  | 0.366       |
|    value_loss           | 8.94        |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -3.87        |
| reward_distance         | 0.273        |
| reward_energy           | -0.337       |
| reward_velocity         | -0.038       |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | -107         |
| time/                   |              |
|    fps                  | 32           |
|    iterations           | 14           |
|    time_elapsed         | 55           |
|    total_timesteps      | 1792         |
| train/                  |              |
|    approx_kl            | 0.0018022172 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.5         |
|    explained_variance   | -0.000286    |
|    learning_rate        | 0.0003       |
|    loss                 | 57.3         |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000161    |
|    std                  | 0.366        |
|    value_loss           | 115          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -3.58        |
| reward_distance         | 0.285        |
| reward_energy           | -0.315       |
| reward_velocity         | -0.0355      |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | -108         |
| time/                   |              |
|    fps                  | 31           |
|    iterations           | 15           |
|    time_elapsed         | 60           |
|    total_timesteps      | 1920         |
| train/                  |              |
|    approx_kl            | 7.881317e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.51        |
|    explained_variance   | -0.0001      |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 140          |
|    policy_gradient_loss | 0.000171     |
|    std                  | 0.366        |
|    value_loss           | 227          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -3.33       |
| reward_distance         | 0.294       |
| reward_energy           | -0.298      |
| reward_velocity         | -0.0333     |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | -101        |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 16          |
|    time_elapsed         | 66          |
|    total_timesteps      | 2048        |
| train/                  |             |
|    approx_kl            | 0.027807403 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.54       |
|    explained_variance   | -0.00148    |
|    learning_rate        | 0.0003      |
|    loss                 | 11.2        |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0155     |
|    std                  | 0.366       |
|    value_loss           | 23.4        |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -3.99        |
| reward_distance         | 0.285        |
| reward_energy           | -0.276       |
| reward_velocity         | -0.04        |
| rollout/                |              |
|    ep_len_mean          | 114          |
|    ep_rew_mean          | -94.3        |
| time/                   |              |
|    fps                  | 30           |
|    iterations           | 17           |
|    time_elapsed         | 72           |
|    total_timesteps      | 2176         |
| train/                  |              |
|    approx_kl            | 0.0077488096 |
|    clip_fraction        | 0.0125       |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.66        |
|    explained_variance   | -0.000314    |
|    learning_rate        | 0.0003       |
|    loss                 | 23.9         |
|    n_updates            | 160          |
|    policy_gradient_loss | 0.00528      |
|    std                  | 0.366        |
|    value_loss           | 47.7         |
------------------------------------------
--------------------------------------
| penalty                 | 0        |
| reward                  | -3.77    |
| reward_distance         | 0.28     |
| reward_energy           | -0.262   |
| reward_velocity         | -0.0379  |
| rollout/                |          |
|    ep_len_mean          | 115      |
|    ep_rew_mean          | -90      |
| time/                   |          |
|    fps                  | 29       |
|    iterations           | 18       |
|    time_elapsed         | 77       |
|    total_timesteps      | 2304     |
| train/                  |          |
|    approx_kl            | 0.027532 |
|    clip_fraction        | 0.00547  |
|    clip_range           | 0.4      |
|    entropy_loss         | -6.63    |
|    explained_variance   | -0.128   |
|    learning_rate        | 0.0003   |
|    loss                 | 7.57     |
|    n_updates            | 170      |
|    policy_gradient_loss | -0.0283  |
|    std                  | 0.365    |
|    value_loss           | 16.4     |
--------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -5.01      |
| reward_distance         | 0.269      |
| reward_energy           | -0.249     |
| reward_velocity         | -0.0503    |
| rollout/                |            |
|    ep_len_mean          | 110        |
|    ep_rew_mean          | -83.2      |
| time/                   |            |
|    fps                  | 29         |
|    iterations           | 19         |
|    time_elapsed         | 82         |
|    total_timesteps      | 2432       |
| train/                  |            |
|    approx_kl            | 0.03089768 |
|    clip_fraction        | 0.00313    |
|    clip_range           | 0.4        |
|    entropy_loss         | -6.58      |
|    explained_variance   | -0.000953  |
|    learning_rate        | 0.0003     |
|    loss                 | 2.25       |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.00655   |
|    std                  | 0.365      |
|    value_loss           | 4.56       |
----------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp1/PPO_40
---------------------------------
| penalty            | 0        |
| reward             | -42.7    |
| reward_distance    | 0.209    |
| reward_energy      | -0.621   |
| reward_velocity    | -0.423   |
| rollout/           |          |
|    ep_len_mean     | 120      |
|    ep_rew_mean     | -413     |
| time/              |          |
|    fps             | 38       |
|    iterations      | 1        |
|    time_elapsed    | 53       |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -3.38      |
| reward_distance         | 0.579      |
| reward_energy           | -0.123     |
| reward_velocity         | -0.0384    |
| rollout/                |            |
|    ep_len_mean          | 120        |
|    ep_rew_mean          | -328       |
| time/                   |            |
|    fps                  | 35         |
|    iterations           | 2          |
|    time_elapsed         | 116        |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.13952416 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.4        |
|    entropy_loss         | -3.95      |
|    explained_variance   | 1.38e-05   |
|    learning_rate        | 0.0003     |
|    loss                 | 798        |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0399    |
|    std                  | 0.368      |
|    value_loss           | 1.7e+03    |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -2.93       |
| reward_distance         | 0.472       |
| reward_energy           | -0.0802     |
| reward_velocity         | -0.0332     |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | -314        |
| time/                   |             |
|    fps                  | 32          |
|    iterations           | 3           |
|    time_elapsed         | 187         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.085797355 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.4         |
|    entropy_loss         | -4          |
|    explained_variance   | 0.00175     |
|    learning_rate        | 0.0003      |
|    loss                 | 331         |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0166     |
|    std                  | 0.368       |
|    value_loss           | 702         |
-----------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp1/PPO_41
---------------------------------
| penalty            | 0        |
| reward             | -65.7    |
| reward_distance    | 0.184    |
| reward_energy      | -0.336   |
| reward_velocity    | -0.655   |
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | -341     |
| time/              |          |
|    fps             | 38       |
|    iterations      | 1        |
|    time_elapsed    | 52       |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -11.3      |
| reward_distance         | 0.399      |
| reward_energy           | -0.317     |
| reward_velocity         | -0.114     |
| rollout/                |            |
|    ep_len_mean          | 113        |
|    ep_rew_mean          | -263       |
| time/                   |            |
|    fps                  | 37         |
|    iterations           | 2          |
|    time_elapsed         | 107        |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.11271989 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.02      |
|    explained_variance   | -9.99e-05  |
|    learning_rate        | 0.0003     |
|    loss                 | 823        |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0242    |
|    std                  | 0.368      |
|    value_loss           | 1.73e+03   |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -9.31      |
| reward_distance         | 0.37       |
| reward_energy           | -0.302     |
| reward_velocity         | -0.0937    |
| rollout/                |            |
|    ep_len_mean          | 112        |
|    ep_rew_mean          | -242       |
| time/                   |            |
|    fps                  | 37         |
|    iterations           | 3          |
|    time_elapsed         | 162        |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.08499662 |
|    clip_fraction        | 0.071      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.07      |
|    explained_variance   | 0.0024     |
|    learning_rate        | 0.0003     |
|    loss                 | 389        |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0153    |
|    std                  | 0.368      |
|    value_loss           | 816        |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -7.77      |
| reward_distance         | 0.361      |
| reward_energy           | -0.221     |
| reward_velocity         | -0.0791    |
| rollout/                |            |
|    ep_len_mean          | 114        |
|    ep_rew_mean          | -214       |
| time/                   |            |
|    fps                  | 37         |
|    iterations           | 4          |
|    time_elapsed         | 217        |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.12005527 |
|    clip_fraction        | 0.0866     |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.25      |
|    explained_variance   | 0.00713    |
|    learning_rate        | 0.0003     |
|    loss                 | 219        |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00899   |
|    std                  | 0.368      |
|    value_loss           | 454        |
----------------------------------------
---------------------------------------
| penalty                 | 0         |
| reward                  | -6.63     |
| reward_distance         | 0.329     |
| reward_energy           | -0.193    |
| reward_velocity         | -0.0677   |
| rollout/                |           |
|    ep_len_mean          | 114       |
|    ep_rew_mean          | -189      |
| time/                   |           |
|    fps                  | 37        |
|    iterations           | 5         |
|    time_elapsed         | 269       |
|    total_timesteps      | 10240     |
| train/                  |           |
|    approx_kl            | 0.5071694 |
|    clip_fraction        | 0.383     |
|    clip_range           | 0.4       |
|    entropy_loss         | -4.74     |
|    explained_variance   | -0.00153  |
|    learning_rate        | 0.0003    |
|    loss                 | 49.8      |
|    n_updates            | 40        |
|    policy_gradient_loss | 0.0333    |
|    std                  | 0.367     |
|    value_loss           | 99.2      |
---------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -8.11       |
| reward_distance         | 0.309       |
| reward_energy           | -0.173      |
| reward_velocity         | -0.0825     |
| rollout/                |             |
|    ep_len_mean          | 111         |
|    ep_rew_mean          | -171        |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 6           |
|    time_elapsed         | 317         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.080553226 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.27       |
|    explained_variance   | 0.00196     |
|    learning_rate        | 0.0003      |
|    loss                 | 129         |
|    n_updates            | 50          |
|    policy_gradient_loss | 0.0139      |
|    std                  | 0.367       |
|    value_loss           | 259         |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -7.02      |
| reward_distance         | 0.275      |
| reward_energy           | -0.125     |
| reward_velocity         | -0.0717    |
| rollout/                |            |
|    ep_len_mean          | 113        |
|    ep_rew_mean          | -120       |
| time/                   |            |
|    fps                  | 38         |
|    iterations           | 7          |
|    time_elapsed         | 374        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.23764884 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.4        |
|    entropy_loss         | -5.62      |
|    explained_variance   | 0.00213    |
|    learning_rate        | 0.0003     |
|    loss                 | 50.4       |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.012     |
|    std                  | 0.367      |
|    value_loss           | 101        |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -6.6       |
| reward_distance         | 0.262      |
| reward_energy           | -0.0958    |
| reward_velocity         | -0.0677    |
| rollout/                |            |
|    ep_len_mean          | 115        |
|    ep_rew_mean          | -99.2      |
| time/                   |            |
|    fps                  | 37         |
|    iterations           | 8          |
|    time_elapsed         | 435        |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.04600761 |
|    clip_fraction        | 0.0433     |
|    clip_range           | 0.4        |
|    entropy_loss         | -5.95      |
|    explained_variance   | 0.00333    |
|    learning_rate        | 0.0003     |
|    loss                 | 30.7       |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.00446   |
|    std                  | 0.367      |
|    value_loss           | 62.6       |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -5.3        |
| reward_distance         | 0.259       |
| reward_energy           | -0.0576     |
| reward_velocity         | -0.055      |
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | -75.9       |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 9           |
|    time_elapsed         | 490         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.020454546 |
|    clip_fraction        | 0.0188      |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.13       |
|    explained_variance   | -0.000272   |
|    learning_rate        | 0.0003      |
|    loss                 | 75.2        |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.00349     |
|    std                  | 0.367       |
|    value_loss           | 153         |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -4.46      |
| reward_distance         | 0.247      |
| reward_energy           | -0.0477    |
| reward_velocity         | -0.0466    |
| rollout/                |            |
|    ep_len_mean          | 117        |
|    ep_rew_mean          | -62.6      |
| time/                   |            |
|    fps                  | 37         |
|    iterations           | 10         |
|    time_elapsed         | 549        |
|    total_timesteps      | 20480      |
| train/                  |            |
|    approx_kl            | 0.05966977 |
|    clip_fraction        | 0.0328     |
|    clip_range           | 0.4        |
|    entropy_loss         | -6.27      |
|    explained_variance   | 0.00446    |
|    learning_rate        | 0.0003     |
|    loss                 | 65         |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.00192   |
|    std                  | 0.366      |
|    value_loss           | 130        |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -3.92      |
| reward_distance         | 0.236      |
| reward_energy           | -0.0335    |
| reward_velocity         | -0.0412    |
| rollout/                |            |
|    ep_len_mean          | 119        |
|    ep_rew_mean          | -53.2      |
| time/                   |            |
|    fps                  | 36         |
|    iterations           | 11         |
|    time_elapsed         | 610        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.07362107 |
|    clip_fraction        | 0.0542     |
|    clip_range           | 0.4        |
|    entropy_loss         | -6.55      |
|    explained_variance   | 0.00731    |
|    learning_rate        | 0.0003     |
|    loss                 | 26.8       |
|    n_updates            | 100        |
|    policy_gradient_loss | -0.0026    |
|    std                  | 0.365      |
|    value_loss           | 53.7       |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -1.36       |
| reward_distance         | 0.248       |
| reward_energy           | -0.0174     |
| reward_velocity         | -0.0159     |
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | -46.6       |
| time/                   |             |
|    fps                  | 36          |
|    iterations           | 12          |
|    time_elapsed         | 676         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011804925 |
|    clip_fraction        | 0.0102      |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.69       |
|    explained_variance   | 0.00584     |
|    learning_rate        | 0.0003      |
|    loss                 | 46.4        |
|    n_updates            | 110         |
|    policy_gradient_loss | 0.00417     |
|    std                  | 0.365       |
|    value_loss           | 95.4        |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -0.373       |
| reward_distance         | 0.253        |
| reward_energy           | -0.0121      |
| reward_velocity         | -0.00614     |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | -44.6        |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 13           |
|    time_elapsed         | 738          |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0044161826 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.73        |
|    explained_variance   | 0.00432      |
|    learning_rate        | 0.0003       |
|    loss                 | 16.2         |
|    n_updates            | 120          |
|    policy_gradient_loss | 0.000472     |
|    std                  | 0.365        |
|    value_loss           | 32.4         |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 0.0516       |
| reward_distance         | 0.255        |
| reward_energy           | -0.00902     |
| reward_velocity         | -0.00194     |
| rollout/                |              |
|    ep_len_mean          | 127          |
|    ep_rew_mean          | -42          |
| time/                   |              |
|    fps                  | 35           |
|    iterations           | 14           |
|    time_elapsed         | 805          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0018135087 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.73        |
|    explained_variance   | 0.00225      |
|    learning_rate        | 0.0003       |
|    loss                 | 30           |
|    n_updates            | 130          |
|    policy_gradient_loss | -0.000686    |
|    std                  | 0.365        |
|    value_loss           | 60.5         |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -2.86       |
| reward_distance         | 0.231       |
| reward_energy           | -0.0205     |
| reward_velocity         | -0.0307     |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | -40.7       |
| time/                   |             |
|    fps                  | 35          |
|    iterations           | 15          |
|    time_elapsed         | 870         |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.024309995 |
|    clip_fraction        | 0.0203      |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.79       |
|    explained_variance   | 0.00443     |
|    learning_rate        | 0.0003      |
|    loss                 | 39.5        |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00517    |
|    std                  | 0.365       |
|    value_loss           | 79.4        |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -2.9         |
| reward_distance         | 0.223        |
| reward_energy           | -0.0226      |
| reward_velocity         | -0.031       |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | -36.9        |
| time/                   |              |
|    fps                  | 35           |
|    iterations           | 16           |
|    time_elapsed         | 933          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0054686707 |
|    clip_fraction        | 0.00234      |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.85        |
|    explained_variance   | -0.00139     |
|    learning_rate        | 0.0003       |
|    loss                 | 28.2         |
|    n_updates            | 150          |
|    policy_gradient_loss | 0.00257      |
|    std                  | 0.365        |
|    value_loss           | 57.1         |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -3.59       |
| reward_distance         | 0.234       |
| reward_energy           | -0.0267     |
| reward_velocity         | -0.038      |
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | -35.1       |
| time/                   |             |
|    fps                  | 34          |
|    iterations           | 17          |
|    time_elapsed         | 1010        |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.023114208 |
|    clip_fraction        | 0.0109      |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.86       |
|    explained_variance   | 0.00667     |
|    learning_rate        | 0.0003      |
|    loss                 | 14.6        |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00408    |
|    std                  | 0.365       |
|    value_loss           | 29          |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -3.95       |
| reward_distance         | 0.224       |
| reward_energy           | -0.0258     |
| reward_velocity         | -0.0415     |
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | -32.6       |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 18          |
|    time_elapsed         | 1091        |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.011130802 |
|    clip_fraction        | 0.00371     |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.88       |
|    explained_variance   | 0.00146     |
|    learning_rate        | 0.0003      |
|    loss                 | 26.9        |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.00139    |
|    std                  | 0.365       |
|    value_loss           | 54.4        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -3.95       |
| reward_distance         | 0.215       |
| reward_energy           | -0.0227     |
| reward_velocity         | -0.0415     |
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | -33.3       |
| time/                   |             |
|    fps                  | 33          |
|    iterations           | 19          |
|    time_elapsed         | 1170        |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.004761899 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.89       |
|    explained_variance   | 0.0163      |
|    learning_rate        | 0.0003      |
|    loss                 | 30.2        |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.000774   |
|    std                  | 0.365       |
|    value_loss           | 60.6        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -3.96       |
| reward_distance         | 0.204       |
| reward_energy           | -0.022      |
| reward_velocity         | -0.0415     |
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | -28.5       |
| time/                   |             |
|    fps                  | 32          |
|    iterations           | 20          |
|    time_elapsed         | 1248        |
|    total_timesteps      | 40960       |
| train/                  |             |
|    approx_kl            | 0.011948673 |
|    clip_fraction        | 0.00293     |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.91       |
|    explained_variance   | 0.0306      |
|    learning_rate        | 0.0003      |
|    loss                 | 19.4        |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.0026     |
|    std                  | 0.365       |
|    value_loss           | 39          |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -1.35        |
| reward_distance         | 0.21         |
| reward_energy           | -0.0131      |
| reward_velocity         | -0.0155      |
| rollout/                |              |
|    ep_len_mean          | 122          |
|    ep_rew_mean          | -27.2        |
| time/                   |              |
|    fps                  | 32           |
|    iterations           | 21           |
|    time_elapsed         | 1327         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0072091175 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.93        |
|    explained_variance   | 0.0054       |
|    learning_rate        | 0.0003       |
|    loss                 | 17.8         |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.000654    |
|    std                  | 0.364        |
|    value_loss           | 35.9         |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -2.65        |
| reward_distance         | 0.217        |
| reward_energy           | -0.0159      |
| reward_velocity         | -0.0285      |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | -28.8        |
| time/                   |              |
|    fps                  | 32           |
|    iterations           | 22           |
|    time_elapsed         | 1405         |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0025558632 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.94        |
|    explained_variance   | -0.001       |
|    learning_rate        | 0.0003       |
|    loss                 | 38.7         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000439    |
|    std                  | 0.364        |
|    value_loss           | 77.3         |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -1.95       |
| reward_distance         | 0.221       |
| reward_energy           | -0.0118     |
| reward_velocity         | -0.0215     |
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | -30.3       |
| time/                   |             |
|    fps                  | 31          |
|    iterations           | 23          |
|    time_elapsed         | 1489        |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.004933997 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.00391     |
|    learning_rate        | 0.0003      |
|    loss                 | 28.4        |
|    n_updates            | 220         |
|    policy_gradient_loss | -0.00117    |
|    std                  | 0.364       |
|    value_loss           | 56.9        |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -1.95      |
| reward_distance         | 0.213      |
| reward_energy           | -0.0118    |
| reward_velocity         | -0.0215    |
| rollout/                |            |
|    ep_len_mean          | 124        |
|    ep_rew_mean          | -30.5      |
| time/                   |            |
|    fps                  | 31         |
|    iterations           | 24         |
|    time_elapsed         | 1581       |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.03672444 |
|    clip_fraction        | 0.0254     |
|    clip_range           | 0.4        |
|    entropy_loss         | -6.96      |
|    explained_variance   | 0.0112     |
|    learning_rate        | 0.0003     |
|    loss                 | 21.4       |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.00607   |
|    std                  | 0.364      |
|    value_loss           | 42.8       |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -1.41       |
| reward_distance         | 0.221       |
| reward_energy           | -0.0103     |
| reward_velocity         | -0.0162     |
| rollout/                |             |
|    ep_len_mean          | 124         |
|    ep_rew_mean          | -29.1       |
| time/                   |             |
|    fps                  | 30          |
|    iterations           | 25          |
|    time_elapsed         | 1682        |
|    total_timesteps      | 51200       |
| train/                  |             |
|    approx_kl            | 0.030854536 |
|    clip_fraction        | 0.0299      |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.98       |
|    explained_variance   | -1.57e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 22.9        |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.000528    |
|    std                  | 0.364       |
|    value_loss           | 45.9        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -1.39       |
| reward_distance         | 0.239       |
| reward_energy           | -0.0103     |
| reward_velocity         | -0.0162     |
| rollout/                |             |
|    ep_len_mean          | 124         |
|    ep_rew_mean          | -32.9       |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 26          |
|    time_elapsed         | 1784        |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.009506972 |
|    clip_fraction        | 0.00137     |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.99       |
|    explained_variance   | 0.0368      |
|    learning_rate        | 0.0003      |
|    loss                 | 54.6        |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00273    |
|    std                  | 0.364       |
|    value_loss           | 110         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -1.1        |
| reward_distance         | 0.243       |
| reward_energy           | -0.00774    |
| reward_velocity         | -0.0133     |
| rollout/                |             |
|    ep_len_mean          | 125         |
|    ep_rew_mean          | -35.9       |
| time/                   |             |
|    fps                  | 29          |
|    iterations           | 27          |
|    time_elapsed         | 1891        |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.006920796 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.99       |
|    explained_variance   | 0.00253     |
|    learning_rate        | 0.0003      |
|    loss                 | 40.6        |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.000671   |
|    std                  | 0.364       |
|    value_loss           | 81.2        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -0.138      |
| reward_distance         | 0.234       |
| reward_energy           | -0.00576    |
| reward_velocity         | -0.00367    |
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | -32.2       |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 28          |
|    time_elapsed         | 1996        |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.009440535 |
|    clip_fraction        | 0.00156     |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.99       |
|    explained_variance   | -0.00364    |
|    learning_rate        | 0.0003      |
|    loss                 | 9.58        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.000989   |
|    std                  | 0.364       |
|    value_loss           | 19.2        |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.236      |
| reward_distance         | 0.24       |
| reward_energy           | -0.0034    |
| reward_velocity         | -5.42e-07  |
| rollout/                |            |
|    ep_len_mean          | 128        |
|    ep_rew_mean          | -33.8      |
| time/                   |            |
|    fps                  | 28         |
|    iterations           | 29         |
|    time_elapsed         | 2104       |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.00965628 |
|    clip_fraction        | 0.00371    |
|    clip_range           | 0.4        |
|    entropy_loss         | -7         |
|    explained_variance   | -0.00139   |
|    learning_rate        | 0.0003     |
|    loss                 | 35.4       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.000574  |
|    std                  | 0.364      |
|    value_loss           | 70.8       |
----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 0.246        |
| reward_distance         | 0.249        |
| reward_energy           | -0.00345     |
| reward_velocity         | -5.31e-07    |
| rollout/                |              |
|    ep_len_mean          | 128          |
|    ep_rew_mean          | -37.4        |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 30           |
|    time_elapsed         | 2214         |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0011905081 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -7.01        |
|    explained_variance   | -0.00147     |
|    learning_rate        | 0.0003       |
|    loss                 | 45.1         |
|    n_updates            | 290          |
|    policy_gradient_loss | 0.000147     |
|    std                  | 0.364        |
|    value_loss           | 90.2         |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.251       |
| reward_distance         | 0.255       |
| reward_energy           | -0.00368    |
| reward_velocity         | -9.34e-08   |
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | -35.2       |
| time/                   |             |
|    fps                  | 27          |
|    iterations           | 31          |
|    time_elapsed         | 2324        |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.004660168 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -7.01       |
|    explained_variance   | -0.00195    |
|    learning_rate        | 0.0003      |
|    loss                 | 16.4        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00163    |
|    std                  | 0.364       |
|    value_loss           | 32.7        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 0.244       |
| reward_distance         | 0.247       |
| reward_energy           | -0.00374    |
| reward_velocity         | -3.11e-08   |
| rollout/                |             |
|    ep_len_mean          | 128         |
|    ep_rew_mean          | -34.2       |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 32          |
|    time_elapsed         | 2433        |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.007862032 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -7          |
|    explained_variance   | -0.012      |
|    learning_rate        | 0.0003      |
|    loss                 | 31.8        |
|    n_updates            | 310         |
|    policy_gradient_loss | -0.0016     |
|    std                  | 0.364       |
|    value_loss           | 63.8        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -0.0379     |
| reward_distance         | 0.237       |
| reward_energy           | -0.00536    |
| reward_velocity         | -0.0027     |
| rollout/                |             |
|    ep_len_mean          | 127         |
|    ep_rew_mean          | -33.7       |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 33          |
|    time_elapsed         | 2541        |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.016649336 |
|    clip_fraction        | 0.00215     |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.99       |
|    explained_variance   | -0.000965   |
|    learning_rate        | 0.0003      |
|    loss                 | 22.6        |
|    n_updates            | 320         |
|    policy_gradient_loss | -0.00368    |
|    std                  | 0.364       |
|    value_loss           | 45.3        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -0.0323     |
| reward_distance         | 0.243       |
| reward_energy           | -0.00537    |
| reward_velocity         | -0.0027     |
| rollout/                |             |
|    ep_len_mean          | 127         |
|    ep_rew_mean          | -35.7       |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 34          |
|    time_elapsed         | 2646        |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.086604625 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.98       |
|    explained_variance   | 0.00215     |
|    learning_rate        | 0.0003      |
|    loss                 | 36.9        |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00698    |
|    std                  | 0.363       |
|    value_loss           | 73.9        |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -0.0297     |
| reward_distance         | 0.245       |
| reward_energy           | -0.00523    |
| reward_velocity         | -0.0027     |
| rollout/                |             |
|    ep_len_mean          | 127         |
|    ep_rew_mean          | -36         |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 35          |
|    time_elapsed         | 2747        |
|    total_timesteps      | 71680       |
| train/                  |             |
|    approx_kl            | 0.024989534 |
|    clip_fraction        | 0.0162      |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.94       |
|    explained_variance   | -0.0022     |
|    learning_rate        | 0.0003      |
|    loss                 | 30.5        |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.000534   |
|    std                  | 0.363       |
|    value_loss           | 61          |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -0.423      |
| reward_distance         | 0.235       |
| reward_energy           | -0.00724    |
| reward_velocity         | -0.0065     |
| rollout/                |             |
|    ep_len_mean          | 125         |
|    ep_rew_mean          | -30.2       |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 36          |
|    time_elapsed         | 2851        |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.003003115 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.92       |
|    explained_variance   | -0.00088    |
|    learning_rate        | 0.0003      |
|    loss                 | 19.3        |
|    n_updates            | 350         |
|    policy_gradient_loss | 0.000699    |
|    std                  | 0.363       |
|    value_loss           | 38.6        |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -0.425       |
| reward_distance         | 0.232        |
| reward_energy           | -0.00716     |
| reward_velocity         | -0.0065      |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | -33.7        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 37           |
|    time_elapsed         | 2959         |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0053263316 |
|    clip_fraction        | 0.000195     |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.92        |
|    explained_variance   | 0.0326       |
|    learning_rate        | 0.0003       |
|    loss                 | 27           |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00231     |
|    std                  | 0.363        |
|    value_loss           | 54.5         |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -0.967       |
| reward_distance         | 0.233        |
| reward_energy           | -0.0112      |
| reward_velocity         | -0.0119      |
| rollout/                |              |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | -37.2        |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 38           |
|    time_elapsed         | 3074         |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0019356229 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.93        |
|    explained_variance   | -0.000705    |
|    learning_rate        | 0.0003       |
|    loss                 | 63.9         |
|    n_updates            | 370          |
|    policy_gradient_loss | 0.000525     |
|    std                  | 0.363        |
|    value_loss           | 128          |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | -0.687        |
| reward_distance         | 0.241         |
| reward_energy           | -0.00952      |
| reward_velocity         | -0.00919      |
| rollout/                |               |
|    ep_len_mean          | 124           |
|    ep_rew_mean          | -37.9         |
| time/                   |               |
|    fps                  | 25            |
|    iterations           | 39            |
|    time_elapsed         | 3183          |
|    total_timesteps      | 79872         |
| train/                  |               |
|    approx_kl            | 0.00047204795 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -6.93         |
|    explained_variance   | 0.00118       |
|    learning_rate        | 0.0003        |
|    loss                 | 36.2          |
|    n_updates            | 380           |
|    policy_gradient_loss | -0.000143     |
|    std                  | 0.363         |
|    value_loss           | 72.4          |
-------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -2.13        |
| reward_distance         | 0.233        |
| reward_energy           | -0.0138      |
| reward_velocity         | -0.0235      |
| rollout/                |              |
|    ep_len_mean          | 122          |
|    ep_rew_mean          | -39.8        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 40           |
|    time_elapsed         | 3299         |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0017483195 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.94        |
|    explained_variance   | 0.000836     |
|    learning_rate        | 0.0003       |
|    loss                 | 29.8         |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.000809    |
|    std                  | 0.363        |
|    value_loss           | 59.6         |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -2.13       |
| reward_distance         | 0.228       |
| reward_energy           | -0.0137     |
| reward_velocity         | -0.0235     |
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | -37.6       |
| time/                   |             |
|    fps                  | 24          |
|    iterations           | 41          |
|    time_elapsed         | 3418        |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.018430792 |
|    clip_fraction        | 0.00605     |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.95       |
|    explained_variance   | 0.0447      |
|    learning_rate        | 0.0003      |
|    loss                 | 19.2        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00736    |
|    std                  | 0.362       |
|    value_loss           | 38.6        |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | -1.76      |
| reward_distance         | 0.221      |
| reward_energy           | -0.0117    |
| reward_velocity         | -0.0197    |
| rollout/                |            |
|    ep_len_mean          | 123        |
|    ep_rew_mean          | -42.2      |
| time/                   |            |
|    fps                  | 24         |
|    iterations           | 42         |
|    time_elapsed         | 3538       |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.01564635 |
|    clip_fraction        | 0.00449    |
|    clip_range           | 0.4        |
|    entropy_loss         | -6.94      |
|    explained_variance   | 0.00137    |
|    learning_rate        | 0.0003     |
|    loss                 | 38         |
|    n_updates            | 410        |
|    policy_gradient_loss | 0.00203    |
|    std                  | 0.362      |
|    value_loss           | 76.2       |
----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -2.3         |
| reward_distance         | 0.219        |
| reward_energy           | -0.017       |
| reward_velocity         | -0.025       |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | -40.1        |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 43           |
|    time_elapsed         | 3663         |
|    total_timesteps      | 88064        |
| train/                  |              |
|    approx_kl            | 0.0059556114 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.94        |
|    explained_variance   | -0.00147     |
|    learning_rate        | 0.0003       |
|    loss                 | 33.4         |
|    n_updates            | 420          |
|    policy_gradient_loss | -0.00114     |
|    std                  | 0.362        |
|    value_loss           | 66.8         |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | -1.77       |
| reward_distance         | 0.208       |
| reward_energy           | -0.0129     |
| reward_velocity         | -0.0196     |
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | -37.6       |
| time/                   |             |
|    fps                  | 23          |
|    iterations           | 44          |
|    time_elapsed         | 3784        |
|    total_timesteps      | 90112       |
| train/                  |             |
|    approx_kl            | 0.009584626 |
|    clip_fraction        | 0.00195     |
|    clip_range           | 0.4         |
|    entropy_loss         | -6.93       |
|    explained_variance   | 0.000631    |
|    learning_rate        | 0.0003      |
|    loss                 | 32.1        |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.000444   |
|    std                  | 0.362       |
|    value_loss           | 64.2        |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -1.76        |
| reward_distance         | 0.216        |
| reward_energy           | -0.0129      |
| reward_velocity         | -0.0196      |
| rollout/                |              |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | -37.3        |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 45           |
|    time_elapsed         | 3908         |
|    total_timesteps      | 92160        |
| train/                  |              |
|    approx_kl            | 0.0045535695 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.94        |
|    explained_variance   | 0.000371     |
|    learning_rate        | 0.0003       |
|    loss                 | 42.2         |
|    n_updates            | 440          |
|    policy_gradient_loss | -0.000209    |
|    std                  | 0.362        |
|    value_loss           | 84.5         |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | -0.326       |
| reward_distance         | 0.218        |
| reward_energy           | -0.00869     |
| reward_velocity         | -0.00535     |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | -36.7        |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 46           |
|    time_elapsed         | 4033         |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0012177867 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.93        |
|    explained_variance   | 0.0169       |
|    learning_rate        | 0.0003       |
|    loss                 | 45           |
|    n_updates            | 450          |
|    policy_gradient_loss | 0.00283      |
|    std                  | 0.363        |
|    value_loss           | 91.1         |
------------------------------------------
Num timesteps: 96000
Best mean reward: -inf - Last mean reward per episode: -40.25
Saving new best model to assets/out/models/exp1/best_model.zip
------------------------------------------
| penalty                 | 0            |
| reward                  | -0.308       |
| reward_distance         | 0.235        |
| reward_energy           | -0.00888     |
| reward_velocity         | -0.00535     |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | -40.7        |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 47           |
|    time_elapsed         | 4160         |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0013846214 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -6.94        |
|    explained_variance   | 0.000875     |
|    learning_rate        | 0.0003       |
|    loss                 | 39.2         |
|    n_updates            | 460          |
|    policy_gradient_loss | -8.1e-05     |
|    std                  | 0.363        |
|    value_loss           | 78.8         |
------------------------------------------
Terminated
running build_ext
Traceback (most recent call last):
  File "train.py", line 249, in <module>
    'track_lst' : ['joint_pos', 'action', 'velocity', 'position', 'true_joint_pos']
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 105, in make_vec_env
    return vec_env_cls([make_env(i + start_index) for i in range(n_envs)], **vec_env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/env_util.py", line 80, in _init
    env = gym.make(env_id, **env_kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 145, in make
    return registry.make(id, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 90, in make
    env = spec.make(**kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/envs/registration.py", line 60, in make
    env = cls(**_kwargs)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 106, in __init__
    observation, _reward, done, _info = self.step(action)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 306, in step
    reward, done, info = self.do_simulation(action, n_frames = self._frame_skip)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 481, in do_simulation
    reward_energy = np.exp(reward_energy)
FloatingPointError: underflow encountered in exp
running build_ext
Using cuda device
Logging to assets/out/models/exp1/PPO_42
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 128      |
|    ep_rew_mean     | 265      |
| time/              |          |
|    fps             | 35       |
|    iterations      | 1        |
|    time_elapsed    | 58       |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.22        |
| reward_distance         | 0.407       |
| reward_energy           | 0.847       |
| reward_velocity         | 0.964       |
| rollout/                |             |
|    ep_len_mean          | 124         |
|    ep_rew_mean          | 256         |
| time/                   |             |
|    fps                  | 36          |
|    iterations           | 2           |
|    time_elapsed         | 111         |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.052024912 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.4         |
|    entropy_loss         | -3.95       |
|    explained_variance   | 0.00133     |
|    learning_rate        | 0.0003      |
|    loss                 | 87          |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0165     |
|    std                  | 0.368       |
|    value_loss           | 248         |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 2.15       |
| reward_distance         | 0.346      |
| reward_energy           | 0.867      |
| reward_velocity         | 0.939      |
| rollout/                |            |
|    ep_len_mean          | 115        |
|    ep_rew_mean          | 238        |
| time/                   |            |
|    fps                  | 38         |
|    iterations           | 3          |
|    time_elapsed         | 158        |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.03759283 |
|    clip_fraction        | 0.0227     |
|    clip_range           | 0.4        |
|    entropy_loss         | -3.95      |
|    explained_variance   | -0.0161    |
|    learning_rate        | 0.0003     |
|    loss                 | 85.4       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0141    |
|    std                  | 0.368      |
|    value_loss           | 231        |
----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 2.13       |
| reward_distance         | 0.313      |
| reward_energy           | 0.86       |
| reward_velocity         | 0.953      |
| rollout/                |            |
|    ep_len_mean          | 118        |
|    ep_rew_mean          | 247        |
| time/                   |            |
|    fps                  | 39         |
|    iterations           | 4          |
|    time_elapsed         | 208        |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.04104991 |
|    clip_fraction        | 0.0411     |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.04      |
|    explained_variance   | -0.00946   |
|    learning_rate        | 0.0003     |
|    loss                 | 97.3       |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.00643   |
|    std                  | 0.368      |
|    value_loss           | 237        |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.16        |
| reward_distance         | 0.343       |
| reward_energy           | 0.857       |
| reward_velocity         | 0.958       |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 254         |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 5           |
|    time_elapsed         | 264         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.011738777 |
|    clip_fraction        | 0.00864     |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.12       |
|    explained_variance   | -0.000769   |
|    learning_rate        | 0.0003      |
|    loss                 | 125         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.000934   |
|    std                  | 0.368       |
|    value_loss           | 283         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.16        |
| reward_distance         | 0.326       |
| reward_energy           | 0.88        |
| reward_velocity         | 0.955       |
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 251         |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 6           |
|    time_elapsed         | 319         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.007712295 |
|    clip_fraction        | 0.00254     |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.16       |
|    explained_variance   | -0.000407   |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.00222    |
|    std                  | 0.368       |
|    value_loss           | 272         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.18         |
| reward_distance         | 0.331        |
| reward_energy           | 0.893        |
| reward_velocity         | 0.96         |
| rollout/                |              |
|    ep_len_mean          | 118          |
|    ep_rew_mean          | 253          |
| time/                   |              |
|    fps                  | 38           |
|    iterations           | 7            |
|    time_elapsed         | 376          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0078111337 |
|    clip_fraction        | 0.004        |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.22        |
|    explained_variance   | -0.000136    |
|    learning_rate        | 0.0003       |
|    loss                 | 129          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00209     |
|    std                  | 0.368        |
|    value_loss           | 283          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.17        |
| reward_distance         | 0.319       |
| reward_energy           | 0.896       |
| reward_velocity         | 0.951       |
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 247         |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 8           |
|    time_elapsed         | 431         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.008759293 |
|    clip_fraction        | 0.00342     |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.28       |
|    explained_variance   | -0.00114    |
|    learning_rate        | 0.0003      |
|    loss                 | 127         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.00172    |
|    std                  | 0.368       |
|    value_loss           | 276         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.17        |
| reward_distance         | 0.304       |
| reward_energy           | 0.91        |
| reward_velocity         | 0.961       |
| rollout/                |             |
|    ep_len_mean          | 117         |
|    ep_rew_mean          | 254         |
| time/                   |             |
|    fps                  | 37          |
|    iterations           | 9           |
|    time_elapsed         | 488         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.002268623 |
|    clip_fraction        | 0.000586    |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.32       |
|    explained_variance   | 0.00144     |
|    learning_rate        | 0.0003      |
|    loss                 | 113         |
|    n_updates            | 80          |
|    policy_gradient_loss | 0.000806    |
|    std                  | 0.368       |
|    value_loss           | 248         |
-----------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.18          |
| reward_distance         | 0.307         |
| reward_energy           | 0.917         |
| reward_velocity         | 0.958         |
| rollout/                |               |
|    ep_len_mean          | 115           |
|    ep_rew_mean          | 252           |
| time/                   |               |
|    fps                  | 38            |
|    iterations           | 10            |
|    time_elapsed         | 538           |
|    total_timesteps      | 20480         |
| train/                  |               |
|    approx_kl            | 0.00040889953 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -4.34         |
|    explained_variance   | 6.45e-05      |
|    learning_rate        | 0.0003        |
|    loss                 | 124           |
|    n_updates            | 90            |
|    policy_gradient_loss | -0.000308     |
|    std                  | 0.368         |
|    value_loss           | 270           |
-------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.16        |
| reward_distance         | 0.266       |
| reward_energy           | 0.937       |
| reward_velocity         | 0.956       |
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 249         |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 11          |
|    time_elapsed         | 587         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.002808498 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.32       |
|    explained_variance   | -3.46e-06   |
|    learning_rate        | 0.0003      |
|    loss                 | 114         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00131    |
|    std                  | 0.368       |
|    value_loss           | 249         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.14         |
| reward_distance         | 0.253        |
| reward_energy           | 0.934        |
| reward_velocity         | 0.955        |
| rollout/                |              |
|    ep_len_mean          | 114          |
|    ep_rew_mean          | 248          |
| time/                   |              |
|    fps                  | 38           |
|    iterations           | 12           |
|    time_elapsed         | 642          |
|    total_timesteps      | 24576        |
| train/                  |              |
|    approx_kl            | 0.0028306416 |
|    clip_fraction        | 0.000586     |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.32        |
|    explained_variance   | -0.000726    |
|    learning_rate        | 0.0003       |
|    loss                 | 118          |
|    n_updates            | 110          |
|    policy_gradient_loss | -0.00175     |
|    std                  | 0.368        |
|    value_loss           | 256          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.14        |
| reward_distance         | 0.25        |
| reward_energy           | 0.939       |
| reward_velocity         | 0.956       |
| rollout/                |             |
|    ep_len_mean          | 114         |
|    ep_rew_mean          | 247         |
| time/                   |             |
|    fps                  | 38          |
|    iterations           | 13          |
|    time_elapsed         | 699         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.004822959 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.37       |
|    explained_variance   | -9.24e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 119         |
|    n_updates            | 120         |
|    policy_gradient_loss | 0.000739    |
|    std                  | 0.368       |
|    value_loss           | 259         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.17         |
| reward_distance         | 0.248        |
| reward_energy           | 0.953        |
| reward_velocity         | 0.969        |
| rollout/                |              |
|    ep_len_mean          | 117          |
|    ep_rew_mean          | 254          |
| time/                   |              |
|    fps                  | 37           |
|    iterations           | 14           |
|    time_elapsed         | 761          |
|    total_timesteps      | 28672        |
| train/                  |              |
|    approx_kl            | 0.0012243736 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.37        |
|    explained_variance   | -0.000306    |
|    learning_rate        | 0.0003       |
|    loss                 | 116          |
|    n_updates            | 130          |
|    policy_gradient_loss | 0.000379     |
|    std                  | 0.368        |
|    value_loss           | 253          |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.14          |
| reward_distance         | 0.255         |
| reward_energy           | 0.934         |
| reward_velocity         | 0.955         |
| rollout/                |               |
|    ep_len_mean          | 113           |
|    ep_rew_mean          | 246           |
| time/                   |               |
|    fps                  | 37            |
|    iterations           | 15            |
|    time_elapsed         | 822           |
|    total_timesteps      | 30720         |
| train/                  |               |
|    approx_kl            | 1.1626515e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -4.38         |
|    explained_variance   | 4.04e-05      |
|    learning_rate        | 0.0003        |
|    loss                 | 121           |
|    n_updates            | 140           |
|    policy_gradient_loss | -3.3e-05      |
|    std                  | 0.368         |
|    value_loss           | 263           |
-------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.12         |
| reward_distance         | 0.248        |
| reward_energy           | 0.935        |
| reward_velocity         | 0.939        |
| rollout/                |              |
|    ep_len_mean          | 110          |
|    ep_rew_mean          | 240          |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 16           |
|    time_elapsed         | 886          |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0035321473 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.36        |
|    explained_variance   | 0.000104     |
|    learning_rate        | 0.0003       |
|    loss                 | 106          |
|    n_updates            | 150          |
|    policy_gradient_loss | -0.00233     |
|    std                  | 0.368        |
|    value_loss           | 231          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.13         |
| reward_distance         | 0.247        |
| reward_energy           | 0.932        |
| reward_velocity         | 0.948        |
| rollout/                |              |
|    ep_len_mean          | 114          |
|    ep_rew_mean          | 248          |
| time/                   |              |
|    fps                  | 36           |
|    iterations           | 17           |
|    time_elapsed         | 949          |
|    total_timesteps      | 34816        |
| train/                  |              |
|    approx_kl            | 0.0026659193 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.34        |
|    explained_variance   | 1.4e-05      |
|    learning_rate        | 0.0003       |
|    loss                 | 118          |
|    n_updates            | 160          |
|    policy_gradient_loss | 7.63e-05     |
|    std                  | 0.368        |
|    value_loss           | 256          |
------------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp1/PPO_43
---------------------------------
| penalty            | 0        |
| reward             | 0.183    |
| reward_distance    | 0.183    |
| reward_energy      | 9.16e-05 |
| reward_velocity    | 2.53e-22 |
| rollout/           |          |
|    ep_len_mean     | 113      |
|    ep_rew_mean     | 154      |
| time/              |          |
|    fps             | 37       |
|    iterations      | 1        |
|    time_elapsed    | 55       |
|    total_timesteps | 2048     |
---------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 1.35       |
| reward_distance         | 0.372      |
| reward_energy           | 0.357      |
| reward_velocity         | 0.625      |
| rollout/                |            |
|    ep_len_mean          | 116        |
|    ep_rew_mean          | 175        |
| time/                   |            |
|    fps                  | 33         |
|    iterations           | 2          |
|    time_elapsed         | 122        |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.31154275 |
|    clip_fraction        | 0.374      |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.04      |
|    explained_variance   | -0.00132   |
|    learning_rate        | 0.0003     |
|    loss                 | 31.3       |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0748    |
|    std                  | 0.368      |
|    value_loss           | 104        |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.53        |
| reward_distance         | 0.324       |
| reward_energy           | 0.499       |
| reward_velocity         | 0.711       |
| rollout/                |             |
|    ep_len_mean          | 118         |
|    ep_rew_mean          | 186         |
| time/                   |             |
|    fps                  | 31          |
|    iterations           | 3           |
|    time_elapsed         | 197         |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.051196523 |
|    clip_fraction        | 0.03        |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.05       |
|    explained_variance   | -0.0149     |
|    learning_rate        | 0.0003      |
|    loss                 | 54.9        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0217     |
|    std                  | 0.368       |
|    value_loss           | 153         |
-----------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 1.61       |
| reward_distance         | 0.318      |
| reward_energy           | 0.554      |
| reward_velocity         | 0.736      |
| rollout/                |            |
|    ep_len_mean          | 118        |
|    ep_rew_mean          | 197        |
| time/                   |            |
|    fps                  | 29         |
|    iterations           | 4          |
|    time_elapsed         | 280        |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.11467931 |
|    clip_fraction        | 0.0885     |
|    clip_range           | 0.4        |
|    entropy_loss         | -4.28      |
|    explained_variance   | -0.00723   |
|    learning_rate        | 0.0003     |
|    loss                 | 76.5       |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.024     |
|    std                  | 0.368      |
|    value_loss           | 186        |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.64        |
| reward_distance         | 0.308       |
| reward_energy           | 0.58        |
| reward_velocity         | 0.753       |
| rollout/                |             |
|    ep_len_mean          | 119         |
|    ep_rew_mean          | 204         |
| time/                   |             |
|    fps                  | 28          |
|    iterations           | 5           |
|    time_elapsed         | 362         |
|    total_timesteps      | 10240       |
| train/                  |             |
|    approx_kl            | 0.044034876 |
|    clip_fraction        | 0.0448      |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.5        |
|    explained_variance   | 0.00105     |
|    learning_rate        | 0.0003      |
|    loss                 | 96.5        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00293    |
|    std                  | 0.368       |
|    value_loss           | 220         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.68         |
| reward_distance         | 0.29         |
| reward_energy           | 0.601        |
| reward_velocity         | 0.79         |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 209          |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 6            |
|    time_elapsed         | 444          |
|    total_timesteps      | 12288        |
| train/                  |              |
|    approx_kl            | 0.0126169585 |
|    clip_fraction        | 0.0118       |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.64        |
|    explained_variance   | -0.000188    |
|    learning_rate        | 0.0003       |
|    loss                 | 97.5         |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00223     |
|    std                  | 0.368        |
|    value_loss           | 218          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.76         |
| reward_distance         | 0.291        |
| reward_energy           | 0.644        |
| reward_velocity         | 0.821        |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 223          |
| time/                   |              |
|    fps                  | 27           |
|    iterations           | 7            |
|    time_elapsed         | 527          |
|    total_timesteps      | 14336        |
| train/                  |              |
|    approx_kl            | 0.0071029933 |
|    clip_fraction        | 0.00371      |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.72        |
|    explained_variance   | 9.6e-06      |
|    learning_rate        | 0.0003       |
|    loss                 | 111          |
|    n_updates            | 60           |
|    policy_gradient_loss | -0.00307     |
|    std                  | 0.368        |
|    value_loss           | 245          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.85        |
| reward_distance         | 0.266       |
| reward_energy           | 0.726       |
| reward_velocity         | 0.854       |
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | 234         |
| time/                   |             |
|    fps                  | 26          |
|    iterations           | 8           |
|    time_elapsed         | 610         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.004928991 |
|    clip_fraction        | 0.00107     |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.76       |
|    explained_variance   | 0.00049     |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0005     |
|    std                  | 0.368       |
|    value_loss           | 230         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.85         |
| reward_distance         | 0.252        |
| reward_energy           | 0.746        |
| reward_velocity         | 0.854        |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 237          |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 9            |
|    time_elapsed         | 694          |
|    total_timesteps      | 18432        |
| train/                  |              |
|    approx_kl            | 0.0042718034 |
|    clip_fraction        | 0.00156      |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.81        |
|    explained_variance   | 0.000119     |
|    learning_rate        | 0.0003       |
|    loss                 | 98.3         |
|    n_updates            | 80           |
|    policy_gradient_loss | -0.00178     |
|    std                  | 0.368        |
|    value_loss           | 217          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.89         |
| reward_distance         | 0.231        |
| reward_energy           | 0.782        |
| reward_velocity         | 0.877        |
| rollout/                |              |
|    ep_len_mean          | 122          |
|    ep_rew_mean          | 242          |
| time/                   |              |
|    fps                  | 26           |
|    iterations           | 10           |
|    time_elapsed         | 780          |
|    total_timesteps      | 20480        |
| train/                  |              |
|    approx_kl            | 0.0070406077 |
|    clip_fraction        | 0.0022       |
|    clip_range           | 0.4          |
|    entropy_loss         | -4.87        |
|    explained_variance   | 0.000664     |
|    learning_rate        | 0.0003       |
|    loss                 | 104          |
|    n_updates            | 90           |
|    policy_gradient_loss | -0.00354     |
|    std                  | 0.368        |
|    value_loss           | 228          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.97        |
| reward_distance         | 0.234       |
| reward_energy           | 0.83        |
| reward_velocity         | 0.911       |
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 249         |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 11          |
|    time_elapsed         | 867         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.009086771 |
|    clip_fraction        | 0.0064      |
|    clip_range           | 0.4         |
|    entropy_loss         | -4.95       |
|    explained_variance   | 0.000268    |
|    learning_rate        | 0.0003      |
|    loss                 | 109         |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00197    |
|    std                  | 0.368       |
|    value_loss           | 239         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.01        |
| reward_distance         | 0.235       |
| reward_energy           | 0.863       |
| reward_velocity         | 0.911       |
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 249         |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 12          |
|    time_elapsed         | 955         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.008008618 |
|    clip_fraction        | 0.00488     |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.01       |
|    explained_variance   | -0.000253   |
|    learning_rate        | 0.0003      |
|    loss                 | 106         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.00449    |
|    std                  | 0.368       |
|    value_loss           | 231         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.99         |
| reward_distance         | 0.221        |
| reward_energy           | 0.864        |
| reward_velocity         | 0.91         |
| rollout/                |              |
|    ep_len_mean          | 124          |
|    ep_rew_mean          | 253          |
| time/                   |              |
|    fps                  | 25           |
|    iterations           | 13           |
|    time_elapsed         | 1044         |
|    total_timesteps      | 26624        |
| train/                  |              |
|    approx_kl            | 0.0076713064 |
|    clip_fraction        | 0.00352      |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.07        |
|    explained_variance   | 0.000374     |
|    learning_rate        | 0.0003       |
|    loss                 | 103          |
|    n_updates            | 120          |
|    policy_gradient_loss | -0.0033      |
|    std                  | 0.368        |
|    value_loss           | 226          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.02        |
| reward_distance         | 0.253       |
| reward_energy           | 0.849       |
| reward_velocity         | 0.918       |
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 253         |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 14          |
|    time_elapsed         | 1134        |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.023611946 |
|    clip_fraction        | 0.0225      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.14       |
|    explained_variance   | 0.000923    |
|    learning_rate        | 0.0003      |
|    loss                 | 123         |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.0107     |
|    std                  | 0.368       |
|    value_loss           | 268         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.99        |
| reward_distance         | 0.259       |
| reward_energy           | 0.824       |
| reward_velocity         | 0.91        |
| rollout/                |             |
|    ep_len_mean          | 122         |
|    ep_rew_mean          | 254         |
| time/                   |             |
|    fps                  | 25          |
|    iterations           | 15          |
|    time_elapsed         | 1225        |
|    total_timesteps      | 30720       |
| train/                  |             |
|    approx_kl            | 0.014068341 |
|    clip_fraction        | 0.0118      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.23       |
|    explained_variance   | 0.000166    |
|    learning_rate        | 0.0003      |
|    loss                 | 104         |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00124    |
|    std                  | 0.368       |
|    value_loss           | 226         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2            |
| reward_distance         | 0.266        |
| reward_energy           | 0.822        |
| reward_velocity         | 0.913        |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 251          |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 16           |
|    time_elapsed         | 1317         |
|    total_timesteps      | 32768        |
| train/                  |              |
|    approx_kl            | 0.0031103615 |
|    clip_fraction        | 0.000781     |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.29        |
|    explained_variance   | 0.000215     |
|    learning_rate        | 0.0003       |
|    loss                 | 107          |
|    n_updates            | 150          |
|    policy_gradient_loss | 0.0014       |
|    std                  | 0.368        |
|    value_loss           | 233          |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 1.99          |
| reward_distance         | 0.256         |
| reward_energy           | 0.821         |
| reward_velocity         | 0.912         |
| rollout/                |               |
|    ep_len_mean          | 120           |
|    ep_rew_mean          | 251           |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 17            |
|    time_elapsed         | 1408          |
|    total_timesteps      | 34816         |
| train/                  |               |
|    approx_kl            | 0.00042380753 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.31         |
|    explained_variance   | 3.08e-05      |
|    learning_rate        | 0.0003        |
|    loss                 | 104           |
|    n_updates            | 160           |
|    policy_gradient_loss | 0.000147      |
|    std                  | 0.368         |
|    value_loss           | 227           |
-------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.01         |
| reward_distance         | 0.262        |
| reward_energy           | 0.825        |
| reward_velocity         | 0.922        |
| rollout/                |              |
|    ep_len_mean          | 122          |
|    ep_rew_mean          | 256          |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 18           |
|    time_elapsed         | 1501         |
|    total_timesteps      | 36864        |
| train/                  |              |
|    approx_kl            | 0.0011510926 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.3         |
|    explained_variance   | 2.43e-05     |
|    learning_rate        | 0.0003       |
|    loss                 | 107          |
|    n_updates            | 170          |
|    policy_gradient_loss | -0.00179     |
|    std                  | 0.367        |
|    value_loss           | 232          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.04         |
| reward_distance         | 0.271        |
| reward_energy           | 0.837        |
| reward_velocity         | 0.93         |
| rollout/                |              |
|    ep_len_mean          | 120          |
|    ep_rew_mean          | 254          |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 19           |
|    time_elapsed         | 1598         |
|    total_timesteps      | 38912        |
| train/                  |              |
|    approx_kl            | 0.0015472329 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.28        |
|    explained_variance   | -7.51e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 109          |
|    n_updates            | 180          |
|    policy_gradient_loss | -0.000484    |
|    std                  | 0.367        |
|    value_loss           | 237          |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.03          |
| reward_distance         | 0.234         |
| reward_energy           | 0.859         |
| reward_velocity         | 0.94          |
| rollout/                |               |
|    ep_len_mean          | 122           |
|    ep_rew_mean          | 256           |
| time/                   |               |
|    fps                  | 24            |
|    iterations           | 20            |
|    time_elapsed         | 1693          |
|    total_timesteps      | 40960         |
| train/                  |               |
|    approx_kl            | 0.00010706851 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.26         |
|    explained_variance   | 1.25e-05      |
|    learning_rate        | 0.0003        |
|    loss                 | 93.6          |
|    n_updates            | 190           |
|    policy_gradient_loss | 1.05e-05      |
|    std                  | 0.367         |
|    value_loss           | 204           |
-------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.1          |
| reward_distance         | 0.238        |
| reward_energy           | 0.897        |
| reward_velocity         | 0.97         |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | 263          |
| time/                   |              |
|    fps                  | 24           |
|    iterations           | 21           |
|    time_elapsed         | 1787         |
|    total_timesteps      | 43008        |
| train/                  |              |
|    approx_kl            | 0.0003156883 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.26        |
|    explained_variance   | 9.36e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 102          |
|    n_updates            | 200          |
|    policy_gradient_loss | -0.00016     |
|    std                  | 0.367        |
|    value_loss           | 221          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.08         |
| reward_distance         | 0.223        |
| reward_energy           | 0.888        |
| reward_velocity         | 0.97         |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | 263          |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 22           |
|    time_elapsed         | 1884         |
|    total_timesteps      | 45056        |
| train/                  |              |
|    approx_kl            | 0.0013385658 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.27        |
|    explained_variance   | 0.000167     |
|    learning_rate        | 0.0003       |
|    loss                 | 96.4         |
|    n_updates            | 210          |
|    policy_gradient_loss | -0.000417    |
|    std                  | 0.367        |
|    value_loss           | 210          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.08         |
| reward_distance         | 0.222        |
| reward_energy           | 0.887        |
| reward_velocity         | 0.97         |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | 262          |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 23           |
|    time_elapsed         | 1980         |
|    total_timesteps      | 47104        |
| train/                  |              |
|    approx_kl            | 0.0004977549 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.29        |
|    explained_variance   | 6.38e-05     |
|    learning_rate        | 0.0003       |
|    loss                 | 101          |
|    n_updates            | 220          |
|    policy_gradient_loss | 0.000312     |
|    std                  | 0.367        |
|    value_loss           | 219          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.03         |
| reward_distance         | 0.22         |
| reward_energy           | 0.859        |
| reward_velocity         | 0.95         |
| rollout/                |              |
|    ep_len_mean          | 124          |
|    ep_rew_mean          | 259          |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 24           |
|    time_elapsed         | 2076         |
|    total_timesteps      | 49152        |
| train/                  |              |
|    approx_kl            | 0.0002379478 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.29        |
|    explained_variance   | 0.000198     |
|    learning_rate        | 0.0003       |
|    loss                 | 99.6         |
|    n_updates            | 230          |
|    policy_gradient_loss | -0.000193    |
|    std                  | 0.367        |
|    value_loss           | 216          |
------------------------------------------
----------------------------------------
| penalty                 | 0          |
| reward                  | 2          |
| reward_distance         | 0.216      |
| reward_energy           | 0.848      |
| reward_velocity         | 0.941      |
| rollout/                |            |
|    ep_len_mean          | 124        |
|    ep_rew_mean          | 259        |
| time/                   |            |
|    fps                  | 23         |
|    iterations           | 25         |
|    time_elapsed         | 2173       |
|    total_timesteps      | 51200      |
| train/                  |            |
|    approx_kl            | 0.00527371 |
|    clip_fraction        | 0.000342   |
|    clip_range           | 0.4        |
|    entropy_loss         | -5.3       |
|    explained_variance   | 0.000157   |
|    learning_rate        | 0.0003     |
|    loss                 | 103        |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.00364   |
|    std                  | 0.367      |
|    value_loss           | 224        |
----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2           |
| reward_distance         | 0.23        |
| reward_energy           | 0.841       |
| reward_velocity         | 0.931       |
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | 258         |
| time/                   |             |
|    fps                  | 23          |
|    iterations           | 26          |
|    time_elapsed         | 2272        |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.016144678 |
|    clip_fraction        | 0.0101      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.34       |
|    explained_variance   | 3.38e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 100         |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00541    |
|    std                  | 0.367       |
|    value_loss           | 218         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.98        |
| reward_distance         | 0.236       |
| reward_energy           | 0.83        |
| reward_velocity         | 0.911       |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 254         |
| time/                   |             |
|    fps                  | 23          |
|    iterations           | 27          |
|    time_elapsed         | 2371        |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.023421641 |
|    clip_fraction        | 0.0238      |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.43       |
|    explained_variance   | 2.96e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 260         |
|    policy_gradient_loss | -0.00758    |
|    std                  | 0.367       |
|    value_loss           | 219         |
-----------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2           |
| reward_distance         | 0.245       |
| reward_energy           | 0.84        |
| reward_velocity         | 0.91        |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 256         |
| time/                   |             |
|    fps                  | 23          |
|    iterations           | 28          |
|    time_elapsed         | 2469        |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.003743238 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.5        |
|    explained_variance   | 1.43e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.00157     |
|    std                  | 0.367       |
|    value_loss           | 224         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 1.94         |
| reward_distance         | 0.243        |
| reward_energy           | 0.813        |
| reward_velocity         | 0.88         |
| rollout/                |              |
|    ep_len_mean          | 117          |
|    ep_rew_mean          | 249          |
| time/                   |              |
|    fps                  | 23           |
|    iterations           | 29           |
|    time_elapsed         | 2573         |
|    total_timesteps      | 59392        |
| train/                  |              |
|    approx_kl            | 0.0003148093 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.51        |
|    explained_variance   | 1.5e-05      |
|    learning_rate        | 0.0003       |
|    loss                 | 98.1         |
|    n_updates            | 280          |
|    policy_gradient_loss | -0.000182    |
|    std                  | 0.367        |
|    value_loss           | 213          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2            |
| reward_distance         | 0.254        |
| reward_energy           | 0.846        |
| reward_velocity         | 0.9          |
| rollout/                |              |
|    ep_len_mean          | 115          |
|    ep_rew_mean          | 248          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 30           |
|    time_elapsed         | 2674         |
|    total_timesteps      | 61440        |
| train/                  |              |
|    approx_kl            | 0.0010481661 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.5         |
|    explained_variance   | 3.21e-05     |
|    learning_rate        | 0.0003       |
|    loss                 | 110          |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.000952    |
|    std                  | 0.367        |
|    value_loss           | 236          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 1.99        |
| reward_distance         | 0.243       |
| reward_energy           | 0.845       |
| reward_velocity         | 0.9         |
| rollout/                |             |
|    ep_len_mean          | 115         |
|    ep_rew_mean          | 246         |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 31          |
|    time_elapsed         | 2775        |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.011634488 |
|    clip_fraction        | 0.00625     |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.55       |
|    explained_variance   | 2.37e-05    |
|    learning_rate        | 0.0003      |
|    loss                 | 96.9        |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.00329    |
|    std                  | 0.367       |
|    value_loss           | 209         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2            |
| reward_distance         | 0.243        |
| reward_energy           | 0.852        |
| reward_velocity         | 0.91         |
| rollout/                |              |
|    ep_len_mean          | 117          |
|    ep_rew_mean          | 249          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 32           |
|    time_elapsed         | 2875         |
|    total_timesteps      | 65536        |
| train/                  |              |
|    approx_kl            | 0.0078119934 |
|    clip_fraction        | 0.00332      |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.6         |
|    explained_variance   | 0.000108     |
|    learning_rate        | 0.0003       |
|    loss                 | 100          |
|    n_updates            | 310          |
|    policy_gradient_loss | -0.00181     |
|    std                  | 0.367        |
|    value_loss           | 216          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.04         |
| reward_distance         | 0.238        |
| reward_energy           | 0.873        |
| reward_velocity         | 0.93         |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 254          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 33           |
|    time_elapsed         | 2971         |
|    total_timesteps      | 67584        |
| train/                  |              |
|    approx_kl            | 0.0015848906 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.63        |
|    explained_variance   | 8.4e-06      |
|    learning_rate        | 0.0003       |
|    loss                 | 93.5         |
|    n_updates            | 320          |
|    policy_gradient_loss | 0.000204     |
|    std                  | 0.367        |
|    value_loss           | 202          |
------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.06          |
| reward_distance         | 0.236         |
| reward_energy           | 0.88          |
| reward_velocity         | 0.94          |
| rollout/                |               |
|    ep_len_mean          | 120           |
|    ep_rew_mean          | 257           |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 34            |
|    time_elapsed         | 3071          |
|    total_timesteps      | 69632         |
| train/                  |               |
|    approx_kl            | 0.00070411124 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.64         |
|    explained_variance   | 4.41e-06      |
|    learning_rate        | 0.0003        |
|    loss                 | 98.2          |
|    n_updates            | 330           |
|    policy_gradient_loss | -0.000533     |
|    std                  | 0.367         |
|    value_loss           | 212           |
-------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.12         |
| reward_distance         | 0.241        |
| reward_energy           | 0.907        |
| reward_velocity         | 0.97         |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | 268          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 35           |
|    time_elapsed         | 3172         |
|    total_timesteps      | 71680        |
| train/                  |              |
|    approx_kl            | 0.0027375184 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.66        |
|    explained_variance   | 5.13e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 102          |
|    n_updates            | 340          |
|    policy_gradient_loss | -0.00128     |
|    std                  | 0.367        |
|    value_loss           | 219          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.13        |
| reward_distance         | 0.23        |
| reward_energy           | 0.917       |
| reward_velocity         | 0.98        |
| rollout/                |             |
|    ep_len_mean          | 125         |
|    ep_rew_mean          | 267         |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 36          |
|    time_elapsed         | 3274        |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.003889226 |
|    clip_fraction        | 0           |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.68       |
|    explained_variance   | -2.38e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 98.7        |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.00167    |
|    std                  | 0.367       |
|    value_loss           | 213         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.15         |
| reward_distance         | 0.233        |
| reward_energy           | 0.931        |
| reward_velocity         | 0.99         |
| rollout/                |              |
|    ep_len_mean          | 127          |
|    ep_rew_mean          | 272          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 37           |
|    time_elapsed         | 3369         |
|    total_timesteps      | 75776        |
| train/                  |              |
|    approx_kl            | 0.0032202797 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.68        |
|    explained_variance   | 1.02e-05     |
|    learning_rate        | 0.0003       |
|    loss                 | 102          |
|    n_updates            | 360          |
|    policy_gradient_loss | -0.00135     |
|    std                  | 0.367        |
|    value_loss           | 220          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.15         |
| reward_distance         | 0.244        |
| reward_energy           | 0.923        |
| reward_velocity         | 0.98         |
| rollout/                |              |
|    ep_len_mean          | 127          |
|    ep_rew_mean          | 273          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 38           |
|    time_elapsed         | 3467         |
|    total_timesteps      | 77824        |
| train/                  |              |
|    approx_kl            | 0.0008607778 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.69        |
|    explained_variance   | -4.77e-07    |
|    learning_rate        | 0.0003       |
|    loss                 | 105          |
|    n_updates            | 370          |
|    policy_gradient_loss | 0.000185     |
|    std                  | 0.367        |
|    value_loss           | 225          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.13         |
| reward_distance         | 0.249        |
| reward_energy           | 0.912        |
| reward_velocity         | 0.97         |
| rollout/                |              |
|    ep_len_mean          | 125          |
|    ep_rew_mean          | 271          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 39           |
|    time_elapsed         | 3562         |
|    total_timesteps      | 79872        |
| train/                  |              |
|    approx_kl            | 0.0023699482 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.69        |
|    explained_variance   | 6.68e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 99.5         |
|    n_updates            | 380          |
|    policy_gradient_loss | -0.00194     |
|    std                  | 0.367        |
|    value_loss           | 214          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.11         |
| reward_distance         | 0.246        |
| reward_energy           | 0.906        |
| reward_velocity         | 0.96         |
| rollout/                |              |
|    ep_len_mean          | 124          |
|    ep_rew_mean          | 268          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 40           |
|    time_elapsed         | 3659         |
|    total_timesteps      | 81920        |
| train/                  |              |
|    approx_kl            | 0.0067961975 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.72        |
|    explained_variance   | 8.05e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 99           |
|    n_updates            | 390          |
|    policy_gradient_loss | -0.00282     |
|    std                  | 0.367        |
|    value_loss           | 212          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.11        |
| reward_distance         | 0.242       |
| reward_energy           | 0.907       |
| reward_velocity         | 0.96        |
| rollout/                |             |
|    ep_len_mean          | 124         |
|    ep_rew_mean          | 267         |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 41          |
|    time_elapsed         | 3753        |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.005044652 |
|    clip_fraction        | 0.000781    |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.76       |
|    explained_variance   | 3.22e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 101         |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00162    |
|    std                  | 0.367       |
|    value_loss           | 216         |
-----------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.09         |
| reward_distance         | 0.239        |
| reward_energy           | 0.899        |
| reward_velocity         | 0.95         |
| rollout/                |              |
|    ep_len_mean          | 123          |
|    ep_rew_mean          | 265          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 42           |
|    time_elapsed         | 3846         |
|    total_timesteps      | 86016        |
| train/                  |              |
|    approx_kl            | 0.0033637309 |
|    clip_fraction        | 0.000391     |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.77        |
|    explained_variance   | -1.84e-05    |
|    learning_rate        | 0.0003       |
|    loss                 | 100          |
|    n_updates            | 410          |
|    policy_gradient_loss | -0.00148     |
|    std                  | 0.367        |
|    value_loss           | 214          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.04        |
| reward_distance         | 0.229       |
| reward_energy           | 0.883       |
| reward_velocity         | 0.93        |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 258         |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 43          |
|    time_elapsed         | 3940        |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.003298482 |
|    clip_fraction        | 0.000195    |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.79       |
|    explained_variance   | 1.49e-06    |
|    learning_rate        | 0.0003      |
|    loss                 | 102         |
|    n_updates            | 420         |
|    policy_gradient_loss | -0.00125    |
|    std                  | 0.367       |
|    value_loss           | 218         |
-----------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.05          |
| reward_distance         | 0.217         |
| reward_energy           | 0.894         |
| reward_velocity         | 0.939         |
| rollout/                |               |
|    ep_len_mean          | 120           |
|    ep_rew_mean          | 257           |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 44            |
|    time_elapsed         | 4027          |
|    total_timesteps      | 90112         |
| train/                  |               |
|    approx_kl            | 3.4070225e-05 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.81         |
|    explained_variance   | -2.38e-07     |
|    learning_rate        | 0.0003        |
|    loss                 | 102           |
|    n_updates            | 430           |
|    policy_gradient_loss | 0.000362      |
|    std                  | 0.367         |
|    value_loss           | 216           |
-------------------------------------------
-------------------------------------------
| penalty                 | 0             |
| reward                  | 2.04          |
| reward_distance         | 0.208         |
| reward_energy           | 0.897         |
| reward_velocity         | 0.939         |
| rollout/                |               |
|    ep_len_mean          | 120           |
|    ep_rew_mean          | 256           |
| time/                   |               |
|    fps                  | 22            |
|    iterations           | 45            |
|    time_elapsed         | 4117          |
|    total_timesteps      | 92160         |
| train/                  |               |
|    approx_kl            | 0.00092749717 |
|    clip_fraction        | 0             |
|    clip_range           | 0.4           |
|    entropy_loss         | -5.8          |
|    explained_variance   | -1.67e-06     |
|    learning_rate        | 0.0003        |
|    loss                 | 102           |
|    n_updates            | 440           |
|    policy_gradient_loss | -1.04e-05     |
|    std                  | 0.367         |
|    value_loss           | 217           |
-------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.02         |
| reward_distance         | 0.198        |
| reward_energy           | 0.89         |
| reward_velocity         | 0.929        |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 252          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 46           |
|    time_elapsed         | 4212         |
|    total_timesteps      | 94208        |
| train/                  |              |
|    approx_kl            | 0.0013903889 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.79        |
|    explained_variance   | 3.87e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 100          |
|    n_updates            | 450          |
|    policy_gradient_loss | -0.000382    |
|    std                  | 0.367        |
|    value_loss           | 212          |
------------------------------------------
Num timesteps: 96000
Best mean reward: -inf - Last mean reward per episode: 252.84
Saving new best model to assets/out/models/exp1/best_model.zip
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.02         |
| reward_distance         | 0.205        |
| reward_energy           | 0.887        |
| reward_velocity         | 0.929        |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 253          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 47           |
|    time_elapsed         | 4303         |
|    total_timesteps      | 96256        |
| train/                  |              |
|    approx_kl            | 0.0006834731 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.78        |
|    explained_variance   | -3.22e-06    |
|    learning_rate        | 0.0003       |
|    loss                 | 115          |
|    n_updates            | 460          |
|    policy_gradient_loss | -6.81e-05    |
|    std                  | 0.367        |
|    value_loss           | 242          |
------------------------------------------
------------------------------------------
| penalty                 | 0            |
| reward                  | 2.01         |
| reward_distance         | 0.212        |
| reward_energy           | 0.884        |
| reward_velocity         | 0.92         |
| rollout/                |              |
|    ep_len_mean          | 119          |
|    ep_rew_mean          | 253          |
| time/                   |              |
|    fps                  | 22           |
|    iterations           | 48           |
|    time_elapsed         | 4395         |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 8.423676e-05 |
|    clip_fraction        | 0            |
|    clip_range           | 0.4          |
|    entropy_loss         | -5.78        |
|    explained_variance   | 2.98e-06     |
|    learning_rate        | 0.0003       |
|    loss                 | 112          |
|    n_updates            | 470          |
|    policy_gradient_loss | -1.71e-05    |
|    std                  | 0.367        |
|    value_loss           | 236          |
------------------------------------------
-----------------------------------------
| penalty                 | 0           |
| reward                  | 2.03        |
| reward_distance         | 0.204       |
| reward_energy           | 0.893       |
| reward_velocity         | 0.929       |
| rollout/                |             |
|    ep_len_mean          | 120         |
|    ep_rew_mean          | 255         |
| time/                   |             |
|    fps                  | 22          |
|    iterations           | 49          |
|    time_elapsed         | 4484        |
|    total_timesteps      | 100352      |
| train/                  |             |
|    approx_kl            | 0.005239148 |
|    clip_fraction        | 0.000977    |
|    clip_range           | 0.4         |
|    entropy_loss         | -5.79       |
|    explained_variance   | -7.15e-07   |
|    learning_rate        | 0.0003      |
|    loss                 | 110         |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00188    |
|    std                  | 0.367       |
|    value_loss           | 230         |
-----------------------------------------
Traceback (most recent call last):
  File "train.py", line 399, in <module>
    print(stable_baselines3.common.evaluation.evaluate_policy(model, env, render=True))
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/evaluation.py", line 123, in evaluate_policy
    env.render()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 87, in render
    return super().render(mode=mode)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 178, in render
    imgs = self.get_images()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 70, in get_images
    return [env.render(mode="rgb_array") for env in self.envs]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 70, in <listcomp>
    return [env.render(mode="rgb_array") for env in self.envs]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/core.py", line 240, in render
    return self.env.render(mode, **kwargs)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/core.py", line 240, in render
    return self.env.render(mode, **kwargs)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 516, in render
    self._get_viewer(mode).render(width, height, camera_id=camera_id)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 545, in _get_viewer
    self.viewer = mujoco_py.MjRenderContextOffscreen(self.sim, -1)
AttributeError: module 'mujoco_py' has no attribute 'MjRenderContextOffscreen'
