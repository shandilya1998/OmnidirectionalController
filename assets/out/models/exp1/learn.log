running build_ext
Using cuda device
Logging to assets/out/models/exp2/PPO_2
running build_ext
Using cuda device
Traceback (most recent call last):
  File "train.py", line 284, in <module>
    train_freq = params['TRAIN_FREQ'],
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 115, in __init__
    supported_action_spaces=(gym.spaces.Box),
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 125, in __init__
    supported_action_spaces=supported_action_spaces,
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/base_class.py", line 170, in __init__
    "Error: the model does not support multiple envs; it requires " "a single vectorized environment."
ValueError: Error: the model does not support multiple envs; it requires a single vectorized environment.
---------------------------------
| penalty            | 0        |
| reward             | 0.999    |
| reward_distance    | 5.83e-05 |
| reward_energy      | 7.47e-40 |
| reward_velocity    | 0.999    |
| rollout/           |          |
|    ep_len_mean     | 14.5     |
|    ep_rew_mean     | 13.1     |
| time/              |          |
|    fps             | 13       |
|    iterations      | 1        |
|    time_elapsed    | 149      |
|    total_timesteps | 2048     |
---------------------------------
running build_ext
Using cuda device
Traceback (most recent call last):
  File "train.py", line 284, in <module>
    train_freq = params['TRAIN_FREQ'],
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 123, in __init__
    self._setup_model()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 126, in _setup_model
    super(TD3, self)._setup_model()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 226, in _setup_model
    **self.policy_kwargs,  # pytype:disable=not-instantiable
TypeError: __init__() got an unexpected keyword argument 'log_std_init'
running build_ext
Using cuda device
Traceback (most recent call last):
  File "train.py", line 285, in <module>
    train_freq = params['TRAIN_FREQ'],
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 123, in __init__
    self._setup_model()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 126, in _setup_model
    super(TD3, self)._setup_model()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 226, in _setup_model
    **self.policy_kwargs,  # pytype:disable=not-instantiable
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/policies.py", line 336, in __init__
    share_features_extractor,
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/policies.py", line 166, in __init__
    self._build(lr_schedule)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/policies.py", line 171, in _build
    self.actor = self.make_actor(features_extractor=None)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/policies.py", line 214, in make_actor
    return Actor(**actor_kwargs).to(self.device)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/policies.py", line 58, in __init__
    actor_net = create_mlp(features_dim, action_dim, net_arch, activation_fn, squash_output=True)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/torch_layers.py", line 120, in create_mlp
    modules = [nn.Linear(input_dim, net_arch[0]), activation_fn()]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py", line 78, in __init__
    self.weight = Parameter(torch.Tensor(out_features, in_features))
TypeError: new() received an invalid combination of arguments - got (dict, int), but expected one of:
 * (*, torch.device device)
      didn't match because some of the arguments have invalid types: (!dict!, !int!)
 * (torch.Storage storage)
 * (Tensor other)
 * (tuple of ints size, *, torch.device device)
 * (object data, *, torch.device device)

running build_ext
Using cuda device
Logging to assets/out/models/exp2/TD3_1
---------------------------------
| penalty            | 0        |
| reward             | 0.987    |
| reward_distance    | 0.197    |
| reward_energy      | 0.124    |
| reward_velocity    | 0.667    |
| rollout/           |          |
|    ep_len_mean     | 1.75     |
|    ep_rew_mean     | 1.75     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 35       |
|    time_elapsed    | 0        |
|    total timesteps | 7        |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.88     |
|    ep_rew_mean     | 1.87     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 49       |
|    time_elapsed    | 0        |
|    total timesteps | 15       |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.83     |
|    ep_rew_mean     | 1.83     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 52       |
|    time_elapsed    | 0        |
|    total timesteps | 22       |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.81     |
|    ep_rew_mean     | 1.81     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 53       |
|    time_elapsed    | 0        |
|    total timesteps | 29       |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.85     |
|    ep_rew_mean     | 1.84     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 57       |
|    time_elapsed    | 0        |
|    total timesteps | 37       |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.75     |
|    ep_rew_mean     | 1.75     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 55       |
|    time_elapsed    | 0        |
|    total timesteps | 42       |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1.75     |
|    ep_rew_mean     | 1.75     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 55       |
|    time_elapsed    | 0        |
|    total timesteps | 49       |
---------------------------------
---------------------------------
| penalty            | 0        |
| reward             | 0.992    |
| reward_distance    | 0.107    |
| reward_energy      | 0.0676   |
| reward_velocity    | 0.817    |
| rollout/           |          |
|    ep_len_mean     | 5.66     |
|    ep_rew_mean     | 2.77     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 48       |
|    time_elapsed    | 3        |
|    total timesteps | 181      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.25     |
|    ep_rew_mean     | 2.69     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 48       |
|    time_elapsed    | 3        |
|    total timesteps | 189      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 4.9      |
|    ep_rew_mean     | 2.59     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 48       |
|    time_elapsed    | 4        |
|    total timesteps | 196      |
---------------------------------
---------------------------------
| penalty            | 0        |
| reward             | 0.985    |
| reward_distance    | 0.0972   |
| reward_energy      | 0.0572   |
| reward_velocity    | 0.831    |
| rollout/           |          |
|    ep_len_mean     | 7.5      |
|    ep_rew_mean     | 3.96     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 48       |
|    time_elapsed    | 6        |
|    total timesteps | 330      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.04     |
|    ep_rew_mean     | 3.8      |
| time/              |          |
|    episodes        | 48       |
|    fps             | 48       |
|    time_elapsed    | 6        |
|    total timesteps | 338      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.62     |
|    ep_rew_mean     | 3.62     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 48       |
|    time_elapsed    | 7        |
|    total timesteps | 344      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.27     |
|    ep_rew_mean     | 3.49     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 48       |
|    time_elapsed    | 7        |
|    total timesteps | 351      |
---------------------------------
---------------------------------
| penalty            | 0        |
| reward             | 0.984    |
| reward_distance    | 0.0889   |
| reward_energy      | 0.048    |
| reward_velocity    | 0.847    |
| rollout/           |          |
|    ep_len_mean     | 8.07     |
|    ep_rew_mean     | 4.5      |
| time/              |          |
|    episodes        | 60       |
|    fps             | 48       |
|    time_elapsed    | 9        |
|    total timesteps | 484      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 7.69     |
|    ep_rew_mean     | 4.35     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 48       |
|    time_elapsed    | 10       |
|    total timesteps | 492      |
---------------------------------
---------------------------------
| penalty            | 0        |
| reward             | 0.989    |
| reward_distance    | 0.0873   |
| reward_energy      | 0.0446   |
| reward_velocity    | 0.858    |
| rollout/           |          |
|    ep_len_mean     | 9.19     |
|    ep_rew_mean     | 4.48     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 47       |
|    time_elapsed    | 13       |
|    total timesteps | 625      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.78     |
|    ep_rew_mean     | 4.33     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 47       |
|    time_elapsed    | 13       |
|    total timesteps | 632      |
---------------------------------
---------------------------------
| penalty            | 0        |
| reward             | 0.983    |
| reward_distance    | 0.0908   |
| reward_energy      | 0.0446   |
| reward_velocity    | 0.847    |
| rollout/           |          |
|    ep_len_mean     | 10.1     |
|    ep_rew_mean     | 4.37     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 46       |
|    time_elapsed    | 16       |
|    total timesteps | 765      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.65     |
|    ep_rew_mean     | 4.23     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 46       |
|    time_elapsed    | 16       |
|    total timesteps | 772      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.27     |
|    ep_rew_mean     | 4.12     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 46       |
|    time_elapsed    | 16       |
|    total timesteps | 779      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.92     |
|    ep_rew_mean     | 4        |
| time/              |          |
|    episodes        | 88       |
|    fps             | 46       |
|    time_elapsed    | 16       |
|    total timesteps | 785      |
---------------------------------
---------------------------------
| penalty            | 0        |
| reward             | 0.973    |
| reward_distance    | 0.0911   |
| reward_energy      | 0.0446   |
| reward_velocity    | 0.837    |
| rollout/           |          |
|    ep_len_mean     | 9.99     |
|    ep_rew_mean     | 4.15     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 46       |
|    time_elapsed    | 19       |
|    total timesteps | 919      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.64     |
|    ep_rew_mean     | 4.04     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 47       |
|    time_elapsed    | 19       |
|    total timesteps | 925      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.32     |
|    ep_rew_mean     | 3.95     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 47       |
|    time_elapsed    | 19       |
|    total timesteps | 932      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.32     |
|    ep_rew_mean     | 3.95     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 47       |
|    time_elapsed    | 19       |
|    total timesteps | 939      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.3      |
|    ep_rew_mean     | 3.93     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 47       |
|    time_elapsed    | 20       |
|    total timesteps | 945      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.31     |
|    ep_rew_mean     | 3.94     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 47       |
|    time_elapsed    | 20       |
|    total timesteps | 953      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.3      |
|    ep_rew_mean     | 3.93     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 47       |
|    time_elapsed    | 20       |
|    total timesteps | 959      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.28     |
|    ep_rew_mean     | 3.91     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 47       |
|    time_elapsed    | 20       |
|    total timesteps | 965      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.31     |
|    ep_rew_mean     | 3.94     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 47       |
|    time_elapsed    | 20       |
|    total timesteps | 973      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.31     |
|    ep_rew_mean     | 3.94     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 47       |
|    time_elapsed    | 20       |
|    total timesteps | 980      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.06     |
|    ep_rew_mean     | 3.61     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 47       |
|    time_elapsed    | 20       |
|    total timesteps | 987      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.03     |
|    ep_rew_mean     | 3.58     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 47       |
|    time_elapsed    | 20       |
|    total timesteps | 992      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.03     |
|    ep_rew_mean     | 3.58     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 47       |
|    time_elapsed    | 21       |
|    total timesteps | 999      |
---------------------------------
---------------------------------
| penalty            | 0        |
| reward             | 0.986    |
| reward_distance    | 0.0742   |
| reward_energy      | 0.0446   |
| reward_velocity    | 0.868    |
| rollout/           |          |
|    ep_len_mean     | 6.75     |
|    ep_rew_mean     | 2.94     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 47       |
|    time_elapsed    | 21       |
|    total timesteps | 1005     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.74     |
|    ep_rew_mean     | 2.93     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 47       |
|    time_elapsed    | 21       |
|    total timesteps | 1012     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.76     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 47       |
|    time_elapsed    | 21       |
|    total timesteps | 1020     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 6.76     |
|    ep_rew_mean     | 2.95     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 47       |
|    time_elapsed    | 21       |
|    total timesteps | 1027     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 5.5      |
|    ep_rew_mean     | 2.27     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 47       |
|    time_elapsed    | 21       |
|    total timesteps | 1034     |
---------------------------------
Traceback (most recent call last):
  File "train.py", line 380, in <module>
    model.learn(total_timesteps=int(steps), callback=callback)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/td3/td3.py", line 212, in learn
    reset_num_timesteps=reset_num_timesteps,
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 360, in learn
    log_interval=log_interval,
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/off_policy_algorithm.py", line 567, in collect_rollouts
    new_obs, reward, done, infos = env.step(action)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 163, in step
    return self.step_wait()
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 44, in step_wait
    self.actions[env_idx]
  File "/home/ubuntu/.local/lib/python3.6/site-packages/stable_baselines3/common/monitor.py", line 90, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/ubuntu/.local/lib/python3.6/site-packages/gym/wrappers/time_limit.py", line 16, in step
    observation, reward, done, info = self.env.step(action)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 287, in step
    reward, done, info = self.do_simulation(action, n_frames = self._frame_skip)
  File "/home/ubuntu/OmnidirectionalController/simulations/quadruped.py", line 479, in do_simulation
    reward_velocity = np.exp(params['reward_velocity_coef'] * reward_velocity)
FloatingPointError: underflow encountered in exp
----------------------------------------
| penalty                 | 0          |
| reward                  | 0.973      |
| reward_distance         | 0.0911     |
| reward_energy           | 0.0446     |
| reward_velocity         | 0.837      |
| rollout/                |            |
|    ep_len_mean          | 26.8       |
|    ep_rew_mean          | 23.1       |
| time/                   |            |
|    fps                  | 12         |
|    iterations           | 2          |
|    time_elapsed         | 319        |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.11428054 |
|    clip_fraction        | 0.285      |
|    clip_range           | 0.4        |
|    entropy_loss         | -1.97      |
|    explained_variance   | 0.00238    |
|    learning_rate        | 0.0003     |
|    loss                 | 28.1       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.061     |
|    std                  | 0.367      |
|    value_loss           | 61         |
----------------------------------------
---------------------------------------
| penalty                 | 0         |
| reward                  | 1.05      |
| reward_distance         | 0.0808    |
| reward_energy           | 0.1       |
| reward_velocity         | 0.865     |
| rollout/                |           |
|    ep_len_mean          | 44.6      |
|    ep_rew_mean          | 44.4      |
| time/                   |           |
|    fps                  | 11        |
|    iterations           | 3         |
|    time_elapsed         | 517       |
|    total_timesteps      | 6144      |
| train/                  |           |
|    approx_kl            | 0.1210902 |
|    clip_fraction        | 0.109     |
|    clip_range           | 0.4       |
|    entropy_loss         | -2.02     |
|    explained_variance   | -0.00371  |
|    learning_rate        | 0.0003    |
|    loss                 | 49        |
|    n_updates            | 40        |
|    policy_gradient_loss | -0.0367   |
|    std                  | 0.365     |
|    value_loss           | 106       |
---------------------------------------
--------------------------------------
| penalty                 | 0        |
| reward                  | 1.18     |
| reward_distance         | 0.105    |
| reward_energy           | 0.186    |
| reward_velocity         | 0.889    |
| rollout/                |          |
|    ep_len_mean          | 52.2     |
|    ep_rew_mean          | 61       |
| time/                   |          |
|    fps                  | 11       |
|    iterations           | 4        |
|    time_elapsed         | 717      |
|    total_timesteps      | 8192     |
| train/                  |          |
|    approx_kl            | 0.181009 |
|    clip_fraction        | 0.142    |
|    clip_range           | 0.4      |
|    entropy_loss         | -2.37    |
|    explained_variance   | -0.006   |
|    learning_rate        | 0.0003   |
|    loss                 | 66.2     |
|    n_updates            | 60       |
|    policy_gradient_loss | -0.0288  |
|    std                  | 0.364    |
|    value_loss           | 143      |
--------------------------------------
