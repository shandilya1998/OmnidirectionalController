running build_ext
Using cuda device
Logging to assets/out/models/exp2/PPO_13
Num timesteps: 96000
Best mean reward: -inf - Last mean reward per episode: 1128.07
Saving new best model to assets/out/models/exp2/best_model.zip
---------------------------------
| reward             | 0.418    |
| reward_ang_vel     | 0.0188   |
| reward_contact     | 0.06     |
| reward_ctrl        | 0.0138   |
| reward_energy      | 2.8e-07  |
| reward_orientation | 0.0188   |
| reward_position    | 0.143    |
| reward_velocity    | 0.121    |
| rollout/           |          |
|    ep_len_mean     | 2.65e+03 |
|    ep_rew_mean     | 1.19e+03 |
| time/              |          |
|    fps             | 1120     |
|    iterations      | 1        |
|    time_elapsed    | 142      |
|    total_timesteps | 160000   |
---------------------------------
Num timesteps: 192000
Best mean reward: 1128.07 - Last mean reward per episode: 1304.76
Saving new best model to assets/out/models/exp2/best_model.zip
Num timesteps: 288000
Best mean reward: 1304.76 - Last mean reward per episode: 1398.70
Saving new best model to assets/out/models/exp2/best_model.zip
----------------------------------------
| reward                  | 0.426      |
| reward_ang_vel          | 0.0189     |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0147     |
| reward_energy           | 1.64e-06   |
| reward_orientation      | 0.0189     |
| reward_position         | 0.15       |
| reward_velocity         | 0.121      |
| rollout/                |            |
|    ep_len_mean          | 3.07e+03   |
|    ep_rew_mean          | 1.39e+03   |
| time/                   |            |
|    fps                  | 1114       |
|    iterations           | 2          |
|    time_elapsed         | 287        |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.02437043 |
|    clip_fraction        | 0.0368     |
|    clip_range           | 0.4        |
|    entropy_loss         | -8.08      |
|    explained_variance   | 0.00547    |
|    learning_rate        | 0.0003     |
|    loss                 | 6.46       |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.00674   |
|    std                  | 0.367      |
|    value_loss           | 15.2       |
----------------------------------------
Num timesteps: 384000
Best mean reward: 1398.70 - Last mean reward per episode: 1452.68
Saving new best model to assets/out/models/exp2/best_model.zip
Num timesteps: 480000
Best mean reward: 1452.68 - Last mean reward per episode: 1565.68
Saving new best model to assets/out/models/exp2/best_model.zip
-----------------------------------------
| reward                  | 0.43        |
| reward_ang_vel          | 0.0188      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0147      |
| reward_energy           | 2.35e-06    |
| reward_orientation      | 0.0188      |
| reward_position         | 0.154       |
| reward_velocity         | 0.12        |
| rollout/                |             |
|    ep_len_mean          | 3.47e+03    |
|    ep_rew_mean          | 1.57e+03    |
| time/                   |             |
|    fps                  | 1112        |
|    iterations           | 3           |
|    time_elapsed         | 431         |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.012413477 |
|    clip_fraction        | 0.0186      |
|    clip_range           | 0.4         |
|    entropy_loss         | -8.03       |
|    explained_variance   | 0.00427     |
|    learning_rate        | 0.0003      |
|    loss                 | 5.8         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00225    |
|    std                  | 0.367       |
|    value_loss           | 14.2        |
-----------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp2/PPO_15
Num timesteps: 96000
Best mean reward: -inf - Last mean reward per episode: 1084.44
Saving new best model to assets/out/models/exp2/best_model.zip
---------------------------------
| reward             | 0.39     |
| reward_ang_vel     | 0.0116   |
| reward_contact     | 0.06     |
| reward_ctrl        | 0.00866  |
| reward_energy      | 9.41e-08 |
| reward_orientation | 0.0116   |
| reward_position    | 0.123    |
| reward_velocity    | 0.119    |
| rollout/           |          |
|    ep_len_mean     | 3.05e+03 |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    fps             | 1128     |
|    iterations      | 1        |
|    time_elapsed    | 141      |
|    total_timesteps | 160000   |
---------------------------------
Num timesteps: 192000
Best mean reward: 1084.44 - Last mean reward per episode: 1366.16
Saving new best model to assets/out/models/exp2/best_model.zip
Num timesteps: 288000
Best mean reward: 1366.16 - Last mean reward per episode: 1424.08
Saving new best model to assets/out/models/exp2/best_model.zip
-----------------------------------------
| reward                  | 0.394       |
| reward_ang_vel          | 0.0111      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.00953     |
| reward_energy           | 5.96e-08    |
| reward_orientation      | 0.0111      |
| reward_position         | 0.131       |
| reward_velocity         | 0.116       |
| rollout/                |             |
|    ep_len_mean          | 3.41e+03    |
|    ep_rew_mean          | 1.42e+03    |
| time/                   |             |
|    fps                  | 1114        |
|    iterations           | 2           |
|    time_elapsed         | 287         |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.022664078 |
|    clip_fraction        | 0.0333      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.00207     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00562    |
|    std                  | 0.368       |
|    value_loss           | 5.58        |
-----------------------------------------
Num timesteps: 384000
Best mean reward: 1424.08 - Last mean reward per episode: 1415.62
Num timesteps: 480000
Best mean reward: 1424.08 - Last mean reward per episode: 1505.64
Saving new best model to assets/out/models/exp2/best_model.zip
----------------------------------------
| reward                  | 0.395      |
| reward_ang_vel          | 0.012      |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.00973    |
| reward_energy           | 1.88e-07   |
| reward_orientation      | 0.012      |
| reward_position         | 0.135      |
| reward_velocity         | 0.114      |
| rollout/                |            |
|    ep_len_mean          | 3.6e+03    |
|    ep_rew_mean          | 1.51e+03   |
| time/                   |            |
|    fps                  | 1110       |
|    iterations           | 3          |
|    time_elapsed         | 432        |
|    total_timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.01759521 |
|    clip_fraction        | 0.019      |
|    clip_range           | 0.4        |
|    entropy_loss         | -18.2      |
|    explained_variance   | 0.0563     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.242      |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.00241   |
|    std                  | 0.367      |
|    value_loss           | 3.84       |
----------------------------------------
Num timesteps: 576000
Best mean reward: 1505.64 - Last mean reward per episode: 1461.16
-----------------------------------------
| reward                  | 0.402       |
| reward_ang_vel          | 0.013       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.00901     |
| reward_energy           | 1.81e-07    |
| reward_orientation      | 0.013       |
| reward_position         | 0.139       |
| reward_velocity         | 0.116       |
| rollout/                |             |
|    ep_len_mean          | 3.27e+03    |
|    ep_rew_mean          | 1.39e+03    |
| time/                   |             |
|    fps                  | 1109        |
|    iterations           | 4           |
|    time_elapsed         | 576         |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.016351273 |
|    clip_fraction        | 0.0213      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.0438      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.425       |
|    n_updates            | 60          |
|    policy_gradient_loss | 0.000221    |
|    std                  | 0.367       |
|    value_loss           | 4.74        |
-----------------------------------------
Num timesteps: 672000
Best mean reward: 1505.64 - Last mean reward per episode: 1443.27
Num timesteps: 768000
Best mean reward: 1505.64 - Last mean reward per episode: 1419.30
-----------------------------------------
| reward                  | 0.408       |
| reward_ang_vel          | 0.0132      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.00924     |
| reward_energy           | 1.88e-07    |
| reward_orientation      | 0.0132      |
| reward_position         | 0.14        |
| reward_velocity         | 0.119       |
| rollout/                |             |
|    ep_len_mean          | 3.36e+03    |
|    ep_rew_mean          | 1.44e+03    |
| time/                   |             |
|    fps                  | 1107        |
|    iterations           | 5           |
|    time_elapsed         | 722         |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.015368285 |
|    clip_fraction        | 0.0154      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.3       |
|    explained_variance   | 0.0483      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.403       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00183    |
|    std                  | 0.367       |
|    value_loss           | 3.92        |
-----------------------------------------
Num timesteps: 864000
Best mean reward: 1505.64 - Last mean reward per episode: 1456.71
Num timesteps: 960000
Best mean reward: 1505.64 - Last mean reward per episode: 1452.43
-----------------------------------------
| reward                  | 0.414       |
| reward_ang_vel          | 0.0154      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.00999     |
| reward_energy           | 5.02e-06    |
| reward_orientation      | 0.0154      |
| reward_position         | 0.141       |
| reward_velocity         | 0.118       |
| rollout/                |             |
|    ep_len_mean          | 3.37e+03    |
|    ep_rew_mean          | 1.45e+03    |
| time/                   |             |
|    fps                  | 1107        |
|    iterations           | 6           |
|    time_elapsed         | 867         |
|    total_timesteps      | 960000      |
| train/                  |             |
|    approx_kl            | 0.022752266 |
|    clip_fraction        | 0.0199      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.017       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.589       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.00149    |
|    std                  | 0.367       |
|    value_loss           | 3.92        |
-----------------------------------------
Num timesteps: 1056000
Best mean reward: 1505.64 - Last mean reward per episode: 1452.04
---------------------------------------
| reward                  | 0.413     |
| reward_ang_vel          | 0.0155    |
| reward_contact          | 0.06      |
| reward_ctrl             | 0.0102    |
| reward_energy           | 4.94e-06  |
| reward_orientation      | 0.0155    |
| reward_position         | 0.143     |
| reward_velocity         | 0.117     |
| rollout/                |           |
|    ep_len_mean          | 3.35e+03  |
|    ep_rew_mean          | 1.45e+03  |
| time/                   |           |
|    fps                  | 1106      |
|    iterations           | 7         |
|    time_elapsed         | 1011      |
|    total_timesteps      | 1120000   |
| train/                  |           |
|    approx_kl            | 0.0133018 |
|    clip_fraction        | 0.0196    |
|    clip_range           | 0.4       |
|    entropy_loss         | -18.6     |
|    explained_variance   | 0.0115    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.616     |
|    n_updates            | 120       |
|    policy_gradient_loss | -0.000826 |
|    std                  | 0.367     |
|    value_loss           | 3.64      |
---------------------------------------
Num timesteps: 1152000
Best mean reward: 1505.64 - Last mean reward per episode: 1504.52
Num timesteps: 1248000
Best mean reward: 1505.64 - Last mean reward per episode: 1542.16
Saving new best model to assets/out/models/exp2/best_model.zip
-----------------------------------------
| reward                  | 0.402       |
| reward_ang_vel          | 0.0144      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0106      |
| reward_energy           | 1.32e-06    |
| reward_orientation      | 0.0144      |
| reward_position         | 0.137       |
| reward_velocity         | 0.115       |
| rollout/                |             |
|    ep_len_mean          | 3.68e+03    |
|    ep_rew_mean          | 1.58e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 8           |
|    time_elapsed         | 1157        |
|    total_timesteps      | 1280000     |
| train/                  |             |
|    approx_kl            | 0.016210575 |
|    clip_fraction        | 0.024       |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.9       |
|    explained_variance   | 0.00919     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.556       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.000633   |
|    std                  | 0.367       |
|    value_loss           | 3.11        |
-----------------------------------------
Num timesteps: 1344000
Best mean reward: 1542.16 - Last mean reward per episode: 1615.95
Saving new best model to assets/out/models/exp2/best_model.zip
Num timesteps: 1440000
Best mean reward: 1615.95 - Last mean reward per episode: 1684.70
Saving new best model to assets/out/models/exp2/best_model.zip
-----------------------------------------
| reward                  | 0.398       |
| reward_ang_vel          | 0.0137      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0115      |
| reward_energy           | 1.79e-06    |
| reward_orientation      | 0.0137      |
| reward_position         | 0.134       |
| reward_velocity         | 0.113       |
| rollout/                |             |
|    ep_len_mean          | 3.94e+03    |
|    ep_rew_mean          | 1.68e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 9           |
|    time_elapsed         | 1302        |
|    total_timesteps      | 1440000     |
| train/                  |             |
|    approx_kl            | 0.018386502 |
|    clip_fraction        | 0.0231      |
|    clip_range           | 0.4         |
|    entropy_loss         | -19         |
|    explained_variance   | 0.00564     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.578       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00114    |
|    std                  | 0.367       |
|    value_loss           | 3           |
-----------------------------------------
Num timesteps: 1536000
Best mean reward: 1684.70 - Last mean reward per episode: 1654.64
-----------------------------------------
| reward                  | 0.414       |
| reward_ang_vel          | 0.0163      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0128      |
| reward_energy           | 1.37e-06    |
| reward_orientation      | 0.0163      |
| reward_position         | 0.143       |
| reward_velocity         | 0.114       |
| rollout/                |             |
|    ep_len_mean          | 3.75e+03    |
|    ep_rew_mean          | 1.62e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 10          |
|    time_elapsed         | 1447        |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.017302617 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.4       |
|    explained_variance   | 0.00427     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.5         |
|    n_updates            | 180         |
|    policy_gradient_loss | -0.000672   |
|    std                  | 0.367       |
|    value_loss           | 2.52        |
-----------------------------------------
Num timesteps: 1632000
Best mean reward: 1684.70 - Last mean reward per episode: 1589.35
Num timesteps: 1728000
Best mean reward: 1684.70 - Last mean reward per episode: 1614.26
-----------------------------------------
| reward                  | 0.414       |
| reward_ang_vel          | 0.0155      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0119      |
| reward_energy           | 2.86e-07    |
| reward_orientation      | 0.0155      |
| reward_position         | 0.145       |
| reward_velocity         | 0.115       |
| rollout/                |             |
|    ep_len_mean          | 3.62e+03    |
|    ep_rew_mean          | 1.56e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 11          |
|    time_elapsed         | 1592        |
|    total_timesteps      | 1760000     |
| train/                  |             |
|    approx_kl            | 0.020333175 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.6       |
|    explained_variance   | 0.00285     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.524       |
|    n_updates            | 200         |
|    policy_gradient_loss | -0.00171    |
|    std                  | 0.366       |
|    value_loss           | 2.41        |
-----------------------------------------
Num timesteps: 1824000
Best mean reward: 1684.70 - Last mean reward per episode: 1580.88
Num timesteps: 1920000
Best mean reward: 1684.70 - Last mean reward per episode: 1530.91
-----------------------------------------
| reward                  | 0.416       |
| reward_ang_vel          | 0.0158      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0125      |
| reward_energy           | 7.96e-07    |
| reward_orientation      | 0.0158      |
| reward_position         | 0.146       |
| reward_velocity         | 0.116       |
| rollout/                |             |
|    ep_len_mean          | 3.53e+03    |
|    ep_rew_mean          | 1.53e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 12          |
|    time_elapsed         | 1736        |
|    total_timesteps      | 1920000     |
| train/                  |             |
|    approx_kl            | 0.013897326 |
|    clip_fraction        | 0.0302      |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.8       |
|    explained_variance   | 0.00187     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.505       |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.000242    |
|    std                  | 0.366       |
|    value_loss           | 2.14        |
-----------------------------------------
Num timesteps: 2016000
Best mean reward: 1684.70 - Last mean reward per episode: 1669.73
-----------------------------------------
| reward                  | 0.412       |
| reward_ang_vel          | 0.0151      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0115      |
| reward_energy           | 1.36e-06    |
| reward_orientation      | 0.0151      |
| reward_position         | 0.145       |
| reward_velocity         | 0.114       |
| rollout/                |             |
|    ep_len_mean          | 3.73e+03    |
|    ep_rew_mean          | 1.61e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 13          |
|    time_elapsed         | 1881        |
|    total_timesteps      | 2080000     |
| train/                  |             |
|    approx_kl            | 0.016112527 |
|    clip_fraction        | 0.029       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20         |
|    explained_variance   | 0.00164     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.544       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00118    |
|    std                  | 0.366       |
|    value_loss           | 2           |
-----------------------------------------
Num timesteps: 2112000
Best mean reward: 1684.70 - Last mean reward per episode: 1599.27
Num timesteps: 2208000
Best mean reward: 1684.70 - Last mean reward per episode: 1534.38
---------------------------------------
| reward                  | 0.407     |
| reward_ang_vel          | 0.0144    |
| reward_contact          | 0.06      |
| reward_ctrl             | 0.0122    |
| reward_energy           | 5.44e-06  |
| reward_orientation      | 0.0144    |
| reward_position         | 0.142     |
| reward_velocity         | 0.113     |
| rollout/                |           |
|    ep_len_mean          | 3.44e+03  |
|    ep_rew_mean          | 1.49e+03  |
| time/                   |           |
|    fps                  | 1105      |
|    iterations           | 14        |
|    time_elapsed         | 2026      |
|    total_timesteps      | 2240000   |
| train/                  |           |
|    approx_kl            | 0.0296674 |
|    clip_fraction        | 0.0746    |
|    clip_range           | 0.4       |
|    entropy_loss         | -20.4     |
|    explained_variance   | 0.00126   |
|    learning_rate        | 0.0003    |
|    loss                 | 0.599     |
|    n_updates            | 260       |
|    policy_gradient_loss | -0.00106  |
|    std                  | 0.365     |
|    value_loss           | 1.84      |
---------------------------------------
Num timesteps: 2304000
Best mean reward: 1684.70 - Last mean reward per episode: 1537.27
Num timesteps: 2400000
Best mean reward: 1684.70 - Last mean reward per episode: 1485.11
-----------------------------------------
| reward                  | 0.409       |
| reward_ang_vel          | 0.0139      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0136      |
| reward_energy           | 6.29e-06    |
| reward_orientation      | 0.0139      |
| reward_position         | 0.142       |
| reward_velocity         | 0.113       |
| rollout/                |             |
|    ep_len_mean          | 3.44e+03    |
|    ep_rew_mean          | 1.49e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 15          |
|    time_elapsed         | 2171        |
|    total_timesteps      | 2400000     |
| train/                  |             |
|    approx_kl            | 0.028998848 |
|    clip_fraction        | 0.067       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.5       |
|    explained_variance   | 0.0013      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.669       |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.000185    |
|    std                  | 0.365       |
|    value_loss           | 1.98        |
-----------------------------------------
Num timesteps: 2496000
Best mean reward: 1684.70 - Last mean reward per episode: 1538.17
-----------------------------------------
| reward                  | 0.416       |
| reward_ang_vel          | 0.0153      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0132      |
| reward_energy           | 2.22e-06    |
| reward_orientation      | 0.0153      |
| reward_position         | 0.147       |
| reward_velocity         | 0.114       |
| rollout/                |             |
|    ep_len_mean          | 3.57e+03    |
|    ep_rew_mean          | 1.55e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 16          |
|    time_elapsed         | 2315        |
|    total_timesteps      | 2560000     |
| train/                  |             |
|    approx_kl            | 0.027647732 |
|    clip_fraction        | 0.061       |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.6       |
|    explained_variance   | 0.000972    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.8         |
|    n_updates            | 300         |
|    policy_gradient_loss | -0.000986   |
|    std                  | 0.364       |
|    value_loss           | 1.97        |
-----------------------------------------
Num timesteps: 2592000
Best mean reward: 1684.70 - Last mean reward per episode: 1577.29
Num timesteps: 2688000
Best mean reward: 1684.70 - Last mean reward per episode: 1491.16
-----------------------------------------
| reward                  | 0.415       |
| reward_ang_vel          | 0.015       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0135      |
| reward_energy           | 5.61e-06    |
| reward_orientation      | 0.015       |
| reward_position         | 0.148       |
| reward_velocity         | 0.113       |
| rollout/                |             |
|    ep_len_mean          | 3.33e+03    |
|    ep_rew_mean          | 1.45e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 17          |
|    time_elapsed         | 2460        |
|    total_timesteps      | 2720000     |
| train/                  |             |
|    approx_kl            | 0.028710779 |
|    clip_fraction        | 0.0671      |
|    clip_range           | 0.4         |
|    entropy_loss         | -20.8       |
|    explained_variance   | 0.000749    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.796       |
|    n_updates            | 320         |
|    policy_gradient_loss | 0.000107    |
|    std                  | 0.364       |
|    value_loss           | 2.07        |
-----------------------------------------
Num timesteps: 2784000
Best mean reward: 1684.70 - Last mean reward per episode: 1466.69
Num timesteps: 2880000
Best mean reward: 1684.70 - Last mean reward per episode: 1494.82
-----------------------------------------
| reward                  | 0.409       |
| reward_ang_vel          | 0.0146      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0136      |
| reward_energy           | 7.18e-06    |
| reward_orientation      | 0.0146      |
| reward_position         | 0.142       |
| reward_velocity         | 0.111       |
| rollout/                |             |
|    ep_len_mean          | 3.45e+03    |
|    ep_rew_mean          | 1.49e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 18          |
|    time_elapsed         | 2604        |
|    total_timesteps      | 2880000     |
| train/                  |             |
|    approx_kl            | 0.029577423 |
|    clip_fraction        | 0.056       |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.1       |
|    explained_variance   | 0.000674    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.896       |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00162    |
|    std                  | 0.364       |
|    value_loss           | 1.96        |
-----------------------------------------
Num timesteps: 2976000
Best mean reward: 1684.70 - Last mean reward per episode: 1416.05
-----------------------------------------
| reward                  | 0.409       |
| reward_ang_vel          | 0.0158      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0135      |
| reward_energy           | 6.38e-06    |
| reward_orientation      | 0.0158      |
| reward_position         | 0.141       |
| reward_velocity         | 0.113       |
| rollout/                |             |
|    ep_len_mean          | 3.38e+03    |
|    ep_rew_mean          | 1.47e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 19          |
|    time_elapsed         | 2748        |
|    total_timesteps      | 3040000     |
| train/                  |             |
|    approx_kl            | 0.031875506 |
|    clip_fraction        | 0.0772      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.4       |
|    explained_variance   | 0.000638    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.866       |
|    n_updates            | 360         |
|    policy_gradient_loss | -5.82e-05   |
|    std                  | 0.363       |
|    value_loss           | 2.06        |
-----------------------------------------
Num timesteps: 3072000
Best mean reward: 1684.70 - Last mean reward per episode: 1562.02
Num timesteps: 3168000
Best mean reward: 1684.70 - Last mean reward per episode: 1499.42
-----------------------------------------
| reward                  | 0.414       |
| reward_ang_vel          | 0.0168      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0138      |
| reward_energy           | 6e-06       |
| reward_orientation      | 0.0168      |
| reward_position         | 0.146       |
| reward_velocity         | 0.113       |
| rollout/                |             |
|    ep_len_mean          | 3.45e+03    |
|    ep_rew_mean          | 1.51e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 20          |
|    time_elapsed         | 2893        |
|    total_timesteps      | 3200000     |
| train/                  |             |
|    approx_kl            | 0.027758067 |
|    clip_fraction        | 0.0675      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.8       |
|    explained_variance   | 0.000574    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.903       |
|    n_updates            | 380         |
|    policy_gradient_loss | -0.000374   |
|    std                  | 0.363       |
|    value_loss           | 1.93        |
-----------------------------------------
Num timesteps: 3264000
Best mean reward: 1684.70 - Last mean reward per episode: 1448.41
Num timesteps: 3360000
Best mean reward: 1684.70 - Last mean reward per episode: 1400.96
-----------------------------------------
| reward                  | 0.412       |
| reward_ang_vel          | 0.0153      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0129      |
| reward_energy           | 3.6e-06     |
| reward_orientation      | 0.0153      |
| reward_position         | 0.145       |
| reward_velocity         | 0.113       |
| rollout/                |             |
|    ep_len_mean          | 3.17e+03    |
|    ep_rew_mean          | 1.4e+03     |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 21          |
|    time_elapsed         | 3038        |
|    total_timesteps      | 3360000     |
| train/                  |             |
|    approx_kl            | 0.020501394 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.4         |
|    entropy_loss         | -21.6       |
|    explained_variance   | 0.000432    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.00151    |
|    std                  | 0.362       |
|    value_loss           | 2.49        |
-----------------------------------------
Num timesteps: 3456000
Best mean reward: 1684.70 - Last mean reward per episode: 1403.73
----------------------------------------
| reward                  | 0.414      |
| reward_ang_vel          | 0.0142     |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0127     |
| reward_energy           | 6.9e-07    |
| reward_orientation      | 0.0142     |
| reward_position         | 0.148      |
| reward_velocity         | 0.115      |
| rollout/                |            |
|    ep_len_mean          | 3.2e+03    |
|    ep_rew_mean          | 1.4e+03    |
| time/                   |            |
|    fps                  | 1105       |
|    iterations           | 22         |
|    time_elapsed         | 3182       |
|    total_timesteps      | 3520000    |
| train/                  |            |
|    approx_kl            | 0.04994659 |
|    clip_fraction        | 0.0801     |
|    clip_range           | 0.4        |
|    entropy_loss         | -22        |
|    explained_variance   | 0.00059    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.12       |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.000695  |
|    std                  | 0.362      |
|    value_loss           | 2.32       |
----------------------------------------
Num timesteps: 3552000
Best mean reward: 1684.70 - Last mean reward per episode: 1367.38
Num timesteps: 3648000
Best mean reward: 1684.70 - Last mean reward per episode: 1378.37
-----------------------------------------
| reward                  | 0.412       |
| reward_ang_vel          | 0.015       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0135      |
| reward_energy           | 6.58e-07    |
| reward_orientation      | 0.015       |
| reward_position         | 0.145       |
| reward_velocity         | 0.115       |
| rollout/                |             |
|    ep_len_mean          | 3.3e+03     |
|    ep_rew_mean          | 1.43e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 23          |
|    time_elapsed         | 3327        |
|    total_timesteps      | 3680000     |
| train/                  |             |
|    approx_kl            | 0.030749444 |
|    clip_fraction        | 0.0444      |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.4       |
|    explained_variance   | 0.000514    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14        |
|    n_updates            | 440         |
|    policy_gradient_loss | -9.67e-05   |
|    std                  | 0.361       |
|    value_loss           | 2.4         |
-----------------------------------------
Num timesteps: 3744000
Best mean reward: 1684.70 - Last mean reward per episode: 1436.49
Num timesteps: 3840000
Best mean reward: 1684.70 - Last mean reward per episode: 1446.50
-----------------------------------------
| reward                  | 0.42        |
| reward_ang_vel          | 0.0167      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0129      |
| reward_energy           | 5.85e-07    |
| reward_orientation      | 0.0167      |
| reward_position         | 0.144       |
| reward_velocity         | 0.118       |
| rollout/                |             |
|    ep_len_mean          | 3.3e+03     |
|    ep_rew_mean          | 1.45e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 24          |
|    time_elapsed         | 3472        |
|    total_timesteps      | 3840000     |
| train/                  |             |
|    approx_kl            | 0.034108665 |
|    clip_fraction        | 0.0403      |
|    clip_range           | 0.4         |
|    entropy_loss         | -22.6       |
|    explained_variance   | 0.000533    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 460         |
|    policy_gradient_loss | -0.00182    |
|    std                  | 0.36        |
|    value_loss           | 2.29        |
-----------------------------------------
Num timesteps: 3936000
Best mean reward: 1684.70 - Last mean reward per episode: 1507.40
-----------------------------------------
| reward                  | 0.416       |
| reward_ang_vel          | 0.0175      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0127      |
| reward_energy           | 6.79e-07    |
| reward_orientation      | 0.0175      |
| reward_position         | 0.144       |
| reward_velocity         | 0.114       |
| rollout/                |             |
|    ep_len_mean          | 3.35e+03    |
|    ep_rew_mean          | 1.47e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 25          |
|    time_elapsed         | 3617        |
|    total_timesteps      | 4000000     |
| train/                  |             |
|    approx_kl            | 0.042937476 |
|    clip_fraction        | 0.0918      |
|    clip_range           | 0.4         |
|    entropy_loss         | -23         |
|    explained_variance   | 0.000403    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 480         |
|    policy_gradient_loss | -6.26e-06   |
|    std                  | 0.36        |
|    value_loss           | 2.22        |
-----------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp2/PPO_16
Num timesteps: 96000
Best mean reward: -inf - Last mean reward per episode: 1257.11
Saving new best model to assets/out/models/exp2/best_model.zip
---------------------------------
| reward             | 0.382    |
| reward_ang_vel     | 0.0102   |
| reward_contact     | 0.06     |
| reward_ctrl        | 0.0112   |
| reward_energy      | 5.37e-09 |
| reward_orientation | 0.0102   |
| reward_position    | 0.126    |
| reward_velocity    | 0.106    |
| rollout/           |          |
|    ep_len_mean     | 3.09e+03 |
|    ep_rew_mean     | 1.3e+03  |
| time/              |          |
|    fps             | 1120     |
|    iterations      | 1        |
|    time_elapsed    | 142      |
|    total_timesteps | 160000   |
---------------------------------
Num timesteps: 192000
Best mean reward: 1257.11 - Last mean reward per episode: 1414.75
Saving new best model to assets/out/models/exp2/best_model.zip
Num timesteps: 288000
Best mean reward: 1414.75 - Last mean reward per episode: 1444.50
Saving new best model to assets/out/models/exp2/best_model.zip
-----------------------------------------
| reward                  | 0.395       |
| reward_ang_vel          | 0.0114      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0132      |
| reward_energy           | 5.88e-07    |
| reward_orientation      | 0.0114      |
| reward_position         | 0.133       |
| reward_velocity         | 0.108       |
| rollout/                |             |
|    ep_len_mean          | 3.37e+03    |
|    ep_rew_mean          | 1.43e+03    |
| time/                   |             |
|    fps                  | 1105        |
|    iterations           | 2           |
|    time_elapsed         | 289         |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.026303485 |
|    clip_fraction        | 0.0463      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.2       |
|    explained_variance   | 0.00491     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.00781    |
|    std                  | 0.368       |
|    value_loss           | 5.84        |
-----------------------------------------
Num timesteps: 384000
Best mean reward: 1444.50 - Last mean reward per episode: 1473.47
Saving new best model to assets/out/models/exp2/best_model.zip
Num timesteps: 480000
Best mean reward: 1473.47 - Last mean reward per episode: 1506.99
Saving new best model to assets/out/models/exp2/best_model.zip
-----------------------------------------
| reward                  | 0.404       |
| reward_ang_vel          | 0.0122      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0137      |
| reward_energy           | 5.12e-07    |
| reward_orientation      | 0.0122      |
| reward_position         | 0.139       |
| reward_velocity         | 0.112       |
| rollout/                |             |
|    ep_len_mean          | 3.52e+03    |
|    ep_rew_mean          | 1.51e+03    |
| time/                   |             |
|    fps                  | 1101        |
|    iterations           | 3           |
|    time_elapsed         | 435         |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.026971111 |
|    clip_fraction        | 0.0308      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18         |
|    explained_variance   | 0.067       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.162       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00409    |
|    std                  | 0.368       |
|    value_loss           | 4.05        |
-----------------------------------------
Num timesteps: 576000
Best mean reward: 1506.99 - Last mean reward per episode: 1452.32
-----------------------------------------
| reward                  | 0.406       |
| reward_ang_vel          | 0.0127      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.013       |
| reward_energy           | 1.14e-07    |
| reward_orientation      | 0.0127      |
| reward_position         | 0.139       |
| reward_velocity         | 0.116       |
| rollout/                |             |
|    ep_len_mean          | 3.43e+03    |
|    ep_rew_mean          | 1.48e+03    |
| time/                   |             |
|    fps                  | 1100        |
|    iterations           | 4           |
|    time_elapsed         | 581         |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.027529826 |
|    clip_fraction        | 0.043       |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.8       |
|    explained_variance   | 0.042       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.409       |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00186    |
|    std                  | 0.367       |
|    value_loss           | 4.76        |
-----------------------------------------
Num timesteps: 672000
Best mean reward: 1506.99 - Last mean reward per episode: 1456.39
Num timesteps: 768000
Best mean reward: 1506.99 - Last mean reward per episode: 1501.98
-----------------------------------------
| reward                  | 0.402       |
| reward_ang_vel          | 0.0122      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0144      |
| reward_energy           | 1.16e-06    |
| reward_orientation      | 0.0122      |
| reward_position         | 0.132       |
| reward_velocity         | 0.116       |
| rollout/                |             |
|    ep_len_mean          | 3.33e+03    |
|    ep_rew_mean          | 1.44e+03    |
| time/                   |             |
|    fps                  | 1100        |
|    iterations           | 5           |
|    time_elapsed         | 727         |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.015707253 |
|    clip_fraction        | 0.0206      |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.8       |
|    explained_variance   | 0.0301      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.461       |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.00173    |
|    std                  | 0.367       |
|    value_loss           | 4.14        |
-----------------------------------------
Num timesteps: 864000
Best mean reward: 1506.99 - Last mean reward per episode: 1444.12
Num timesteps: 960000
Best mean reward: 1506.99 - Last mean reward per episode: 1497.79
-----------------------------------------
| reward                  | 0.407       |
| reward_ang_vel          | 0.013       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0163      |
| reward_energy           | 5.92e-06    |
| reward_orientation      | 0.013       |
| reward_position         | 0.134       |
| reward_velocity         | 0.114       |
| rollout/                |             |
|    ep_len_mean          | 3.42e+03    |
|    ep_rew_mean          | 1.5e+03     |
| time/                   |             |
|    fps                  | 1100        |
|    iterations           | 6           |
|    time_elapsed         | 872         |
|    total_timesteps      | 960000      |
| train/                  |             |
|    approx_kl            | 0.018613571 |
|    clip_fraction        | 0.0283      |
|    clip_range           | 0.4         |
|    entropy_loss         | -17.9       |
|    explained_variance   | 0.0151      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.639       |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.000969   |
|    std                  | 0.367       |
|    value_loss           | 4.05        |
-----------------------------------------
Num timesteps: 1056000
Best mean reward: 1506.99 - Last mean reward per episode: 1464.90
-----------------------------------------
| reward                  | 0.419       |
| reward_ang_vel          | 0.0147      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0185      |
| reward_energy           | 5.02e-06    |
| reward_orientation      | 0.0147      |
| reward_position         | 0.139       |
| reward_velocity         | 0.117       |
| rollout/                |             |
|    ep_len_mean          | 3.42e+03    |
|    ep_rew_mean          | 1.51e+03    |
| time/                   |             |
|    fps                  | 1100        |
|    iterations           | 7           |
|    time_elapsed         | 1017        |
|    total_timesteps      | 1120000     |
| train/                  |             |
|    approx_kl            | 0.019263878 |
|    clip_fraction        | 0.0336      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.1       |
|    explained_variance   | 0.00923     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.732       |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.00135    |
|    std                  | 0.367       |
|    value_loss           | 4.07        |
-----------------------------------------
Num timesteps: 1152000
Best mean reward: 1506.99 - Last mean reward per episode: 1523.88
Saving new best model to assets/out/models/exp2/best_model.zip
Num timesteps: 1248000
Best mean reward: 1523.88 - Last mean reward per episode: 1508.71
-----------------------------------------
| reward                  | 0.413       |
| reward_ang_vel          | 0.014       |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0196      |
| reward_energy           | 2.6e-06     |
| reward_orientation      | 0.014       |
| reward_position         | 0.134       |
| reward_velocity         | 0.117       |
| rollout/                |             |
|    ep_len_mean          | 3.6e+03     |
|    ep_rew_mean          | 1.58e+03    |
| time/                   |             |
|    fps                  | 1099        |
|    iterations           | 8           |
|    time_elapsed         | 1163        |
|    total_timesteps      | 1280000     |
| train/                  |             |
|    approx_kl            | 0.018162593 |
|    clip_fraction        | 0.0375      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.4       |
|    explained_variance   | 0.00664     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.697       |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.000601   |
|    std                  | 0.367       |
|    value_loss           | 3.65        |
-----------------------------------------
Num timesteps: 1344000
Best mean reward: 1523.88 - Last mean reward per episode: 1534.73
Saving new best model to assets/out/models/exp2/best_model.zip
Num timesteps: 1440000
Best mean reward: 1534.73 - Last mean reward per episode: 1599.49
Saving new best model to assets/out/models/exp2/best_model.zip
-----------------------------------------
| reward                  | 0.419       |
| reward_ang_vel          | 0.0141      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0188      |
| reward_energy           | 6.56e-06    |
| reward_orientation      | 0.0141      |
| reward_position         | 0.142       |
| reward_velocity         | 0.117       |
| rollout/                |             |
|    ep_len_mean          | 3.62e+03    |
|    ep_rew_mean          | 1.6e+03     |
| time/                   |             |
|    fps                  | 1099        |
|    iterations           | 9           |
|    time_elapsed         | 1309        |
|    total_timesteps      | 1440000     |
| train/                  |             |
|    approx_kl            | 0.016092477 |
|    clip_fraction        | 0.0376      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.5       |
|    explained_variance   | 0.00565     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.575       |
|    n_updates            | 160         |
|    policy_gradient_loss | -0.00165    |
|    std                  | 0.367       |
|    value_loss           | 3.04        |
-----------------------------------------
Num timesteps: 1536000
Best mean reward: 1599.49 - Last mean reward per episode: 1540.41
-----------------------------------------
| reward                  | 0.427       |
| reward_ang_vel          | 0.0152      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0189      |
| reward_energy           | 6.78e-06    |
| reward_orientation      | 0.0152      |
| reward_position         | 0.148       |
| reward_velocity         | 0.117       |
| rollout/                |             |
|    ep_len_mean          | 3.34e+03    |
|    ep_rew_mean          | 1.48e+03    |
| time/                   |             |
|    fps                  | 1099        |
|    iterations           | 10          |
|    time_elapsed         | 1455        |
|    total_timesteps      | 1600000     |
| train/                  |             |
|    approx_kl            | 0.023509057 |
|    clip_fraction        | 0.0387      |
|    clip_range           | 0.4         |
|    entropy_loss         | -18.8       |
|    explained_variance   | 0.00287     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.637       |
|    n_updates            | 180         |
|    policy_gradient_loss | 0.000351    |
|    std                  | 0.367       |
|    value_loss           | 3.13        |
-----------------------------------------
Num timesteps: 1632000
Best mean reward: 1599.49 - Last mean reward per episode: 1509.80
Num timesteps: 1728000
Best mean reward: 1599.49 - Last mean reward per episode: 1503.64
-----------------------------------------
| reward                  | 0.421       |
| reward_ang_vel          | 0.0145      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0202      |
| reward_energy           | 3.12e-06    |
| reward_orientation      | 0.0145      |
| reward_position         | 0.144       |
| reward_velocity         | 0.114       |
| rollout/                |             |
|    ep_len_mean          | 3.33e+03    |
|    ep_rew_mean          | 1.48e+03    |
| time/                   |             |
|    fps                  | 1099        |
|    iterations           | 11          |
|    time_elapsed         | 1601        |
|    total_timesteps      | 1760000     |
| train/                  |             |
|    approx_kl            | 0.020501269 |
|    clip_fraction        | 0.0583      |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.2       |
|    explained_variance   | 0.0023      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.617       |
|    n_updates            | 200         |
|    policy_gradient_loss | 0.000417    |
|    std                  | 0.366       |
|    value_loss           | 2.77        |
-----------------------------------------
Num timesteps: 1824000
Best mean reward: 1599.49 - Last mean reward per episode: 1551.34
Num timesteps: 1920000
Best mean reward: 1599.49 - Last mean reward per episode: 1557.49
----------------------------------------
| reward                  | 0.422      |
| reward_ang_vel          | 0.0154     |
| reward_contact          | 0.06       |
| reward_ctrl             | 0.0208     |
| reward_energy           | 6.88e-06   |
| reward_orientation      | 0.0154     |
| reward_position         | 0.143      |
| reward_velocity         | 0.112      |
| rollout/                |            |
|    ep_len_mean          | 3.52e+03   |
|    ep_rew_mean          | 1.56e+03   |
| time/                   |            |
|    fps                  | 1099       |
|    iterations           | 12         |
|    time_elapsed         | 1746       |
|    total_timesteps      | 1920000    |
| train/                  |            |
|    approx_kl            | 0.01433574 |
|    clip_fraction        | 0.0319     |
|    clip_range           | 0.4        |
|    entropy_loss         | -19.4      |
|    explained_variance   | 0.00189    |
|    learning_rate        | 0.0003     |
|    loss                 | 0.541      |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.000748  |
|    std                  | 0.366      |
|    value_loss           | 2.34       |
----------------------------------------
Num timesteps: 2016000
Best mean reward: 1599.49 - Last mean reward per episode: 1575.85
-----------------------------------------
| reward                  | 0.422       |
| reward_ang_vel          | 0.0156      |
| reward_contact          | 0.06        |
| reward_ctrl             | 0.0195      |
| reward_energy           | 4.42e-06    |
| reward_orientation      | 0.0156      |
| reward_position         | 0.141       |
| reward_velocity         | 0.115       |
| rollout/                |             |
|    ep_len_mean          | 3.48e+03    |
|    ep_rew_mean          | 1.54e+03    |
| time/                   |             |
|    fps                  | 1099        |
|    iterations           | 13          |
|    time_elapsed         | 1892        |
|    total_timesteps      | 2080000     |
| train/                  |             |
|    approx_kl            | 0.018232193 |
|    clip_fraction        | 0.0403      |
|    clip_range           | 0.4         |
|    entropy_loss         | -19.6       |
|    explained_variance   | 0.00102     |
|    learning_rate        | 0.0003      |
|    loss                 | 0.564       |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.000576   |
|    std                  | 0.366       |
|    value_loss           | 2.16        |
-----------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp2/PPO_17
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp2/A2C_8
------------------------------------
| time/                 |          |
|    fps                | 1062     |
|    iterations         | 100      |
|    time_elapsed       | 12       |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -39.2    |
|    explained_variance | 0.000158 |
|    learning_rate      | 0.001    |
|    n_updates          | 99       |
|    policy_loss        | 0.105    |
|    std                | 1        |
|    value_loss         | 89.1     |
------------------------------------
------------------------------------
| time/                 |          |
|    fps                | 1068     |
|    iterations         | 200      |
|    time_elapsed       | 23       |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -41.6    |
|    explained_variance | 0.000199 |
|    learning_rate      | 0.001    |
|    n_updates          | 199      |
|    policy_loss        | -0.429   |
|    std                | 0.999    |
|    value_loss         | 69.5     |
------------------------------------
------------------------------------
| reward                | 2.82     |
| reward_ang_vel        | 0.0691   |
| reward_contact        | 1        |
| reward_ctrl           | 0.00956  |
| reward_energy         | 8.7e-18  |
| reward_orientation    | 0.0691   |
| reward_position       | 0.529    |
| reward_velocity       | 0.664    |
| rollout/              |          |
|    ep_len_mean        | 2.04e+03 |
|    ep_rew_mean        | 6.42e+03 |
| time/                 |          |
|    fps                | 1066     |
|    iterations         | 300      |
|    time_elapsed       | 35       |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -40.7    |
|    explained_variance | 4.39e-05 |
|    learning_rate      | 0.001    |
|    n_updates          | 299      |
|    policy_loss        | 0.575    |
|    std                | 0.998    |
|    value_loss         | 70.6     |
------------------------------------
------------------------------------
| reward                | 3.07     |
| reward_ang_vel        | 0.222    |
| reward_contact        | 1        |
| reward_ctrl           | 0.0143   |
| reward_energy         | 8.08e-12 |
| reward_orientation    | 0.222    |
| reward_position       | 0.605    |
| reward_velocity       | 0.748    |
| rollout/              |          |
|    ep_len_mean        | 2.49e+03 |
|    ep_rew_mean        | 7.74e+03 |
| time/                 |          |
|    fps                | 1064     |
|    iterations         | 400      |
|    time_elapsed       | 48       |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -43.8    |
|    explained_variance | 2.87e-05 |
|    learning_rate      | 0.001    |
|    n_updates          | 399      |
|    policy_loss        | 0.333    |
|    std                | 0.997    |
|    value_loss         | 46.7     |
------------------------------------
------------------------------------
| reward                | 3.08     |
| reward_ang_vel        | 0.21     |
| reward_contact        | 1        |
| reward_ctrl           | 0.0142   |
| reward_energy         | 7.4e-12  |
| reward_orientation    | 0.21     |
| reward_position       | 0.622    |
| reward_velocity       | 0.736    |
| rollout/              |          |
|    ep_len_mean        | 2.71e+03 |
|    ep_rew_mean        | 8.47e+03 |
| time/                 |          |
|    fps                | 1066     |
|    iterations         | 500      |
|    time_elapsed       | 60       |
|    total_timesteps    | 64000    |
| train/                |          |
|    entropy_loss       | -43      |
|    explained_variance | 1.19e-05 |
|    learning_rate      | 0.001    |
|    n_updates          | 499      |
|    policy_loss        | -0.134   |
|    std                | 0.999    |
|    value_loss         | 36.4     |
------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp2/A2C_9
------------------------------------
| time/                 |          |
|    fps                | 1072     |
|    iterations         | 100      |
|    time_elapsed       | 11       |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -41.3    |
|    explained_variance | 0.000324 |
|    learning_rate      | 0.001    |
|    n_updates          | 99       |
|    policy_loss        | -0.0215  |
|    std                | 1        |
|    value_loss         | 83       |
------------------------------------
------------------------------------
| reward                | 2.29     |
| reward_ang_vel        | 0.0251   |
| reward_contact        | 1        |
| reward_ctrl           | 0.0254   |
| reward_energy         | 2.5e-133 |
| reward_orientation    | 0.0251   |
| reward_position       | 0.264    |
| reward_velocity       | 0.603    |
| rollout/              |          |
|    ep_len_mean        | 1.5e+03  |
|    ep_rew_mean        | 4.23e+03 |
| time/                 |          |
|    fps                | 1075     |
|    iterations         | 200      |
|    time_elapsed       | 23       |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -41.5    |
|    explained_variance | 9.33e-05 |
|    learning_rate      | 0.001    |
|    n_updates          | 199      |
|    policy_loss        | 0.19     |
|    std                | 1        |
|    value_loss         | 68.5     |
------------------------------------
-------------------------------------
| reward                | 2.81      |
| reward_ang_vel        | 0.248     |
| reward_contact        | 1         |
| reward_ctrl           | 0.0276    |
| reward_energy         | 5.34e-109 |
| reward_orientation    | 0.248     |
| reward_position       | 0.37      |
| reward_velocity       | 0.741     |
| rollout/              |           |
|    ep_len_mean        | 1.88e+03  |
|    ep_rew_mean        | 5.49e+03  |
| time/                 |           |
|    fps                | 1073      |
|    iterations         | 300       |
|    time_elapsed       | 35        |
|    total_timesteps    | 38400     |
| train/                |           |
|    entropy_loss       | -38.6     |
|    explained_variance | 2.2e-05   |
|    learning_rate      | 0.001     |
|    n_updates          | 299       |
|    policy_loss        | -0.202    |
|    std                | 1         |
|    value_loss         | 52.5      |
-------------------------------------
-------------------------------------
| reward                | 2.82      |
| reward_ang_vel        | 0.178     |
| reward_contact        | 1         |
| reward_ctrl           | 0.0196    |
| reward_energy         | 3.59e-108 |
| reward_orientation    | 0.178     |
| reward_position       | 0.491     |
| reward_velocity       | 0.69      |
| rollout/              |           |
|    ep_len_mean        | 2.48e+03  |
|    ep_rew_mean        | 7.5e+03   |
| time/                 |           |
|    fps                | 1072      |
|    iterations         | 400       |
|    time_elapsed       | 47        |
|    total_timesteps    | 51200     |
| train/                |           |
|    entropy_loss       | -38       |
|    explained_variance | 1.86e-05  |
|    learning_rate      | 0.001     |
|    n_updates          | 399       |
|    policy_loss        | -0.32     |
|    std                | 1         |
|    value_loss         | 47.2      |
-------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp2/A2C_22
------------------------------------
| time/                 |          |
|    fps                | 1111     |
|    iterations         | 100      |
|    time_elapsed       | 11       |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -38.8    |
|    explained_variance | 0.000228 |
|    learning_rate      | 0.001    |
|    n_updates          | 99       |
|    policy_loss        | 0.141    |
|    std                | 0.999    |
|    value_loss         | 87.1     |
------------------------------------
------------------------------------
| time/                 |          |
|    fps                | 1114     |
|    iterations         | 200      |
|    time_elapsed       | 22       |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -38.5    |
|    explained_variance | 0.000141 |
|    learning_rate      | 0.001    |
|    n_updates          | 199      |
|    policy_loss        | -0.385   |
|    std                | 1        |
|    value_loss         | 75.6     |
------------------------------------
------------------------------------
| reward                | 3.02     |
| reward_ang_vel        | 0.218    |
| reward_contact        | 0.961    |
| reward_ctrl           | 0.0102   |
| reward_energy         | 0.0672   |
| reward_orientation    | 0.218    |
| reward_position       | 0.579    |
| reward_velocity       | 0.659    |
| rollout/              |          |
|    ep_len_mean        | 2.19e+03 |
|    ep_rew_mean        | 6.8e+03  |
| time/                 |          |
|    fps                | 1112     |
|    iterations         | 300      |
|    time_elapsed       | 34       |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -40.7    |
|    explained_variance | 2.95e-05 |
|    learning_rate      | 0.001    |
|    n_updates          | 299      |
|    policy_loss        | -0.268   |
|    std                | 1        |
|    value_loss         | 74.4     |
------------------------------------
------------------------------------
| reward                | 3.37     |
| reward_ang_vel        | 0.349    |
| reward_contact        | 0.974    |
| reward_ctrl           | 0.0147   |
| reward_energy         | 0.0762   |
| reward_orientation    | 0.349    |
| reward_position       | 0.627    |
| reward_velocity       | 0.73     |
| rollout/              |          |
|    ep_len_mean        | 2.45e+03 |
|    ep_rew_mean        | 7.98e+03 |
| time/                 |          |
|    fps                | 1112     |
|    iterations         | 400      |
|    time_elapsed       | 46       |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -44      |
|    explained_variance | 0.000105 |
|    learning_rate      | 0.001    |
|    n_updates          | 399      |
|    policy_loss        | 0.457    |
|    std                | 1        |
|    value_loss         | 46.4     |
------------------------------------
------------------------------------
| reward                | 3.26     |
| reward_ang_vel        | 0.308    |
| reward_contact        | 0.975    |
| reward_ctrl           | 0.0151   |
| reward_energy         | 0.0799   |
| reward_orientation    | 0.308    |
| reward_position       | 0.579    |
| reward_velocity       | 0.728    |
| rollout/              |          |
|    ep_len_mean        | 2.48e+03 |
|    ep_rew_mean        | 7.93e+03 |
| time/                 |          |
|    fps                | 1114     |
|    iterations         | 500      |
|    time_elapsed       | 57       |
|    total_timesteps    | 64000    |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 1.45e-05 |
|    learning_rate      | 0.001    |
|    n_updates          | 499      |
|    policy_loss        | 0.375    |
|    std                | 1        |
|    value_loss         | 30.2     |
------------------------------------
------------------------------------
| reward                | 3.28     |
| reward_ang_vel        | 0.319    |
| reward_contact        | 0.977    |
| reward_ctrl           | 0.0158   |
| reward_energy         | 0.083    |
| reward_orientation    | 0.319    |
| reward_position       | 0.579    |
| reward_velocity       | 0.743    |
| rollout/              |          |
|    ep_len_mean        | 2.57e+03 |
|    ep_rew_mean        | 8.21e+03 |
| time/                 |          |
|    fps                | 1115     |
|    iterations         | 600      |
|    time_elapsed       | 68       |
|    total_timesteps    | 76800    |
| train/                |          |
|    entropy_loss       | -49.1    |
|    explained_variance | 8.82e-06 |
|    learning_rate      | 0.001    |
|    n_updates          | 599      |
|    policy_loss        | -0.111   |
|    std                | 1        |
|    value_loss         | 22.5     |
------------------------------------
------------------------------------
| reward                | 3.24     |
| reward_ang_vel        | 0.25     |
| reward_contact        | 0.977    |
| reward_ctrl           | 0.0214   |
| reward_energy         | 0.099    |
| reward_orientation    | 0.25     |
| reward_position       | 0.598    |
| reward_velocity       | 0.77     |
| rollout/              |          |
|    ep_len_mean        | 2.48e+03 |
|    ep_rew_mean        | 7.89e+03 |
| time/                 |          |
|    fps                | 1114     |
|    iterations         | 700      |
|    time_elapsed       | 80       |
|    total_timesteps    | 89600    |
| train/                |          |
|    entropy_loss       | -49.2    |
|    explained_variance | 0        |
|    learning_rate      | 0.001    |
|    n_updates          | 699      |
|    policy_loss        | -0.913   |
|    std                | 0.999    |
|    value_loss         | 1.51e+03 |
------------------------------------
Num timesteps: 96000
Best mean reward: -inf - Last mean reward per episode: 9203.69
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 3.22     |
| reward_ang_vel        | 0.223    |
| reward_contact        | 0.977    |
| reward_ctrl           | 0.0249   |
| reward_energy         | 0.105    |
| reward_orientation    | 0.223    |
| reward_position       | 0.595    |
| reward_velocity       | 0.757    |
| rollout/              |          |
|    ep_len_mean        | 2.99e+03 |
|    ep_rew_mean        | 9.53e+03 |
| time/                 |          |
|    fps                | 1111     |
|    iterations         | 800      |
|    time_elapsed       | 92       |
|    total_timesteps    | 102400   |
| train/                |          |
|    entropy_loss       | -49.9    |
|    explained_variance | 7.21e-06 |
|    learning_rate      | 0.001    |
|    n_updates          | 799      |
|    policy_loss        | 0.107    |
|    std                | 0.996    |
|    value_loss         | 14.9     |
------------------------------------
------------------------------------
| reward                | 3.2      |
| reward_ang_vel        | 0.209    |
| reward_contact        | 0.973    |
| reward_ctrl           | 0.0264   |
| reward_energy         | 0.11     |
| reward_orientation    | 0.209    |
| reward_position       | 0.591    |
| reward_velocity       | 0.752    |
| rollout/              |          |
|    ep_len_mean        | 2.88e+03 |
|    ep_rew_mean        | 9.24e+03 |
| time/                 |          |
|    fps                | 1111     |
|    iterations         | 900      |
|    time_elapsed       | 103      |
|    total_timesteps    | 115200   |
| train/                |          |
|    entropy_loss       | -50      |
|    explained_variance | 2.92e-06 |
|    learning_rate      | 0.001    |
|    n_updates          | 899      |
|    policy_loss        | -0.219   |
|    std                | 0.99     |
|    value_loss         | 11.2     |
------------------------------------
------------------------------------
| reward                | 3.22     |
| reward_ang_vel        | 0.194    |
| reward_contact        | 0.975    |
| reward_ctrl           | 0.0273   |
| reward_energy         | 0.117    |
| reward_orientation    | 0.194    |
| reward_position       | 0.616    |
| reward_velocity       | 0.757    |
| rollout/              |          |
|    ep_len_mean        | 2.94e+03 |
|    ep_rew_mean        | 9.48e+03 |
| time/                 |          |
|    fps                | 1111     |
|    iterations         | 1000     |
|    time_elapsed       | 115      |
|    total_timesteps    | 128000   |
| train/                |          |
|    entropy_loss       | -50      |
|    explained_variance | 9.6e-06  |
|    learning_rate      | 0.001    |
|    n_updates          | 999      |
|    policy_loss        | 0.279    |
|    std                | 0.984    |
|    value_loss         | 3.83     |
------------------------------------
------------------------------------
| reward                | 3.26     |
| reward_ang_vel        | 0.207    |
| reward_contact        | 0.978    |
| reward_ctrl           | 0.0312   |
| reward_energy         | 0.139    |
| reward_orientation    | 0.207    |
| reward_position       | 0.617    |
| reward_velocity       | 0.762    |
| rollout/              |          |
|    ep_len_mean        | 2.91e+03 |
|    ep_rew_mean        | 9.44e+03 |
| time/                 |          |
|    fps                | 1110     |
|    iterations         | 1100     |
|    time_elapsed       | 126      |
|    total_timesteps    | 140800   |
| train/                |          |
|    entropy_loss       | -49.9    |
|    explained_variance | 5.36e-07 |
|    learning_rate      | 0.001    |
|    n_updates          | 1099     |
|    policy_loss        | -0.0966  |
|    std                | 0.973    |
|    value_loss         | 5.58     |
------------------------------------
------------------------------------
| reward                | 3.27     |
| reward_ang_vel        | 0.213    |
| reward_contact        | 0.979    |
| reward_ctrl           | 0.0314   |
| reward_energy         | 0.141    |
| reward_orientation    | 0.213    |
| reward_position       | 0.62     |
| reward_velocity       | 0.769    |
| rollout/              |          |
|    ep_len_mean        | 2.88e+03 |
|    ep_rew_mean        | 9.4e+03  |
| time/                 |          |
|    fps                | 1109     |
|    iterations         | 1200     |
|    time_elapsed       | 138      |
|    total_timesteps    | 153600   |
| train/                |          |
|    entropy_loss       | -49.9    |
|    explained_variance | 3.1e-06  |
|    learning_rate      | 0.001    |
|    n_updates          | 1199     |
|    policy_loss        | -0.562   |
|    std                | 0.965    |
|    value_loss         | 4.87     |
------------------------------------
-------------------------------------
| reward                | 3.3       |
| reward_ang_vel        | 0.215     |
| reward_contact        | 0.98      |
| reward_ctrl           | 0.0326    |
| reward_energy         | 0.151     |
| reward_orientation    | 0.215     |
| reward_position       | 0.634     |
| reward_velocity       | 0.771     |
| rollout/              |           |
|    ep_len_mean        | 2.9e+03   |
|    ep_rew_mean        | 9.52e+03  |
| time/                 |           |
|    fps                | 1107      |
|    iterations         | 1300      |
|    time_elapsed       | 150       |
|    total_timesteps    | 166400    |
| train/                |           |
|    entropy_loss       | -49.7     |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.001     |
|    n_updates          | 1299      |
|    policy_loss        | -0.571    |
|    std                | 0.951     |
|    value_loss         | 6.72      |
-------------------------------------
------------------------------------
| reward                | 3.41     |
| reward_ang_vel        | 0.255    |
| reward_contact        | 0.981    |
| reward_ctrl           | 0.0351   |
| reward_energy         | 0.2      |
| reward_orientation    | 0.255    |
| reward_position       | 0.644    |
| reward_velocity       | 0.784    |
| rollout/              |          |
|    ep_len_mean        | 2.95e+03 |
|    ep_rew_mean        | 9.9e+03  |
| time/                 |          |
|    fps                | 1104     |
|    iterations         | 1400     |
|    time_elapsed       | 162      |
|    total_timesteps    | 179200   |
| train/                |          |
|    entropy_loss       | -49.5    |
|    explained_variance | 7.99e-06 |
|    learning_rate      | 0.001    |
|    n_updates          | 1399     |
|    policy_loss        | -0.15    |
|    std                | 0.939    |
|    value_loss         | 8.17     |
------------------------------------
Num timesteps: 192000
Best mean reward: 9203.69 - Last mean reward per episode: 10057.65
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 3.51     |
| reward_ang_vel        | 0.291    |
| reward_contact        | 0.981    |
| reward_ctrl           | 0.0362   |
| reward_energy         | 0.236    |
| reward_orientation    | 0.291    |
| reward_position       | 0.66     |
| reward_velocity       | 0.796    |
| rollout/              |          |
|    ep_len_mean        | 2.96e+03 |
|    ep_rew_mean        | 1.01e+04 |
| time/                 |          |
|    fps                | 1101     |
|    iterations         | 1500     |
|    time_elapsed       | 174      |
|    total_timesteps    | 192000   |
| train/                |          |
|    entropy_loss       | -49.4    |
|    explained_variance | 9.54e-06 |
|    learning_rate      | 0.001    |
|    n_updates          | 1499     |
|    policy_loss        | 0.00795  |
|    std                | 0.929    |
|    value_loss         | 6.45     |
------------------------------------
------------------------------------
| reward                | 3.55     |
| reward_ang_vel        | 0.31     |
| reward_contact        | 0.982    |
| reward_ctrl           | 0.037    |
| reward_energy         | 0.253    |
| reward_orientation    | 0.31     |
| reward_position       | 0.669    |
| reward_velocity       | 0.802    |
| rollout/              |          |
|    ep_len_mean        | 3e+03    |
|    ep_rew_mean        | 1.03e+04 |
| time/                 |          |
|    fps                | 1100     |
|    iterations         | 1600     |
|    time_elapsed       | 186      |
|    total_timesteps    | 204800   |
| train/                |          |
|    entropy_loss       | -49.3    |
|    explained_variance | 1.07e-06 |
|    learning_rate      | 0.001    |
|    n_updates          | 1599     |
|    policy_loss        | -0.107   |
|    std                | 0.918    |
|    value_loss         | 4.38     |
------------------------------------
------------------------------------
| reward                | 3.64     |
| reward_ang_vel        | 0.347    |
| reward_contact        | 0.982    |
| reward_ctrl           | 0.0374   |
| reward_energy         | 0.287    |
| reward_orientation    | 0.347    |
| reward_position       | 0.675    |
| reward_velocity       | 0.813    |
| rollout/              |          |
|    ep_len_mean        | 2.97e+03 |
|    ep_rew_mean        | 1.04e+04 |
| time/                 |          |
|    fps                | 1098     |
|    iterations         | 1700     |
|    time_elapsed       | 198      |
|    total_timesteps    | 217600   |
| train/                |          |
|    entropy_loss       | -49.1    |
|    explained_variance | 0        |
|    learning_rate      | 0.001    |
|    n_updates          | 1699     |
|    policy_loss        | -0.557   |
|    std                | 0.908    |
|    value_loss         | 1.98     |
------------------------------------
------------------------------------
| reward                | 3.71     |
| reward_ang_vel        | 0.383    |
| reward_contact        | 0.983    |
| reward_ctrl           | 0.0381   |
| reward_energy         | 0.317    |
| reward_orientation    | 0.383    |
| reward_position       | 0.678    |
| reward_velocity       | 0.823    |
| rollout/              |          |
|    ep_len_mean        | 2.99e+03 |
|    ep_rew_mean        | 1.08e+04 |
| time/                 |          |
|    fps                | 1096     |
|    iterations         | 1800     |
|    time_elapsed       | 210      |
|    total_timesteps    | 230400   |
| train/                |          |
|    entropy_loss       | -49      |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.001    |
|    n_updates          | 1799     |
|    policy_loss        | -0.168   |
|    std                | 0.898    |
|    value_loss         | 1.52     |
------------------------------------
------------------------------------
| reward                | 3.81     |
| reward_ang_vel        | 0.417    |
| reward_contact        | 0.984    |
| reward_ctrl           | 0.0392   |
| reward_energy         | 0.349    |
| reward_orientation    | 0.417    |
| reward_position       | 0.696    |
| reward_velocity       | 0.835    |
| rollout/              |          |
|    ep_len_mean        | 3.01e+03 |
|    ep_rew_mean        | 1.1e+04  |
| time/                 |          |
|    fps                | 1095     |
|    iterations         | 1900     |
|    time_elapsed       | 222      |
|    total_timesteps    | 243200   |
| train/                |          |
|    entropy_loss       | -48.9    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.001    |
|    n_updates          | 1899     |
|    policy_loss        | 0.147    |
|    std                | 0.89     |
|    value_loss         | 0.823    |
------------------------------------
------------------------------------
| reward                | 3.88     |
| reward_ang_vel        | 0.449    |
| reward_contact        | 0.985    |
| reward_ctrl           | 0.0402   |
| reward_energy         | 0.377    |
| reward_orientation    | 0.449    |
| reward_position       | 0.699    |
| reward_velocity       | 0.845    |
| rollout/              |          |
|    ep_len_mean        | 3.06e+03 |
|    ep_rew_mean        | 1.15e+04 |
| time/                 |          |
|    fps                | 1093     |
|    iterations         | 2000     |
|    time_elapsed       | 234      |
|    total_timesteps    | 256000   |
| train/                |          |
|    entropy_loss       | -48.9    |
|    explained_variance | 1.79e-07 |
|    learning_rate      | 0.001    |
|    n_updates          | 1999     |
|    policy_loss        | 0.378    |
|    std                | 0.888    |
|    value_loss         | 2.56     |
------------------------------------
------------------------------------
| reward                | 3.91     |
| reward_ang_vel        | 0.467    |
| reward_contact        | 0.985    |
| reward_ctrl           | 0.0407   |
| reward_energy         | 0.392    |
| reward_orientation    | 0.467    |
| reward_position       | 0.694    |
| reward_velocity       | 0.848    |
| rollout/              |          |
|    ep_len_mean        | 3.11e+03 |
|    ep_rew_mean        | 1.18e+04 |
| time/                 |          |
|    fps                | 1093     |
|    iterations         | 2100     |
|    time_elapsed       | 245      |
|    total_timesteps    | 268800   |
| train/                |          |
|    entropy_loss       | -48.8    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.001    |
|    n_updates          | 2099     |
|    policy_loss        | -0.555   |
|    std                | 0.886    |
|    value_loss         | 0.248    |
------------------------------------
------------------------------------
| reward                | 3.96     |
| reward_ang_vel        | 0.486    |
| reward_contact        | 0.985    |
| reward_ctrl           | 0.0412   |
| reward_energy         | 0.412    |
| reward_orientation    | 0.486    |
| reward_position       | 0.703    |
| reward_velocity       | 0.85     |
| rollout/              |          |
|    ep_len_mean        | 3.06e+03 |
|    ep_rew_mean        | 1.18e+04 |
| time/                 |          |
|    fps                | 1092     |
|    iterations         | 2200     |
|    time_elapsed       | 257      |
|    total_timesteps    | 281600   |
| train/                |          |
|    entropy_loss       | -48.8    |
|    explained_variance | 1.2e-05  |
|    learning_rate      | 0.001    |
|    n_updates          | 2199     |
|    policy_loss        | 0.481    |
|    std                | 0.885    |
|    value_loss         | 2.1      |
------------------------------------
Num timesteps: 288000
Best mean reward: 10057.65 - Last mean reward per episode: 11839.01
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 4.01     |
| reward_ang_vel        | 0.504    |
| reward_contact        | 0.986    |
| reward_ctrl           | 0.0414   |
| reward_energy         | 0.428    |
| reward_orientation    | 0.504    |
| reward_position       | 0.71     |
| reward_velocity       | 0.856    |
| rollout/              |          |
|    ep_len_mean        | 3.05e+03 |
|    ep_rew_mean        | 1.19e+04 |
| time/                 |          |
|    fps                | 1091     |
|    iterations         | 2300     |
|    time_elapsed       | 269      |
|    total_timesteps    | 294400   |
| train/                |          |
|    entropy_loss       | -48.7    |
|    explained_variance | 0        |
|    learning_rate      | 0.001    |
|    n_updates          | 2299     |
|    policy_loss        | 0.0495   |
|    std                | 0.879    |
|    value_loss         | 2.64     |
------------------------------------
------------------------------------
| reward                | 4.05     |
| reward_ang_vel        | 0.517    |
| reward_contact        | 0.986    |
| reward_ctrl           | 0.0419   |
| reward_energy         | 0.44     |
| reward_orientation    | 0.517    |
| reward_position       | 0.717    |
| reward_velocity       | 0.86     |
| rollout/              |          |
|    ep_len_mean        | 3.05e+03 |
|    ep_rew_mean        | 1.2e+04  |
| time/                 |          |
|    fps                | 1091     |
|    iterations         | 2400     |
|    time_elapsed       | 281      |
|    total_timesteps    | 307200   |
| train/                |          |
|    entropy_loss       | -48.7    |
|    explained_variance | 1.19e-07 |
|    learning_rate      | 0.001    |
|    n_updates          | 2399     |
|    policy_loss        | -1.23    |
|    std                | 0.874    |
|    value_loss         | 0.661    |
------------------------------------
-------------------------------------
| reward                | 4.07      |
| reward_ang_vel        | 0.525     |
| reward_contact        | 0.986     |
| reward_ctrl           | 0.0421    |
| reward_energy         | 0.45      |
| reward_orientation    | 0.525     |
| reward_position       | 0.72      |
| reward_velocity       | 0.864     |
| rollout/              |           |
|    ep_len_mean        | 3.03e+03  |
|    ep_rew_mean        | 1.2e+04   |
| time/                 |           |
|    fps                | 1090      |
|    iterations         | 2500      |
|    time_elapsed       | 293       |
|    total_timesteps    | 320000    |
| train/                |           |
|    entropy_loss       | -48.6     |
|    explained_variance | -6.68e-06 |
|    learning_rate      | 0.001     |
|    n_updates          | 2499      |
|    policy_loss        | -0.16     |
|    std                | 0.869     |
|    value_loss         | 1.09      |
-------------------------------------
------------------------------------
| reward                | 4.11     |
| reward_ang_vel        | 0.546    |
| reward_contact        | 0.987    |
| reward_ctrl           | 0.0425   |
| reward_energy         | 0.466    |
| reward_orientation    | 0.546    |
| reward_position       | 0.721    |
| reward_velocity       | 0.869    |
| rollout/              |          |
|    ep_len_mean        | 3.04e+03 |
|    ep_rew_mean        | 1.22e+04 |
| time/                 |          |
|    fps                | 1090     |
|    iterations         | 2600     |
|    time_elapsed       | 305      |
|    total_timesteps    | 332800   |
| train/                |          |
|    entropy_loss       | -48.6    |
|    explained_variance | 0        |
|    learning_rate      | 0.001    |
|    n_updates          | 2599     |
|    policy_loss        | 0.00932  |
|    std                | 0.866    |
|    value_loss         | 0.526    |
------------------------------------
------------------------------------
| reward                | 4.13     |
| reward_ang_vel        | 0.555    |
| reward_contact        | 0.987    |
| reward_ctrl           | 0.0427   |
| reward_energy         | 0.472    |
| reward_orientation    | 0.555    |
| reward_position       | 0.726    |
| reward_velocity       | 0.871    |
| rollout/              |          |
|    ep_len_mean        | 3.06e+03 |
|    ep_rew_mean        | 1.24e+04 |
| time/                 |          |
|    fps                | 1089     |
|    iterations         | 2700     |
|    time_elapsed       | 317      |
|    total_timesteps    | 345600   |
| train/                |          |
|    entropy_loss       | -48.5    |
|    explained_variance | 5.36e-06 |
|    learning_rate      | 0.001    |
|    n_updates          | 2699     |
|    policy_loss        | -1.01    |
|    std                | 0.862    |
|    value_loss         | 0.491    |
------------------------------------
-------------------------------------
| reward                | 4.22      |
| reward_ang_vel        | 0.584     |
| reward_contact        | 0.988     |
| reward_ctrl           | 0.0444    |
| reward_energy         | 0.503     |
| reward_orientation    | 0.584     |
| reward_position       | 0.737     |
| reward_velocity       | 0.885     |
| rollout/              |           |
|    ep_len_mean        | 3.11e+03  |
|    ep_rew_mean        | 1.28e+04  |
| time/                 |           |
|    fps                | 1088      |
|    iterations         | 2800      |
|    time_elapsed       | 329       |
|    total_timesteps    | 358400    |
| train/                |           |
|    entropy_loss       | -48.4     |
|    explained_variance | -8.34e-07 |
|    learning_rate      | 0.001     |
|    n_updates          | 2799      |
|    policy_loss        | 0.204     |
|    std                | 0.856     |
|    value_loss         | 0.364     |
-------------------------------------
-------------------------------------
| reward                | 4.28      |
| reward_ang_vel        | 0.602     |
| reward_contact        | 0.989     |
| reward_ctrl           | 0.0456    |
| reward_energy         | 0.524     |
| reward_orientation    | 0.602     |
| reward_position       | 0.746     |
| reward_velocity       | 0.894     |
| rollout/              |           |
|    ep_len_mean        | 3.14e+03  |
|    ep_rew_mean        | 1.31e+04  |
| time/                 |           |
|    fps                | 1088      |
|    iterations         | 2900      |
|    time_elapsed       | 341       |
|    total_timesteps    | 371200    |
| train/                |           |
|    entropy_loss       | -48.4     |
|    explained_variance | -3.81e-06 |
|    learning_rate      | 0.001     |
|    n_updates          | 2899      |
|    policy_loss        | -0.0313   |
|    std                | 0.853     |
|    value_loss         | 2.15      |
-------------------------------------
Num timesteps: 384000
Best mean reward: 11839.01 - Last mean reward per episode: 13657.89
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 4.35     |
| reward_ang_vel        | 0.627    |
| reward_contact        | 0.989    |
| reward_ctrl           | 0.0471   |
| reward_energy         | 0.557    |
| reward_orientation    | 0.627    |
| reward_position       | 0.762    |
| reward_velocity       | 0.903    |
| rollout/              |          |
|    ep_len_mean        | 3.21e+03 |
|    ep_rew_mean        | 1.37e+04 |
| time/                 |          |
|    fps                | 1087     |
|    iterations         | 3000     |
|    time_elapsed       | 353      |
|    total_timesteps    | 384000   |
| train/                |          |
|    entropy_loss       | -48.4    |
|    explained_variance | 2.09e-06 |
|    learning_rate      | 0.001    |
|    n_updates          | 2999     |
|    policy_loss        | -0.81    |
|    std                | 0.852    |
|    value_loss         | 1.44     |
------------------------------------
------------------------------------
| reward                | 4.43     |
| reward_ang_vel        | 0.655    |
| reward_contact        | 0.99     |
| reward_ctrl           | 0.048    |
| reward_energy         | 0.589    |
| reward_orientation    | 0.655    |
| reward_position       | 0.768    |
| reward_velocity       | 0.91     |
| rollout/              |          |
|    ep_len_mean        | 3.21e+03 |
|    ep_rew_mean        | 1.39e+04 |
| time/                 |          |
|    fps                | 1087     |
|    iterations         | 3100     |
|    time_elapsed       | 364      |
|    total_timesteps    | 396800   |
| train/                |          |
|    entropy_loss       | -48.3    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.001    |
|    n_updates          | 3099     |
|    policy_loss        | -0.0905  |
|    std                | 0.85     |
|    value_loss         | 0.972    |
------------------------------------
------------------------------------
| reward                | 4.46     |
| reward_ang_vel        | 0.673    |
| reward_contact        | 0.99     |
| reward_ctrl           | 0.0489   |
| reward_energy         | 0.604    |
| reward_orientation    | 0.673    |
| reward_position       | 0.768    |
| reward_velocity       | 0.912    |
| rollout/              |          |
|    ep_len_mean        | 3.29e+03 |
|    ep_rew_mean        | 1.44e+04 |
| time/                 |          |
|    fps                | 1086     |
|    iterations         | 3200     |
|    time_elapsed       | 376      |
|    total_timesteps    | 409600   |
| train/                |          |
|    entropy_loss       | -48.3    |
|    explained_variance | 0        |
|    learning_rate      | 0.001    |
|    n_updates          | 3199     |
|    policy_loss        | -1.96    |
|    std                | 0.847    |
|    value_loss         | 0.615    |
------------------------------------
------------------------------------
| reward                | 4.51     |
| reward_ang_vel        | 0.694    |
| reward_contact        | 0.991    |
| reward_ctrl           | 0.0494   |
| reward_energy         | 0.624    |
| reward_orientation    | 0.694    |
| reward_position       | 0.774    |
| reward_velocity       | 0.92     |
| rollout/              |          |
|    ep_len_mean        | 3.25e+03 |
|    ep_rew_mean        | 1.44e+04 |
| time/                 |          |
|    fps                | 1086     |
|    iterations         | 3300     |
|    time_elapsed       | 388      |
|    total_timesteps    | 422400   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0        |
|    learning_rate      | 0.001    |
|    n_updates          | 3299     |
|    policy_loss        | 0.31     |
|    std                | 0.836    |
|    value_loss         | 0.455    |
------------------------------------
------------------------------------
| reward                | 4.59     |
| reward_ang_vel        | 0.726    |
| reward_contact        | 0.991    |
| reward_ctrl           | 0.0499   |
| reward_energy         | 0.648    |
| reward_orientation    | 0.726    |
| reward_position       | 0.778    |
| reward_velocity       | 0.93     |
| rollout/              |          |
|    ep_len_mean        | 3.17e+03 |
|    ep_rew_mean        | 1.44e+04 |
| time/                 |          |
|    fps                | 1086     |
|    iterations         | 3400     |
|    time_elapsed       | 400      |
|    total_timesteps    | 435200   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 1.46e-05 |
|    learning_rate      | 0.001    |
|    n_updates          | 3399     |
|    policy_loss        | 0.144    |
|    std                | 0.837    |
|    value_loss         | 2.17     |
------------------------------------
------------------------------------
| reward                | 4.69     |
| reward_ang_vel        | 0.774    |
| reward_contact        | 0.993    |
| reward_ctrl           | 0.0513   |
| reward_energy         | 0.687    |
| reward_orientation    | 0.774    |
| reward_position       | 0.795    |
| reward_velocity       | 0.943    |
| rollout/              |          |
|    ep_len_mean        | 3.24e+03 |
|    ep_rew_mean        | 1.49e+04 |
| time/                 |          |
|    fps                | 1086     |
|    iterations         | 3500     |
|    time_elapsed       | 412      |
|    total_timesteps    | 448000   |
| train/                |          |
|    entropy_loss       | -48.2    |
|    explained_variance | 0        |
|    learning_rate      | 0.001    |
|    n_updates          | 3499     |
|    policy_loss        | 0.124    |
|    std                | 0.839    |
|    value_loss         | 0.645    |
------------------------------------
------------------------------------
| reward                | 4.76     |
| reward_ang_vel        | 0.803    |
| reward_contact        | 0.993    |
| reward_ctrl           | 0.0505   |
| reward_energy         | 0.706    |
| reward_orientation    | 0.803    |
| reward_position       | 0.802    |
| reward_velocity       | 0.952    |
| rollout/              |          |
|    ep_len_mean        | 3.3e+03  |
|    ep_rew_mean        | 1.55e+04 |
| time/                 |          |
|    fps                | 1085     |
|    iterations         | 3600     |
|    time_elapsed       | 424      |
|    total_timesteps    | 460800   |
| train/                |          |
|    entropy_loss       | -48.2    |
|    explained_variance | 0        |
|    learning_rate      | 0.001    |
|    n_updates          | 3599     |
|    policy_loss        | 0.278    |
|    std                | 0.84     |
|    value_loss         | 1.23     |
------------------------------------
------------------------------------
| reward                | 4.81     |
| reward_ang_vel        | 0.822    |
| reward_contact        | 0.993    |
| reward_ctrl           | 0.0506   |
| reward_energy         | 0.721    |
| reward_orientation    | 0.822    |
| reward_position       | 0.809    |
| reward_velocity       | 0.957    |
| rollout/              |          |
|    ep_len_mean        | 3.3e+03  |
|    ep_rew_mean        | 1.56e+04 |
| time/                 |          |
|    fps                | 1085     |
|    iterations         | 3700     |
|    time_elapsed       | 436      |
|    total_timesteps    | 473600   |
| train/                |          |
|    entropy_loss       | -48.2    |
|    explained_variance | 5.96e-08 |
|    learning_rate      | 0.001    |
|    n_updates          | 3699     |
|    policy_loss        | -0.182   |
|    std                | 0.844    |
|    value_loss         | 0.678    |
------------------------------------
Num timesteps: 480000
Best mean reward: 13657.89 - Last mean reward per episode: 15658.68
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 4.85     |
| reward_ang_vel        | 0.842    |
| reward_contact        | 0.993    |
| reward_ctrl           | 0.0511   |
| reward_energy         | 0.739    |
| reward_orientation    | 0.842    |
| reward_position       | 0.811    |
| reward_velocity       | 0.961    |
| rollout/              |          |
|    ep_len_mean        | 3.3e+03  |
|    ep_rew_mean        | 1.57e+04 |
| time/                 |          |
|    fps                | 1085     |
|    iterations         | 3800     |
|    time_elapsed       | 448      |
|    total_timesteps    | 486400   |
| train/                |          |
|    entropy_loss       | -48.2    |
|    explained_variance | 0        |
|    learning_rate      | 0.001    |
|    n_updates          | 3799     |
|    policy_loss        | 0.457    |
|    std                | 0.842    |
|    value_loss         | 1.28     |
------------------------------------
------------------------------------
| reward                | 4.88     |
| reward_ang_vel        | 0.858    |
| reward_contact        | 0.993    |
| reward_ctrl           | 0.0509   |
| reward_energy         | 0.748    |
| reward_orientation    | 0.858    |
| reward_position       | 0.81     |
| reward_velocity       | 0.964    |
| rollout/              |          |
|    ep_len_mean        | 3.29e+03 |
|    ep_rew_mean        | 1.58e+04 |
| time/                 |          |
|    fps                | 1085     |
|    iterations         | 3900     |
|    time_elapsed       | 459      |
|    total_timesteps    | 499200   |
| train/                |          |
|    entropy_loss       | -48.2    |
|    explained_variance | 5.04e-05 |
|    learning_rate      | 0.001    |
|    n_updates          | 3899     |
|    policy_loss        | -0.469   |
|    std                | 0.841    |
|    value_loss         | 1.37     |
------------------------------------
------------------------------------
| reward                | 4.89     |
| reward_ang_vel        | 0.866    |
| reward_contact        | 0.993    |
| reward_ctrl           | 0.0506   |
| reward_energy         | 0.752    |
| reward_orientation    | 0.866    |
| reward_position       | 0.81     |
| reward_velocity       | 0.966    |
| rollout/              |          |
|    ep_len_mean        | 3.28e+03 |
|    ep_rew_mean        | 1.58e+04 |
| time/                 |          |
|    fps                | 1085     |
|    iterations         | 4000     |
|    time_elapsed       | 471      |
|    total_timesteps    | 512000   |
| train/                |          |
|    entropy_loss       | -48.2    |
|    explained_variance | 1.11e-05 |
|    learning_rate      | 0.001    |
|    n_updates          | 3999     |
|    policy_loss        | 0.346    |
|    std                | 0.842    |
|    value_loss         | 1.39     |
------------------------------------
------------------------------------
| reward                | 4.9      |
| reward_ang_vel        | 0.871    |
| reward_contact        | 0.993    |
| reward_ctrl           | 0.0508   |
| reward_energy         | 0.753    |
| reward_orientation    | 0.871    |
| reward_position       | 0.814    |
| reward_velocity       | 0.968    |
| rollout/              |          |
|    ep_len_mean        | 3.3e+03  |
|    ep_rew_mean        | 1.6e+04  |
| time/                 |          |
|    fps                | 1085     |
|    iterations         | 4100     |
|    time_elapsed       | 483      |
|    total_timesteps    | 524800   |
| train/                |          |
|    entropy_loss       | -48.2    |
|    explained_variance | 6.9e-05  |
|    learning_rate      | 0.001    |
|    n_updates          | 4099     |
|    policy_loss        | 0.455    |
|    std                | 0.84     |
|    value_loss         | 1.26     |
------------------------------------
-------------------------------------
| reward                | 4.91      |
| reward_ang_vel        | 0.875     |
| reward_contact        | 0.994     |
| reward_ctrl           | 0.0509    |
| reward_energy         | 0.757     |
| reward_orientation    | 0.875     |
| reward_position       | 0.809     |
| reward_velocity       | 0.967     |
| rollout/              |           |
|    ep_len_mean        | 3.3e+03   |
|    ep_rew_mean        | 1.61e+04  |
| time/                 |           |
|    fps                | 1084      |
|    iterations         | 4200      |
|    time_elapsed       | 495       |
|    total_timesteps    | 537600    |
| train/                |           |
|    entropy_loss       | -48.2     |
|    explained_variance | -1.84e-05 |
|    learning_rate      | 0.001     |
|    n_updates          | 4199      |
|    policy_loss        | 0.382     |
|    std                | 0.841     |
|    value_loss         | 0.904     |
-------------------------------------
-------------------------------------
| reward                | 4.91      |
| reward_ang_vel        | 0.878     |
| reward_contact        | 0.994     |
| reward_ctrl           | 0.0514    |
| reward_energy         | 0.757     |
| reward_orientation    | 0.878     |
| reward_position       | 0.811     |
| reward_velocity       | 0.968     |
| rollout/              |           |
|    ep_len_mean        | 3.35e+03  |
|    ep_rew_mean        | 1.64e+04  |
| time/                 |           |
|    fps                | 1084      |
|    iterations         | 4300      |
|    time_elapsed       | 507       |
|    total_timesteps    | 550400    |
| train/                |           |
|    entropy_loss       | -48.2     |
|    explained_variance | -4.89e-06 |
|    learning_rate      | 0.001     |
|    n_updates          | 4299      |
|    policy_loss        | -0.893    |
|    std                | 0.84      |
|    value_loss         | 0.846     |
-------------------------------------
------------------------------------
| reward                | 4.92     |
| reward_ang_vel        | 0.882    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0516   |
| reward_energy         | 0.757    |
| reward_orientation    | 0.882    |
| reward_position       | 0.816    |
| reward_velocity       | 0.969    |
| rollout/              |          |
|    ep_len_mean        | 3.35e+03 |
|    ep_rew_mean        | 1.65e+04 |
| time/                 |          |
|    fps                | 1084     |
|    iterations         | 4400     |
|    time_elapsed       | 519      |
|    total_timesteps    | 563200   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0.0266   |
|    learning_rate      | 0.001    |
|    n_updates          | 4399     |
|    policy_loss        | 0.208    |
|    std                | 0.839    |
|    value_loss         | 0.358    |
------------------------------------
Num timesteps: 576000
Best mean reward: 15658.68 - Last mean reward per episode: 16578.49
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 4.92     |
| reward_ang_vel        | 0.878    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0514   |
| reward_energy         | 0.758    |
| reward_orientation    | 0.878    |
| reward_position       | 0.813    |
| reward_velocity       | 0.969    |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 1.66e+04 |
| time/                 |          |
|    fps                | 1084     |
|    iterations         | 4500     |
|    time_elapsed       | 531      |
|    total_timesteps    | 576000   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0.401    |
|    learning_rate      | 0.001    |
|    n_updates          | 4499     |
|    policy_loss        | -0.54    |
|    std                | 0.838    |
|    value_loss         | 0.284    |
------------------------------------
------------------------------------
| reward                | 4.91     |
| reward_ang_vel        | 0.878    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0515   |
| reward_energy         | 0.756    |
| reward_orientation    | 0.878    |
| reward_position       | 0.813    |
| reward_velocity       | 0.963    |
| rollout/              |          |
|    ep_len_mean        | 3.34e+03 |
|    ep_rew_mean        | 1.65e+04 |
| time/                 |          |
|    fps                | 1084     |
|    iterations         | 4600     |
|    time_elapsed       | 542      |
|    total_timesteps    | 588800   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0.927    |
|    learning_rate      | 0.001    |
|    n_updates          | 4599     |
|    policy_loss        | -0.192   |
|    std                | 0.837    |
|    value_loss         | 0.542    |
------------------------------------
------------------------------------
| reward                | 4.91     |
| reward_ang_vel        | 0.877    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0514   |
| reward_energy         | 0.756    |
| reward_orientation    | 0.877    |
| reward_position       | 0.818    |
| reward_velocity       | 0.963    |
| rollout/              |          |
|    ep_len_mean        | 3.35e+03 |
|    ep_rew_mean        | 1.65e+04 |
| time/                 |          |
|    fps                | 1084     |
|    iterations         | 4700     |
|    time_elapsed       | 554      |
|    total_timesteps    | 601600   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0.99     |
|    learning_rate      | 0.001    |
|    n_updates          | 4699     |
|    policy_loss        | -0.53    |
|    std                | 0.837    |
|    value_loss         | 0.839    |
------------------------------------
------------------------------------
| reward                | 4.91     |
| reward_ang_vel        | 0.88     |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0513   |
| reward_energy         | 0.756    |
| reward_orientation    | 0.88     |
| reward_position       | 0.817    |
| reward_velocity       | 0.964    |
| rollout/              |          |
|    ep_len_mean        | 3.33e+03 |
|    ep_rew_mean        | 1.65e+04 |
| time/                 |          |
|    fps                | 1084     |
|    iterations         | 4800     |
|    time_elapsed       | 566      |
|    total_timesteps    | 614400   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 4799     |
|    policy_loss        | 0.0505   |
|    std                | 0.835    |
|    value_loss         | 0.975    |
------------------------------------
------------------------------------
| reward                | 4.91     |
| reward_ang_vel        | 0.883    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0513   |
| reward_energy         | 0.756    |
| reward_orientation    | 0.883    |
| reward_position       | 0.814    |
| reward_velocity       | 0.963    |
| rollout/              |          |
|    ep_len_mean        | 3.39e+03 |
|    ep_rew_mean        | 1.68e+04 |
| time/                 |          |
|    fps                | 1084     |
|    iterations         | 4900     |
|    time_elapsed       | 578      |
|    total_timesteps    | 627200   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 4899     |
|    policy_loss        | 0.405    |
|    std                | 0.835    |
|    value_loss         | 0.229    |
------------------------------------
------------------------------------
| reward                | 4.92     |
| reward_ang_vel        | 0.887    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0516   |
| reward_energy         | 0.758    |
| reward_orientation    | 0.887    |
| reward_position       | 0.813    |
| reward_velocity       | 0.968    |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 1.7e+04  |
| time/                 |          |
|    fps                | 1083     |
|    iterations         | 5000     |
|    time_elapsed       | 590      |
|    total_timesteps    | 640000   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0.779    |
|    learning_rate      | 0.001    |
|    n_updates          | 4999     |
|    policy_loss        | -0.254   |
|    std                | 0.833    |
|    value_loss         | 0.467    |
------------------------------------
------------------------------------
| reward                | 4.92     |
| reward_ang_vel        | 0.888    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0515   |
| reward_energy         | 0.758    |
| reward_orientation    | 0.888    |
| reward_position       | 0.813    |
| reward_velocity       | 0.968    |
| rollout/              |          |
|    ep_len_mean        | 3.5e+03  |
|    ep_rew_mean        | 1.73e+04 |
| time/                 |          |
|    fps                | 1083     |
|    iterations         | 5100     |
|    time_elapsed       | 602      |
|    total_timesteps    | 652800   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0.463    |
|    learning_rate      | 0.001    |
|    n_updates          | 5099     |
|    policy_loss        | -0.758   |
|    std                | 0.835    |
|    value_loss         | 1.05     |
------------------------------------
------------------------------------
| reward                | 4.92     |
| reward_ang_vel        | 0.888    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0515   |
| reward_energy         | 0.758    |
| reward_orientation    | 0.888    |
| reward_position       | 0.81     |
| reward_velocity       | 0.968    |
| rollout/              |          |
|    ep_len_mean        | 3.5e+03  |
|    ep_rew_mean        | 1.73e+04 |
| time/                 |          |
|    fps                | 1083     |
|    iterations         | 5200     |
|    time_elapsed       | 614      |
|    total_timesteps    | 665600   |
| train/                |          |
|    entropy_loss       | -48      |
|    explained_variance | 0.972    |
|    learning_rate      | 0.001    |
|    n_updates          | 5199     |
|    policy_loss        | -0.289   |
|    std                | 0.832    |
|    value_loss         | 0.298    |
------------------------------------
Num timesteps: 672000
Best mean reward: 16578.49 - Last mean reward per episode: 17219.35
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 4.93     |
| reward_ang_vel        | 0.89     |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0514   |
| reward_energy         | 0.759    |
| reward_orientation    | 0.89     |
| reward_position       | 0.816    |
| reward_velocity       | 0.969    |
| rollout/              |          |
|    ep_len_mean        | 3.51e+03 |
|    ep_rew_mean        | 1.74e+04 |
| time/                 |          |
|    fps                | 1083     |
|    iterations         | 5300     |
|    time_elapsed       | 626      |
|    total_timesteps    | 678400   |
| train/                |          |
|    entropy_loss       | -48      |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 5299     |
|    policy_loss        | -0.113   |
|    std                | 0.832    |
|    value_loss         | 0.696    |
------------------------------------
------------------------------------
| reward                | 4.92     |
| reward_ang_vel        | 0.89     |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0517   |
| reward_energy         | 0.759    |
| reward_orientation    | 0.89     |
| reward_position       | 0.814    |
| reward_velocity       | 0.97     |
| rollout/              |          |
|    ep_len_mean        | 3.52e+03 |
|    ep_rew_mean        | 1.75e+04 |
| time/                 |          |
|    fps                | 1083     |
|    iterations         | 5400     |
|    time_elapsed       | 638      |
|    total_timesteps    | 691200   |
| train/                |          |
|    entropy_loss       | -48      |
|    explained_variance | 0.853    |
|    learning_rate      | 0.001    |
|    n_updates          | 5399     |
|    policy_loss        | -0.0327  |
|    std                | 0.832    |
|    value_loss         | 3.64     |
------------------------------------
------------------------------------
| reward                | 4.93     |
| reward_ang_vel        | 0.893    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0515   |
| reward_energy         | 0.759    |
| reward_orientation    | 0.893    |
| reward_position       | 0.815    |
| reward_velocity       | 0.97     |
| rollout/              |          |
|    ep_len_mean        | 3.53e+03 |
|    ep_rew_mean        | 1.75e+04 |
| time/                 |          |
|    fps                | 1083     |
|    iterations         | 5500     |
|    time_elapsed       | 649      |
|    total_timesteps    | 704000   |
| train/                |          |
|    entropy_loss       | -48      |
|    explained_variance | 0.85     |
|    learning_rate      | 0.001    |
|    n_updates          | 5499     |
|    policy_loss        | 0.0815   |
|    std                | 0.826    |
|    value_loss         | 3.49     |
------------------------------------
------------------------------------
| reward                | 4.93     |
| reward_ang_vel        | 0.894    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0514   |
| reward_energy         | 0.759    |
| reward_orientation    | 0.894    |
| reward_position       | 0.814    |
| reward_velocity       | 0.97     |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 1.78e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 5600     |
|    time_elapsed       | 662      |
|    total_timesteps    | 716800   |
| train/                |          |
|    entropy_loss       | -47.9    |
|    explained_variance | 0.775    |
|    learning_rate      | 0.001    |
|    n_updates          | 5599     |
|    policy_loss        | -0.263   |
|    std                | 0.824    |
|    value_loss         | 4.77     |
------------------------------------
------------------------------------
| reward                | 4.93     |
| reward_ang_vel        | 0.895    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0516   |
| reward_energy         | 0.759    |
| reward_orientation    | 0.895    |
| reward_position       | 0.816    |
| reward_velocity       | 0.971    |
| rollout/              |          |
|    ep_len_mean        | 3.64e+03 |
|    ep_rew_mean        | 1.81e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 5700     |
|    time_elapsed       | 673      |
|    total_timesteps    | 729600   |
| train/                |          |
|    entropy_loss       | -47.9    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 5699     |
|    policy_loss        | -0.241   |
|    std                | 0.825    |
|    value_loss         | 0.337    |
------------------------------------
------------------------------------
| reward                | 4.93     |
| reward_ang_vel        | 0.899    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0518   |
| reward_energy         | 0.759    |
| reward_orientation    | 0.899    |
| reward_position       | 0.809    |
| reward_velocity       | 0.97     |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 1.77e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 5800     |
|    time_elapsed       | 685      |
|    total_timesteps    | 742400   |
| train/                |          |
|    entropy_loss       | -47.9    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 5799     |
|    policy_loss        | 0.0756   |
|    std                | 0.823    |
|    value_loss         | 0.379    |
------------------------------------
------------------------------------
| reward                | 4.94     |
| reward_ang_vel        | 0.903    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0518   |
| reward_energy         | 0.759    |
| reward_orientation    | 0.903    |
| reward_position       | 0.817    |
| reward_velocity       | 0.971    |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 1.78e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 5900     |
|    time_elapsed       | 697      |
|    total_timesteps    | 755200   |
| train/                |          |
|    entropy_loss       | -47.9    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 5899     |
|    policy_loss        | -0.568   |
|    std                | 0.819    |
|    value_loss         | 0.284    |
------------------------------------
Num timesteps: 768000
Best mean reward: 17219.35 - Last mean reward per episode: 17698.03
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 4.95     |
| reward_ang_vel        | 0.908    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0517   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.908    |
| reward_position       | 0.817    |
| reward_velocity       | 0.971    |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 1.77e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 6000     |
|    time_elapsed       | 709      |
|    total_timesteps    | 768000   |
| train/                |          |
|    entropy_loss       | -47.8    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 5999     |
|    policy_loss        | 0.204    |
|    std                | 0.818    |
|    value_loss         | 0.297    |
------------------------------------
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.917    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0513   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.917    |
| reward_position       | 0.827    |
| reward_velocity       | 0.974    |
| rollout/              |          |
|    ep_len_mean        | 3.63e+03 |
|    ep_rew_mean        | 1.81e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 6100     |
|    time_elapsed       | 721      |
|    total_timesteps    | 780800   |
| train/                |          |
|    entropy_loss       | -47.8    |
|    explained_variance | 0.97     |
|    learning_rate      | 0.001    |
|    n_updates          | 6099     |
|    policy_loss        | 0.00411  |
|    std                | 0.816    |
|    value_loss         | 0.0358   |
------------------------------------
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.919    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0516   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.919    |
| reward_position       | 0.829    |
| reward_velocity       | 0.974    |
| rollout/              |          |
|    ep_len_mean        | 3.64e+03 |
|    ep_rew_mean        | 1.81e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 6200     |
|    time_elapsed       | 733      |
|    total_timesteps    | 793600   |
| train/                |          |
|    entropy_loss       | -47.8    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 6199     |
|    policy_loss        | -0.178   |
|    std                | 0.817    |
|    value_loss         | 2.11     |
------------------------------------
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.921    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0517   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.921    |
| reward_position       | 0.831    |
| reward_velocity       | 0.974    |
| rollout/              |          |
|    ep_len_mean        | 3.63e+03 |
|    ep_rew_mean        | 1.81e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 6300     |
|    time_elapsed       | 745      |
|    total_timesteps    | 806400   |
| train/                |          |
|    entropy_loss       | -47.8    |
|    explained_variance | 0.777    |
|    learning_rate      | 0.001    |
|    n_updates          | 6299     |
|    policy_loss        | 0.00274  |
|    std                | 0.815    |
|    value_loss         | 113      |
------------------------------------
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.921    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0517   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.921    |
| reward_position       | 0.828    |
| reward_velocity       | 0.974    |
| rollout/              |          |
|    ep_len_mean        | 3.63e+03 |
|    ep_rew_mean        | 1.81e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 6400     |
|    time_elapsed       | 757      |
|    total_timesteps    | 819200   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.961    |
|    learning_rate      | 0.001    |
|    n_updates          | 6399     |
|    policy_loss        | 0.286    |
|    std                | 0.811    |
|    value_loss         | 0.0541   |
------------------------------------
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.924    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0521   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.924    |
| reward_position       | 0.832    |
| reward_velocity       | 0.975    |
| rollout/              |          |
|    ep_len_mean        | 3.64e+03 |
|    ep_rew_mean        | 1.82e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 6500     |
|    time_elapsed       | 768      |
|    total_timesteps    | 832000   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.975    |
|    learning_rate      | 0.001    |
|    n_updates          | 6499     |
|    policy_loss        | -0.0681  |
|    std                | 0.811    |
|    value_loss         | 6.98     |
------------------------------------
------------------------------------
| reward                | 4.96     |
| reward_ang_vel        | 0.919    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0522   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.919    |
| reward_position       | 0.829    |
| reward_velocity       | 0.973    |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 1.79e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 6600     |
|    time_elapsed       | 780      |
|    total_timesteps    | 844800   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.746    |
|    learning_rate      | 0.001    |
|    n_updates          | 6599     |
|    policy_loss        | 0.499    |
|    std                | 0.809    |
|    value_loss         | 29.7     |
------------------------------------
------------------------------------
| reward                | 4.96     |
| reward_ang_vel        | 0.919    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0523   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.919    |
| reward_position       | 0.832    |
| reward_velocity       | 0.974    |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 1.79e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 6700     |
|    time_elapsed       | 792      |
|    total_timesteps    | 857600   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.988    |
|    learning_rate      | 0.001    |
|    n_updates          | 6699     |
|    policy_loss        | 0.0629   |
|    std                | 0.808    |
|    value_loss         | 1.95     |
------------------------------------
Num timesteps: 864000
Best mean reward: 17698.03 - Last mean reward per episode: 17946.81
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.922    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0522   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.922    |
| reward_position       | 0.832    |
| reward_velocity       | 0.975    |
| rollout/              |          |
|    ep_len_mean        | 3.66e+03 |
|    ep_rew_mean        | 1.83e+04 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 6800     |
|    time_elapsed       | 804      |
|    total_timesteps    | 870400   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.847    |
|    learning_rate      | 0.001    |
|    n_updates          | 6799     |
|    policy_loss        | 0.943    |
|    std                | 0.805    |
|    value_loss         | 51.5     |
------------------------------------
------------------------------------
| reward                | 4.96     |
| reward_ang_vel        | 0.922    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0517   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.922    |
| reward_position       | 0.825    |
| reward_velocity       | 0.973    |
| rollout/              |          |
|    ep_len_mean        | 3.67e+03 |
|    ep_rew_mean        | 1.83e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6900     |
|    time_elapsed       | 816      |
|    total_timesteps    | 883200   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 6899     |
|    policy_loss        | 0.103    |
|    std                | 0.804    |
|    value_loss         | 0.253    |
------------------------------------
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.925    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0515   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.925    |
| reward_position       | 0.832    |
| reward_velocity       | 0.976    |
| rollout/              |          |
|    ep_len_mean        | 3.69e+03 |
|    ep_rew_mean        | 1.84e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7000     |
|    time_elapsed       | 828      |
|    total_timesteps    | 896000   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.985    |
|    learning_rate      | 0.001    |
|    n_updates          | 6999     |
|    policy_loss        | 0.241    |
|    std                | 0.806    |
|    value_loss         | 0.0883   |
------------------------------------
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.928    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0513   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.928    |
| reward_position       | 0.835    |
| reward_velocity       | 0.976    |
| rollout/              |          |
|    ep_len_mean        | 3.73e+03 |
|    ep_rew_mean        | 1.86e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7100     |
|    time_elapsed       | 840      |
|    total_timesteps    | 908800   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.991    |
|    learning_rate      | 0.001    |
|    n_updates          | 7099     |
|    policy_loss        | 0.139    |
|    std                | 0.806    |
|    value_loss         | 0.181    |
------------------------------------
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.928    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0513   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.928    |
| reward_position       | 0.835    |
| reward_velocity       | 0.976    |
| rollout/              |          |
|    ep_len_mean        | 3.73e+03 |
|    ep_rew_mean        | 1.86e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7200     |
|    time_elapsed       | 851      |
|    total_timesteps    | 921600   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 7199     |
|    policy_loss        | -0.192   |
|    std                | 0.804    |
|    value_loss         | 0.173    |
------------------------------------
------------------------------------
| reward                | 4.98     |
| reward_ang_vel        | 0.927    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0515   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.927    |
| reward_position       | 0.842    |
| reward_velocity       | 0.977    |
| rollout/              |          |
|    ep_len_mean        | 3.72e+03 |
|    ep_rew_mean        | 1.86e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7300     |
|    time_elapsed       | 863      |
|    total_timesteps    | 934400   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 7299     |
|    policy_loss        | 0.000695 |
|    std                | 0.804    |
|    value_loss         | 2.46     |
------------------------------------
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.932    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0517   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.932    |
| reward_position       | 0.829    |
| reward_velocity       | 0.976    |
| rollout/              |          |
|    ep_len_mean        | 3.7e+03  |
|    ep_rew_mean        | 1.85e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7400     |
|    time_elapsed       | 875      |
|    total_timesteps    | 947200   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 1        |
|    learning_rate      | 0.001    |
|    n_updates          | 7399     |
|    policy_loss        | -0.206   |
|    std                | 0.803    |
|    value_loss         | 0.161    |
------------------------------------
Num timesteps: 960000
Best mean reward: 17946.81 - Last mean reward per episode: 18593.73
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 4.97     |
| reward_ang_vel        | 0.932    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0517   |
| reward_energy         | 0.764    |
| reward_orientation    | 0.932    |
| reward_position       | 0.829    |
| reward_velocity       | 0.976    |
| rollout/              |          |
|    ep_len_mean        | 3.72e+03 |
|    ep_rew_mean        | 1.86e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7500     |
|    time_elapsed       | 887      |
|    total_timesteps    | 960000   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.775    |
|    learning_rate      | 0.001    |
|    n_updates          | 7499     |
|    policy_loss        | -0.664   |
|    std                | 0.802    |
|    value_loss         | 45.3     |
------------------------------------
------------------------------------
| reward                | 4.98     |
| reward_ang_vel        | 0.94     |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0511   |
| reward_energy         | 0.766    |
| reward_orientation    | 0.94     |
| reward_position       | 0.821    |
| reward_velocity       | 0.981    |
| rollout/              |          |
|    ep_len_mean        | 3.66e+03 |
|    ep_rew_mean        | 1.83e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7600     |
|    time_elapsed       | 899      |
|    total_timesteps    | 972800   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 7599     |
|    policy_loss        | 0.219    |
|    std                | 0.799    |
|    value_loss         | 1.34     |
------------------------------------
------------------------------------
| reward                | 4.99     |
| reward_ang_vel        | 0.934    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0515   |
| reward_energy         | 0.766    |
| reward_orientation    | 0.934    |
| reward_position       | 0.831    |
| reward_velocity       | 0.982    |
| rollout/              |          |
|    ep_len_mean        | 3.65e+03 |
|    ep_rew_mean        | 1.82e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7700     |
|    time_elapsed       | 911      |
|    total_timesteps    | 985600   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 7699     |
|    policy_loss        | 0.168    |
|    std                | 0.798    |
|    value_loss         | 0.846    |
------------------------------------
------------------------------------
| reward                | 4.99     |
| reward_ang_vel        | 0.934    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0516   |
| reward_energy         | 0.766    |
| reward_orientation    | 0.934    |
| reward_position       | 0.833    |
| reward_velocity       | 0.983    |
| rollout/              |          |
|    ep_len_mean        | 3.65e+03 |
|    ep_rew_mean        | 1.83e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7800     |
|    time_elapsed       | 923      |
|    total_timesteps    | 998400   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 1        |
|    learning_rate      | 0.001    |
|    n_updates          | 7799     |
|    policy_loss        | 0.124    |
|    std                | 0.798    |
|    value_loss         | 0.06     |
------------------------------------
------------------------------------
| reward                | 4.99     |
| reward_ang_vel        | 0.94     |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0517   |
| reward_energy         | 0.762    |
| reward_orientation    | 0.94     |
| reward_position       | 0.83     |
| reward_velocity       | 0.982    |
| rollout/              |          |
|    ep_len_mean        | 3.73e+03 |
|    ep_rew_mean        | 1.86e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 7900     |
|    time_elapsed       | 934      |
|    total_timesteps    | 1011200  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 1        |
|    learning_rate      | 0.001    |
|    n_updates          | 7899     |
|    policy_loss        | -0.24    |
|    std                | 0.795    |
|    value_loss         | 0.0428   |
------------------------------------
------------------------------------
| reward                | 4.99     |
| reward_ang_vel        | 0.943    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0515   |
| reward_energy         | 0.762    |
| reward_orientation    | 0.943    |
| reward_position       | 0.831    |
| reward_velocity       | 0.982    |
| rollout/              |          |
|    ep_len_mean        | 3.67e+03 |
|    ep_rew_mean        | 1.84e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 8000     |
|    time_elapsed       | 946      |
|    total_timesteps    | 1024000  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 7999     |
|    policy_loss        | 0.409    |
|    std                | 0.792    |
|    value_loss         | 0.295    |
------------------------------------
------------------------------------
| reward                | 4.99     |
| reward_ang_vel        | 0.947    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0513   |
| reward_energy         | 0.762    |
| reward_orientation    | 0.947    |
| reward_position       | 0.83     |
| reward_velocity       | 0.982    |
| rollout/              |          |
|    ep_len_mean        | 3.74e+03 |
|    ep_rew_mean        | 1.87e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 8100     |
|    time_elapsed       | 958      |
|    total_timesteps    | 1036800  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 8099     |
|    policy_loss        | -0.045   |
|    std                | 0.793    |
|    value_loss         | 0.421    |
------------------------------------
------------------------------------
| reward                | 4.99     |
| reward_ang_vel        | 0.946    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0508   |
| reward_energy         | 0.762    |
| reward_orientation    | 0.946    |
| reward_position       | 0.828    |
| reward_velocity       | 0.982    |
| rollout/              |          |
|    ep_len_mean        | 3.7e+03  |
|    ep_rew_mean        | 1.85e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 8200     |
|    time_elapsed       | 970      |
|    total_timesteps    | 1049600  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 8199     |
|    policy_loss        | 0.309    |
|    std                | 0.796    |
|    value_loss         | 0.0422   |
------------------------------------
Num timesteps: 1056000
Best mean reward: 18593.73 - Last mean reward per episode: 18513.09
------------------------------------
| reward                | 4.99     |
| reward_ang_vel        | 0.947    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0507   |
| reward_energy         | 0.762    |
| reward_orientation    | 0.947    |
| reward_position       | 0.827    |
| reward_velocity       | 0.982    |
| rollout/              |          |
|    ep_len_mean        | 3.73e+03 |
|    ep_rew_mean        | 1.87e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 8300     |
|    time_elapsed       | 982      |
|    total_timesteps    | 1062400  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 8299     |
|    policy_loss        | 0.375    |
|    std                | 0.795    |
|    value_loss         | 2.21     |
------------------------------------
------------------------------------
| reward                | 4.99     |
| reward_ang_vel        | 0.945    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0505   |
| reward_energy         | 0.762    |
| reward_orientation    | 0.945    |
| reward_position       | 0.83     |
| reward_velocity       | 0.982    |
| rollout/              |          |
|    ep_len_mean        | 3.66e+03 |
|    ep_rew_mean        | 1.83e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 8400     |
|    time_elapsed       | 994      |
|    total_timesteps    | 1075200  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 8399     |
|    policy_loss        | -0.163   |
|    std                | 0.794    |
|    value_loss         | 1.12     |
------------------------------------
------------------------------------
| reward                | 4.99     |
| reward_ang_vel        | 0.946    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0503   |
| reward_energy         | 0.762    |
| reward_orientation    | 0.946    |
| reward_position       | 0.828    |
| reward_velocity       | 0.982    |
| rollout/              |          |
|    ep_len_mean        | 3.63e+03 |
|    ep_rew_mean        | 1.82e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 8500     |
|    time_elapsed       | 1006     |
|    total_timesteps    | 1088000  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.881    |
|    learning_rate      | 0.001    |
|    n_updates          | 8499     |
|    policy_loss        | -0.0488  |
|    std                | 0.795    |
|    value_loss         | 5.88     |
------------------------------------
------------------------------------
| reward                | 5        |
| reward_ang_vel        | 0.948    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.05     |
| reward_energy         | 0.762    |
| reward_orientation    | 0.948    |
| reward_position       | 0.833    |
| reward_velocity       | 0.983    |
| rollout/              |          |
|    ep_len_mean        | 3.71e+03 |
|    ep_rew_mean        | 1.86e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 8600     |
|    time_elapsed       | 1018     |
|    total_timesteps    | 1100800  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.991    |
|    learning_rate      | 0.001    |
|    n_updates          | 8599     |
|    policy_loss        | 0.421    |
|    std                | 0.791    |
|    value_loss         | 1.4      |
------------------------------------
-------------------------------------
| reward                | 5         |
| reward_ang_vel        | 0.948     |
| reward_contact        | 0.994     |
| reward_ctrl           | 0.05      |
| reward_energy         | 0.762     |
| reward_orientation    | 0.948     |
| reward_position       | 0.833     |
| reward_velocity       | 0.983     |
| rollout/              |           |
|    ep_len_mean        | 3.75e+03  |
|    ep_rew_mean        | 1.88e+04  |
| time/                 |           |
|    fps                | 1081      |
|    iterations         | 8700      |
|    time_elapsed       | 1030      |
|    total_timesteps    | 1113600   |
| train/                |           |
|    entropy_loss       | -47.4     |
|    explained_variance | 0.999     |
|    learning_rate      | 0.001     |
|    n_updates          | 8699      |
|    policy_loss        | -0.000264 |
|    std                | 0.788     |
|    value_loss         | 0.0396    |
-------------------------------------
------------------------------------
| reward                | 5.01     |
| reward_ang_vel        | 0.952    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0502   |
| reward_energy         | 0.762    |
| reward_orientation    | 0.952    |
| reward_position       | 0.839    |
| reward_velocity       | 0.984    |
| rollout/              |          |
|    ep_len_mean        | 3.76e+03 |
|    ep_rew_mean        | 1.89e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 8800     |
|    time_elapsed       | 1041     |
|    total_timesteps    | 1126400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.972    |
|    learning_rate      | 0.001    |
|    n_updates          | 8799     |
|    policy_loss        | -0.0524  |
|    std                | 0.787    |
|    value_loss         | 5.21     |
------------------------------------
------------------------------------
| reward                | 5        |
| reward_ang_vel        | 0.953    |
| reward_contact        | 0.994    |
| reward_ctrl           | 0.0501   |
| reward_energy         | 0.762    |
| reward_orientation    | 0.953    |
| reward_position       | 0.834    |
| reward_velocity       | 0.983    |
| rollout/              |          |
|    ep_len_mean        | 3.67e+03 |
|    ep_rew_mean        | 1.84e+04 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 8900     |
|    time_elapsed       | 1053     |
|    total_timesteps    | 1139200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.969    |
|    learning_rate      | 0.001    |
|    n_updates          | 8899     |
|    policy_loss        | -0.533   |
|    std                | 0.786    |
|    value_loss         | 0.218    |
------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp2/A2C_23
------------------------------------
| time/                 |          |
|    fps                | 1093     |
|    iterations         | 100      |
|    time_elapsed       | 11       |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -49.9    |
|    explained_variance | 0.145    |
|    learning_rate      | 0.001    |
|    n_updates          | 99       |
|    policy_loss        | -0.257   |
|    std                | 0.97     |
|    value_loss         | 0.0151   |
------------------------------------
------------------------------------
| reward                | 0.41     |
| reward_ang_vel        | 0.0482   |
| reward_contact        | 0.0982   |
| reward_ctrl           | 0.0596   |
| reward_energy         | 0.0175   |
| reward_orientation    | 0.0482   |
| reward_position       | 0.0557   |
| reward_velocity       | 0.0931   |
| rollout/              |          |
|    ep_len_mean        | 1.5e+03  |
|    ep_rew_mean        | 555      |
| time/                 |          |
|    fps                | 1089     |
|    iterations         | 200      |
|    time_elapsed       | 23       |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -49.7    |
|    explained_variance | 0.144    |
|    learning_rate      | 0.001    |
|    n_updates          | 199      |
|    policy_loss        | -0.215   |
|    std                | 0.952    |
|    value_loss         | 0.0209   |
------------------------------------
------------------------------------
| reward                | 0.449    |
| reward_ang_vel        | 0.0567   |
| reward_contact        | 0.0987   |
| reward_ctrl           | 0.0522   |
| reward_energy         | 0.0387   |
| reward_orientation    | 0.0567   |
| reward_position       | 0.0664   |
| reward_velocity       | 0.0935   |
| rollout/              |          |
|    ep_len_mean        | 2.13e+03 |
|    ep_rew_mean        | 834      |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 300      |
|    time_elapsed       | 35       |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -49.5    |
|    explained_variance | 0.646    |
|    learning_rate      | 0.001    |
|    n_updates          | 299      |
|    policy_loss        | -0.36    |
|    std                | 0.938    |
|    value_loss         | 0.0304   |
------------------------------------
------------------------------------
| reward                | 0.431    |
| reward_ang_vel        | 0.0483   |
| reward_contact        | 0.0985   |
| reward_ctrl           | 0.053    |
| reward_energy         | 0.0313   |
| reward_orientation    | 0.0483   |
| reward_position       | 0.0695   |
| reward_velocity       | 0.0891   |
| rollout/              |          |
|    ep_len_mean        | 2.33e+03 |
|    ep_rew_mean        | 923      |
| time/                 |          |
|    fps                | 1077     |
|    iterations         | 400      |
|    time_elapsed       | 47       |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -49.4    |
|    explained_variance | 0.705    |
|    learning_rate      | 0.001    |
|    n_updates          | 399      |
|    policy_loss        | -0.591   |
|    std                | 0.927    |
|    value_loss         | 0.0222   |
------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp2/A2C_24
------------------------------------
| time/                 |          |
|    fps                | 1109     |
|    iterations         | 100      |
|    time_elapsed       | 11       |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -49.9    |
|    explained_variance | 0.361    |
|    learning_rate      | 0.001    |
|    n_updates          | 99       |
|    policy_loss        | 0.625    |
|    std                | 0.972    |
|    value_loss         | 0.0129   |
------------------------------------
------------------------------------
| time/                 |          |
|    fps                | 1111     |
|    iterations         | 200      |
|    time_elapsed       | 23       |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -49.8    |
|    explained_variance | 0.0479   |
|    learning_rate      | 0.001    |
|    n_updates          | 199      |
|    policy_loss        | -0.388   |
|    std                | 0.963    |
|    value_loss         | 0.0217   |
------------------------------------
------------------------------------
| reward                | 0.37     |
| reward_ang_vel        | 0.0153   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.0548   |
| reward_energy         | 0.0185   |
| reward_orientation    | 0.0153   |
| reward_position       | 0.0619   |
| reward_velocity       | 0.0741   |
| rollout/              |          |
|    ep_len_mean        | 2.07e+03 |
|    ep_rew_mean        | 767      |
| time/                 |          |
|    fps                | 1106     |
|    iterations         | 300      |
|    time_elapsed       | 34       |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -49.8    |
|    explained_variance | 0.62     |
|    learning_rate      | 0.001    |
|    n_updates          | 299      |
|    policy_loss        | 0.258    |
|    std                | 0.959    |
|    value_loss         | 0.013    |
------------------------------------
------------------------------------
| reward                | 0.374    |
| reward_ang_vel        | 0.0162   |
| reward_contact        | 0.0985   |
| reward_ctrl           | 0.0537   |
| reward_energy         | 0.0213   |
| reward_orientation    | 0.0162   |
| reward_position       | 0.062    |
| reward_velocity       | 0.0778   |
| rollout/              |          |
|    ep_len_mean        | 2.51e+03 |
|    ep_rew_mean        | 941      |
| time/                 |          |
|    fps                | 1101     |
|    iterations         | 400      |
|    time_elapsed       | 46       |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -49.7    |
|    explained_variance | 0.339    |
|    learning_rate      | 0.001    |
|    n_updates          | 399      |
|    policy_loss        | -0.218   |
|    std                | 0.949    |
|    value_loss         | 0.0324   |
------------------------------------
------------------------------------
| reward                | 0.377    |
| reward_ang_vel        | 0.0169   |
| reward_contact        | 0.0985   |
| reward_ctrl           | 0.0554   |
| reward_energy         | 0.0227   |
| reward_orientation    | 0.0169   |
| reward_position       | 0.0626   |
| reward_velocity       | 0.0776   |
| rollout/              |          |
|    ep_len_mean        | 2.66e+03 |
|    ep_rew_mean        | 1e+03    |
| time/                 |          |
|    fps                | 1100     |
|    iterations         | 500      |
|    time_elapsed       | 58       |
|    total_timesteps    | 64000    |
| train/                |          |
|    entropy_loss       | -49.5    |
|    explained_variance | 0.354    |
|    learning_rate      | 0.001    |
|    n_updates          | 499      |
|    policy_loss        | -0.541   |
|    std                | 0.938    |
|    value_loss         | 0.0303   |
------------------------------------
------------------------------------
| reward                | 0.382    |
| reward_ang_vel        | 0.0156   |
| reward_contact        | 0.0985   |
| reward_ctrl           | 0.057    |
| reward_energy         | 0.0263   |
| reward_orientation    | 0.0156   |
| reward_position       | 0.0631   |
| reward_velocity       | 0.0779   |
| rollout/              |          |
|    ep_len_mean        | 2.79e+03 |
|    ep_rew_mean        | 1.06e+03 |
| time/                 |          |
|    fps                | 1099     |
|    iterations         | 600      |
|    time_elapsed       | 69       |
|    total_timesteps    | 76800    |
| train/                |          |
|    entropy_loss       | -49.4    |
|    explained_variance | 0.745    |
|    learning_rate      | 0.001    |
|    n_updates          | 599      |
|    policy_loss        | -0.628   |
|    std                | 0.929    |
|    value_loss         | 0.0312   |
------------------------------------
------------------------------------
| reward                | 0.388    |
| reward_ang_vel        | 0.0186   |
| reward_contact        | 0.0985   |
| reward_ctrl           | 0.0556   |
| reward_energy         | 0.0291   |
| reward_orientation    | 0.0186   |
| reward_position       | 0.0637   |
| reward_velocity       | 0.08     |
| rollout/              |          |
|    ep_len_mean        | 2.79e+03 |
|    ep_rew_mean        | 1.08e+03 |
| time/                 |          |
|    fps                | 1097     |
|    iterations         | 700      |
|    time_elapsed       | 81       |
|    total_timesteps    | 89600    |
| train/                |          |
|    entropy_loss       | -49.3    |
|    explained_variance | 0.886    |
|    learning_rate      | 0.001    |
|    n_updates          | 699      |
|    policy_loss        | -0.212   |
|    std                | 0.921    |
|    value_loss         | 0.0237   |
------------------------------------
Num timesteps: 96000
Best mean reward: -inf - Last mean reward per episode: 1120.84
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 0.406    |
| reward_ang_vel        | 0.0247   |
| reward_contact        | 0.0986   |
| reward_ctrl           | 0.0545   |
| reward_energy         | 0.0355   |
| reward_orientation    | 0.0247   |
| reward_position       | 0.066    |
| reward_velocity       | 0.0815   |
| rollout/              |          |
|    ep_len_mean        | 2.86e+03 |
|    ep_rew_mean        | 1.14e+03 |
| time/                 |          |
|    fps                | 1094     |
|    iterations         | 800      |
|    time_elapsed       | 93       |
|    total_timesteps    | 102400   |
| train/                |          |
|    entropy_loss       | -49.1    |
|    explained_variance | 0.863    |
|    learning_rate      | 0.001    |
|    n_updates          | 799      |
|    policy_loss        | 0.363    |
|    std                | 0.905    |
|    value_loss         | 0.0331   |
------------------------------------
------------------------------------
| reward                | 0.42     |
| reward_ang_vel        | 0.0289   |
| reward_contact        | 0.0987   |
| reward_ctrl           | 0.0546   |
| reward_energy         | 0.0399   |
| reward_orientation    | 0.0289   |
| reward_position       | 0.0695   |
| reward_velocity       | 0.0832   |
| rollout/              |          |
|    ep_len_mean        | 2.91e+03 |
|    ep_rew_mean        | 1.18e+03 |
| time/                 |          |
|    fps                | 1093     |
|    iterations         | 900      |
|    time_elapsed       | 105      |
|    total_timesteps    | 115200   |
| train/                |          |
|    entropy_loss       | -48.9    |
|    explained_variance | 0.777    |
|    learning_rate      | 0.001    |
|    n_updates          | 899      |
|    policy_loss        | 0.57     |
|    std                | 0.895    |
|    value_loss         | 0.0319   |
------------------------------------
------------------------------------
| reward                | 0.432    |
| reward_ang_vel        | 0.0366   |
| reward_contact        | 0.0988   |
| reward_ctrl           | 0.0537   |
| reward_energy         | 0.0441   |
| reward_orientation    | 0.0366   |
| reward_position       | 0.069    |
| reward_velocity       | 0.0851   |
| rollout/              |          |
|    ep_len_mean        | 3.28e+03 |
|    ep_rew_mean        | 1.34e+03 |
| time/                 |          |
|    fps                | 1092     |
|    iterations         | 1000     |
|    time_elapsed       | 117      |
|    total_timesteps    | 128000   |
| train/                |          |
|    entropy_loss       | -48.8    |
|    explained_variance | 0.961    |
|    learning_rate      | 0.001    |
|    n_updates          | 999      |
|    policy_loss        | -0.875   |
|    std                | 0.884    |
|    value_loss         | 0.00874  |
------------------------------------
Terminated
running build_ext
Using cuda device
Logging to assets/out/models/exp2/A2C_25
------------------------------------
| time/                 |          |
|    fps                | 1107     |
|    iterations         | 100      |
|    time_elapsed       | 11       |
|    total_timesteps    | 12800    |
| train/                |          |
|    entropy_loss       | -49.8    |
|    explained_variance | 0.233    |
|    learning_rate      | 0.001    |
|    n_updates          | 99       |
|    policy_loss        | 0.192    |
|    std                | 0.957    |
|    value_loss         | 0.00982  |
------------------------------------
------------------------------------
| time/                 |          |
|    fps                | 1104     |
|    iterations         | 200      |
|    time_elapsed       | 23       |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -49.5    |
|    explained_variance | 0.166    |
|    learning_rate      | 0.001    |
|    n_updates          | 199      |
|    policy_loss        | -0.0701  |
|    std                | 0.938    |
|    value_loss         | 0.0211   |
------------------------------------
------------------------------------
| reward                | 0.491    |
| reward_ang_vel        | 0.0341   |
| reward_contact        | 0.0978   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0346   |
| reward_orientation    | 0.0341   |
| reward_position       | 0.0749   |
| reward_velocity       | 0.0853   |
| rollout/              |          |
|    ep_len_mean        | 2.27e+03 |
|    ep_rew_mean        | 1.03e+03 |
| time/                 |          |
|    fps                | 1095     |
|    iterations         | 300      |
|    time_elapsed       | 35       |
|    total_timesteps    | 38400    |
| train/                |          |
|    entropy_loss       | -49.4    |
|    explained_variance | 0.797    |
|    learning_rate      | 0.001    |
|    n_updates          | 299      |
|    policy_loss        | -0.413   |
|    std                | 0.926    |
|    value_loss         | 0.0599   |
------------------------------------
------------------------------------
| reward                | 0.515    |
| reward_ang_vel        | 0.0479   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0362   |
| reward_orientation    | 0.0479   |
| reward_position       | 0.0737   |
| reward_velocity       | 0.0895   |
| rollout/              |          |
|    ep_len_mean        | 2.38e+03 |
|    ep_rew_mean        | 1.08e+03 |
| time/                 |          |
|    fps                | 1092     |
|    iterations         | 400      |
|    time_elapsed       | 46       |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -49.2    |
|    explained_variance | 0.713    |
|    learning_rate      | 0.001    |
|    n_updates          | 399      |
|    policy_loss        | 0.13     |
|    std                | 0.916    |
|    value_loss         | 0.0305   |
------------------------------------
------------------------------------
| reward                | 0.518    |
| reward_ang_vel        | 0.0543   |
| reward_contact        | 0.0982   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0373   |
| reward_orientation    | 0.0543   |
| reward_position       | 0.0722   |
| reward_velocity       | 0.0906   |
| rollout/              |          |
|    ep_len_mean        | 2.72e+03 |
|    ep_rew_mean        | 1.25e+03 |
| time/                 |          |
|    fps                | 1089     |
|    iterations         | 500      |
|    time_elapsed       | 58       |
|    total_timesteps    | 64000    |
| train/                |          |
|    entropy_loss       | -49.1    |
|    explained_variance | 0.58     |
|    learning_rate      | 0.001    |
|    n_updates          | 499      |
|    policy_loss        | 0.13     |
|    std                | 0.905    |
|    value_loss         | 0.0534   |
------------------------------------
------------------------------------
| reward                | 0.53     |
| reward_ang_vel        | 0.0688   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.107    |
| reward_energy         | 0.0352   |
| reward_orientation    | 0.0688   |
| reward_position       | 0.074    |
| reward_velocity       | 0.0942   |
| rollout/              |          |
|    ep_len_mean        | 3.14e+03 |
|    ep_rew_mean        | 1.49e+03 |
| time/                 |          |
|    fps                | 1086     |
|    iterations         | 600      |
|    time_elapsed       | 70       |
|    total_timesteps    | 76800    |
| train/                |          |
|    entropy_loss       | -49      |
|    explained_variance | 0.206    |
|    learning_rate      | 0.001    |
|    n_updates          | 599      |
|    policy_loss        | -0.196   |
|    std                | 0.897    |
|    value_loss         | 0.0188   |
------------------------------------
------------------------------------
| reward                | 0.533    |
| reward_ang_vel        | 0.0724   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.105    |
| reward_energy         | 0.0372   |
| reward_orientation    | 0.0724   |
| reward_position       | 0.0742   |
| reward_velocity       | 0.0947   |
| rollout/              |          |
|    ep_len_mean        | 3.12e+03 |
|    ep_rew_mean        | 1.5e+03  |
| time/                 |          |
|    fps                | 1085     |
|    iterations         | 700      |
|    time_elapsed       | 82       |
|    total_timesteps    | 89600    |
| train/                |          |
|    entropy_loss       | -48.8    |
|    explained_variance | 0.0419   |
|    learning_rate      | 0.001    |
|    n_updates          | 699      |
|    policy_loss        | -0.64    |
|    std                | 0.881    |
|    value_loss         | 0.0211   |
------------------------------------
Num timesteps: 96000
Best mean reward: -inf - Last mean reward per episode: 1588.39
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 0.538    |
| reward_ang_vel        | 0.0738   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.106    |
| reward_energy         | 0.0394   |
| reward_orientation    | 0.0738   |
| reward_position       | 0.0742   |
| reward_velocity       | 0.0951   |
| rollout/              |          |
|    ep_len_mean        | 3.32e+03 |
|    ep_rew_mean        | 1.63e+03 |
| time/                 |          |
|    fps                | 1083     |
|    iterations         | 800      |
|    time_elapsed       | 94       |
|    total_timesteps    | 102400   |
| train/                |          |
|    entropy_loss       | -48.6    |
|    explained_variance | 0.0268   |
|    learning_rate      | 0.001    |
|    n_updates          | 799      |
|    policy_loss        | -0.878   |
|    std                | 0.868    |
|    value_loss         | 0.00427  |
------------------------------------
------------------------------------
| reward                | 0.56     |
| reward_ang_vel        | 0.0798   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0487   |
| reward_orientation    | 0.0798   |
| reward_position       | 0.0772   |
| reward_velocity       | 0.0961   |
| rollout/              |          |
|    ep_len_mean        | 3.15e+03 |
|    ep_rew_mean        | 1.6e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 900      |
|    time_elapsed       | 106      |
|    total_timesteps    | 115200   |
| train/                |          |
|    entropy_loss       | -48.4    |
|    explained_variance | 0.916    |
|    learning_rate      | 0.001    |
|    n_updates          | 899      |
|    policy_loss        | -0.359   |
|    std                | 0.857    |
|    value_loss         | 0.00556  |
------------------------------------
------------------------------------
| reward                | 0.565    |
| reward_ang_vel        | 0.0819   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0519   |
| reward_orientation    | 0.0819   |
| reward_position       | 0.076    |
| reward_velocity       | 0.0965   |
| rollout/              |          |
|    ep_len_mean        | 3.47e+03 |
|    ep_rew_mean        | 1.78e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 1000     |
|    time_elapsed       | 118      |
|    total_timesteps    | 128000   |
| train/                |          |
|    entropy_loss       | -48.3    |
|    explained_variance | 0.984    |
|    learning_rate      | 0.001    |
|    n_updates          | 999      |
|    policy_loss        | 0.257    |
|    std                | 0.85     |
|    value_loss         | 0.00318  |
------------------------------------
------------------------------------
| reward                | 0.571    |
| reward_ang_vel        | 0.0841   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0554   |
| reward_orientation    | 0.0841   |
| reward_position       | 0.0761   |
| reward_velocity       | 0.0967   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 1.77e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 1100     |
|    time_elapsed       | 130      |
|    total_timesteps    | 140800   |
| train/                |          |
|    entropy_loss       | -48.3    |
|    explained_variance | 0.521    |
|    learning_rate      | 0.001    |
|    n_updates          | 1099     |
|    policy_loss        | -0.277   |
|    std                | 0.846    |
|    value_loss         | 0.0125   |
------------------------------------
------------------------------------
| reward                | 0.571    |
| reward_ang_vel        | 0.0834   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0561   |
| reward_orientation    | 0.0834   |
| reward_position       | 0.0757   |
| reward_velocity       | 0.0966   |
| rollout/              |          |
|    ep_len_mean        | 3.31e+03 |
|    ep_rew_mean        | 1.75e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 1200     |
|    time_elapsed       | 141      |
|    total_timesteps    | 153600   |
| train/                |          |
|    entropy_loss       | -48.2    |
|    explained_variance | 0.522    |
|    learning_rate      | 0.001    |
|    n_updates          | 1199     |
|    policy_loss        | -0.692   |
|    std                | 0.84     |
|    value_loss         | 0.00857  |
------------------------------------
------------------------------------
| reward                | 0.572    |
| reward_ang_vel        | 0.085    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0581   |
| reward_orientation    | 0.085    |
| reward_position       | 0.0749   |
| reward_velocity       | 0.0965   |
| rollout/              |          |
|    ep_len_mean        | 3.23e+03 |
|    ep_rew_mean        | 1.73e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 1300     |
|    time_elapsed       | 153      |
|    total_timesteps    | 166400   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | -0.941   |
|    learning_rate      | 0.001    |
|    n_updates          | 1299     |
|    policy_loss        | -0.219   |
|    std                | 0.835    |
|    value_loss         | 0.0237   |
------------------------------------
------------------------------------
| reward                | 0.576    |
| reward_ang_vel        | 0.0862   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0599   |
| reward_orientation    | 0.0862   |
| reward_position       | 0.0756   |
| reward_velocity       | 0.0967   |
| rollout/              |          |
|    ep_len_mean        | 3.18e+03 |
|    ep_rew_mean        | 1.71e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 1400     |
|    time_elapsed       | 165      |
|    total_timesteps    | 179200   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | -0.241   |
|    learning_rate      | 0.001    |
|    n_updates          | 1399     |
|    policy_loss        | 0.347    |
|    std                | 0.835    |
|    value_loss         | 0.00322  |
------------------------------------
Num timesteps: 192000
Best mean reward: 1588.39 - Last mean reward per episode: 1801.81
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 0.582    |
| reward_ang_vel        | 0.0872   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0617   |
| reward_orientation    | 0.0872   |
| reward_position       | 0.0769   |
| reward_velocity       | 0.097    |
| rollout/              |          |
|    ep_len_mean        | 3.29e+03 |
|    ep_rew_mean        | 1.8e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 1500     |
|    time_elapsed       | 177      |
|    total_timesteps    | 192000   |
| train/                |          |
|    entropy_loss       | -48.1    |
|    explained_variance | 0.206    |
|    learning_rate      | 0.001    |
|    n_updates          | 1499     |
|    policy_loss        | -0.125   |
|    std                | 0.832    |
|    value_loss         | 0.0037   |
------------------------------------
------------------------------------
| reward                | 0.585    |
| reward_ang_vel        | 0.088    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0629   |
| reward_orientation    | 0.088    |
| reward_position       | 0.078    |
| reward_velocity       | 0.0972   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 1.87e+03 |
| time/                 |          |
|    fps                | 1080     |
|    iterations         | 1600     |
|    time_elapsed       | 189      |
|    total_timesteps    | 204800   |
| train/                |          |
|    entropy_loss       | -48      |
|    explained_variance | 0.127    |
|    learning_rate      | 0.001    |
|    n_updates          | 1599     |
|    policy_loss        | -0.0649  |
|    std                | 0.829    |
|    value_loss         | 0.00513  |
------------------------------------
------------------------------------
| reward                | 0.585    |
| reward_ang_vel        | 0.0884   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0634   |
| reward_orientation    | 0.0884   |
| reward_position       | 0.0777   |
| reward_velocity       | 0.0972   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 1.94e+03 |
| time/                 |          |
|    fps                | 1080     |
|    iterations         | 1700     |
|    time_elapsed       | 201      |
|    total_timesteps    | 217600   |
| train/                |          |
|    entropy_loss       | -47.9    |
|    explained_variance | 0.734    |
|    learning_rate      | 0.001    |
|    n_updates          | 1699     |
|    policy_loss        | 0.396    |
|    std                | 0.823    |
|    value_loss         | 0.00648  |
------------------------------------
------------------------------------
| reward                | 0.586    |
| reward_ang_vel        | 0.0888   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0638   |
| reward_orientation    | 0.0888   |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0973   |
| rollout/              |          |
|    ep_len_mean        | 3.46e+03 |
|    ep_rew_mean        | 1.93e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 1800     |
|    time_elapsed       | 213      |
|    total_timesteps    | 230400   |
| train/                |          |
|    entropy_loss       | -47.9    |
|    explained_variance | 0.00404  |
|    learning_rate      | 0.001    |
|    n_updates          | 1799     |
|    policy_loss        | -0.419   |
|    std                | 0.821    |
|    value_loss         | 105      |
------------------------------------
------------------------------------
| reward                | 0.587    |
| reward_ang_vel        | 0.0895   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0644   |
| reward_orientation    | 0.0895   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.0974   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 1.91e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 1900     |
|    time_elapsed       | 224      |
|    total_timesteps    | 243200   |
| train/                |          |
|    entropy_loss       | -47.9    |
|    explained_variance | 0.0475   |
|    learning_rate      | 0.001    |
|    n_updates          | 1899     |
|    policy_loss        | -1.12    |
|    std                | 0.818    |
|    value_loss         | 0.0128   |
------------------------------------
------------------------------------
| reward                | 0.589    |
| reward_ang_vel        | 0.09     |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0651   |
| reward_orientation    | 0.09     |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 1.93e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 2000     |
|    time_elapsed       | 236      |
|    total_timesteps    | 256000   |
| train/                |          |
|    entropy_loss       | -47.9    |
|    explained_variance | 0.194    |
|    learning_rate      | 0.001    |
|    n_updates          | 1999     |
|    policy_loss        | 0.099    |
|    std                | 0.819    |
|    value_loss         | 0.0129   |
------------------------------------
------------------------------------
| reward                | 0.589    |
| reward_ang_vel        | 0.0902   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0655   |
| reward_orientation    | 0.0902   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0974   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 1.94e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 2100     |
|    time_elapsed       | 248      |
|    total_timesteps    | 268800   |
| train/                |          |
|    entropy_loss       | -47.8    |
|    explained_variance | 0.163    |
|    learning_rate      | 0.001    |
|    n_updates          | 2099     |
|    policy_loss        | -0.507   |
|    std                | 0.814    |
|    value_loss         | 0.038    |
------------------------------------
------------------------------------
| reward                | 0.59     |
| reward_ang_vel        | 0.0908   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0663   |
| reward_orientation    | 0.0908   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 1.94e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 2200     |
|    time_elapsed       | 260      |
|    total_timesteps    | 281600   |
| train/                |          |
|    entropy_loss       | -47.8    |
|    explained_variance | 0.815    |
|    learning_rate      | 0.001    |
|    n_updates          | 2199     |
|    policy_loss        | 0.675    |
|    std                | 0.814    |
|    value_loss         | 0.0127   |
------------------------------------
Num timesteps: 288000
Best mean reward: 1801.81 - Last mean reward per episode: 1992.24
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 0.591    |
| reward_ang_vel        | 0.0912   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0671   |
| reward_orientation    | 0.0912   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.0974   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 1.97e+03 |
| time/                 |          |
|    fps                | 1080     |
|    iterations         | 2300     |
|    time_elapsed       | 272      |
|    total_timesteps    | 294400   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.981    |
|    learning_rate      | 0.001    |
|    n_updates          | 2299     |
|    policy_loss        | -0.276   |
|    std                | 0.808    |
|    value_loss         | 0.00304  |
------------------------------------
------------------------------------
| reward                | 0.591    |
| reward_ang_vel        | 0.0914   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0673   |
| reward_orientation    | 0.0914   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 1.98e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 2400     |
|    time_elapsed       | 284      |
|    total_timesteps    | 307200   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.808    |
|    learning_rate      | 0.001    |
|    n_updates          | 2399     |
|    policy_loss        | -0.225   |
|    std                | 0.804    |
|    value_loss         | 0.0105   |
------------------------------------
------------------------------------
| reward                | 0.591    |
| reward_ang_vel        | 0.0915   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0674   |
| reward_orientation    | 0.0915   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 1.98e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 2500     |
|    time_elapsed       | 295      |
|    total_timesteps    | 320000   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.987    |
|    learning_rate      | 0.001    |
|    n_updates          | 2499     |
|    policy_loss        | -0.295   |
|    std                | 0.805    |
|    value_loss         | 0.0102   |
------------------------------------
------------------------------------
| reward                | 0.592    |
| reward_ang_vel        | 0.0917   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0681   |
| reward_orientation    | 0.0917   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.39e+03 |
|    ep_rew_mean        | 1.95e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 2600     |
|    time_elapsed       | 307      |
|    total_timesteps    | 332800   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 2599     |
|    policy_loss        | -0.59    |
|    std                | 0.806    |
|    value_loss         | 0.00723  |
------------------------------------
------------------------------------
| reward                | 0.592    |
| reward_ang_vel        | 0.0919   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0683   |
| reward_orientation    | 0.0919   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 1.96e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 2700     |
|    time_elapsed       | 319      |
|    total_timesteps    | 345600   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.889    |
|    learning_rate      | 0.001    |
|    n_updates          | 2699     |
|    policy_loss        | 0.118    |
|    std                | 0.804    |
|    value_loss         | 0.00765  |
------------------------------------
------------------------------------
| reward                | 0.593    |
| reward_ang_vel        | 0.092    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0685   |
| reward_orientation    | 0.092    |
| reward_position       | 0.0798   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 1.96e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 2800     |
|    time_elapsed       | 331      |
|    total_timesteps    | 358400   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.57     |
|    learning_rate      | 0.001    |
|    n_updates          | 2799     |
|    policy_loss        | -0.205   |
|    std                | 0.799    |
|    value_loss         | 0.00485  |
------------------------------------
------------------------------------
| reward                | 0.593    |
| reward_ang_vel        | 0.0922   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0689   |
| reward_orientation    | 0.0922   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 1.99e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 2900     |
|    time_elapsed       | 343      |
|    total_timesteps    | 371200   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.812    |
|    learning_rate      | 0.001    |
|    n_updates          | 2899     |
|    policy_loss        | -1.12    |
|    std                | 0.799    |
|    value_loss         | 0.00852  |
------------------------------------
Num timesteps: 384000
Best mean reward: 1992.24 - Last mean reward per episode: 1992.35
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 0.594    |
| reward_ang_vel        | 0.0921   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0691   |
| reward_orientation    | 0.0921   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 1.99e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 3000     |
|    time_elapsed       | 354      |
|    total_timesteps    | 384000   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.92     |
|    learning_rate      | 0.001    |
|    n_updates          | 2999     |
|    policy_loss        | 0.204    |
|    std                | 0.799    |
|    value_loss         | 0.0031   |
------------------------------------
------------------------------------
| reward                | 0.596    |
| reward_ang_vel        | 0.093    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0697   |
| reward_orientation    | 0.093    |
| reward_position       | 0.0798   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.51e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 3100     |
|    time_elapsed       | 366      |
|    total_timesteps    | 396800   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.659    |
|    learning_rate      | 0.001    |
|    n_updates          | 3099     |
|    policy_loss        | 0.249    |
|    std                | 0.803    |
|    value_loss         | 0.00786  |
------------------------------------
------------------------------------
| reward                | 0.597    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0704   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0799   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.52e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 3200     |
|    time_elapsed       | 378      |
|    total_timesteps    | 409600   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.0684   |
|    learning_rate      | 0.001    |
|    n_updates          | 3199     |
|    policy_loss        | -0.173   |
|    std                | 0.804    |
|    value_loss         | 0.0122   |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0955   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0715   |
| reward_orientation    | 0.0955   |
| reward_position       | 0.0801   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.54e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 3300     |
|    time_elapsed       | 390      |
|    total_timesteps    | 422400   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.714    |
|    learning_rate      | 0.001    |
|    n_updates          | 3299     |
|    policy_loss        | -0.322   |
|    std                | 0.801    |
|    value_loss         | 0.0194   |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0964   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0745   |
| reward_orientation    | 0.0964   |
| reward_position       | 0.081    |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 3400     |
|    time_elapsed       | 402      |
|    total_timesteps    | 435200   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 3399     |
|    policy_loss        | 0.272    |
|    std                | 0.799    |
|    value_loss         | 0.00739  |
------------------------------------
------------------------------------
| reward                | 0.607    |
| reward_ang_vel        | 0.0964   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0756   |
| reward_orientation    | 0.0964   |
| reward_position       | 0.0808   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 3500     |
|    time_elapsed       | 414      |
|    total_timesteps    | 448000   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.92     |
|    learning_rate      | 0.001    |
|    n_updates          | 3499     |
|    policy_loss        | -0.0093  |
|    std                | 0.799    |
|    value_loss         | 0.00883  |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0964   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0756   |
| reward_orientation    | 0.0964   |
| reward_position       | 0.0811   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.7e+03  |
|    ep_rew_mean        | 2.23e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 3600     |
|    time_elapsed       | 426      |
|    total_timesteps    | 460800   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | -0.0135  |
|    learning_rate      | 0.001    |
|    n_updates          | 3599     |
|    policy_loss        | 0.506    |
|    std                | 0.799    |
|    value_loss         | 97.7     |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.096    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0757   |
| reward_orientation    | 0.096    |
| reward_position       | 0.0805   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.71e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 3700     |
|    time_elapsed       | 437      |
|    total_timesteps    | 473600   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 3699     |
|    policy_loss        | 0.458    |
|    std                | 0.798    |
|    value_loss         | 0.00389  |
------------------------------------
Num timesteps: 480000
Best mean reward: 1992.35 - Last mean reward per episode: 2188.17
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0962   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0762   |
| reward_orientation    | 0.0962   |
| reward_position       | 0.0811   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 3800     |
|    time_elapsed       | 449      |
|    total_timesteps    | 486400   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 3799     |
|    policy_loss        | 0.264    |
|    std                | 0.795    |
|    value_loss         | 0.0094   |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0962   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0962   |
| reward_position       | 0.0816   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 3900     |
|    time_elapsed       | 461      |
|    total_timesteps    | 499200   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 3899     |
|    policy_loss        | 0.334    |
|    std                | 0.795    |
|    value_loss         | 0.00367  |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0962   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0962   |
| reward_position       | 0.0808   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 4000     |
|    time_elapsed       | 473      |
|    total_timesteps    | 512000   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 3999     |
|    policy_loss        | -0.122   |
|    std                | 0.794    |
|    value_loss         | 0.00575  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0957   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0957   |
| reward_position       | 0.0807   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 4100     |
|    time_elapsed       | 485      |
|    total_timesteps    | 524800   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.978    |
|    learning_rate      | 0.001    |
|    n_updates          | 4099     |
|    policy_loss        | 0.237    |
|    std                | 0.795    |
|    value_loss         | 0.0106   |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0805   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.16e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 4200     |
|    time_elapsed       | 496      |
|    total_timesteps    | 537600   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.991    |
|    learning_rate      | 0.001    |
|    n_updates          | 4199     |
|    policy_loss        | 0.35     |
|    std                | 0.795    |
|    value_loss         | 0.00465  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0806   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 4300     |
|    time_elapsed       | 508      |
|    total_timesteps    | 550400   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 4299     |
|    policy_loss        | -0.494   |
|    std                | 0.791    |
|    value_loss         | 0.00758  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0801   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 4400     |
|    time_elapsed       | 520      |
|    total_timesteps    | 563200   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 4399     |
|    policy_loss        | 0.143    |
|    std                | 0.792    |
|    value_loss         | 0.00541  |
------------------------------------
Num timesteps: 576000
Best mean reward: 2188.17 - Last mean reward per episode: 2141.55
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0955   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0955   |
| reward_position       | 0.0794   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.52e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 4500     |
|    time_elapsed       | 532      |
|    total_timesteps    | 576000   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.991    |
|    learning_rate      | 0.001    |
|    n_updates          | 4499     |
|    policy_loss        | -0.0124  |
|    std                | 0.792    |
|    value_loss         | 0.00523  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0797   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.52e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 4600     |
|    time_elapsed       | 544      |
|    total_timesteps    | 588800   |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 4599     |
|    policy_loss        | -0.302   |
|    std                | 0.789    |
|    value_loss         | 0.00437  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0798   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 4700     |
|    time_elapsed       | 555      |
|    total_timesteps    | 601600   |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | -0.012   |
|    learning_rate      | 0.001    |
|    n_updates          | 4699     |
|    policy_loss        | 0.33     |
|    std                | 0.788    |
|    value_loss         | 99.1     |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0799   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 4800     |
|    time_elapsed       | 567      |
|    total_timesteps    | 614400   |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.985    |
|    learning_rate      | 0.001    |
|    n_updates          | 4799     |
|    policy_loss        | 0.276    |
|    std                | 0.788    |
|    value_loss         | 0.00507  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0803   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.64e+03 |
|    ep_rew_mean        | 2.22e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 4900     |
|    time_elapsed       | 579      |
|    total_timesteps    | 627200   |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.99     |
|    learning_rate      | 0.001    |
|    n_updates          | 4899     |
|    policy_loss        | -0.0169  |
|    std                | 0.786    |
|    value_loss         | 0.0114   |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0803   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.69e+03 |
|    ep_rew_mean        | 2.25e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 5000     |
|    time_elapsed       | 591      |
|    total_timesteps    | 640000   |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.763    |
|    learning_rate      | 0.001    |
|    n_updates          | 4999     |
|    policy_loss        | -0.0562  |
|    std                | 0.782    |
|    value_loss         | 0.0023   |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0801   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.67e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 5100     |
|    time_elapsed       | 603      |
|    total_timesteps    | 652800   |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.984    |
|    learning_rate      | 0.001    |
|    n_updates          | 5099     |
|    policy_loss        | -0.0493  |
|    std                | 0.785    |
|    value_loss         | 0.0061   |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0803   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 5200     |
|    time_elapsed       | 615      |
|    total_timesteps    | 665600   |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.7      |
|    learning_rate      | 0.001    |
|    n_updates          | 5199     |
|    policy_loss        | 0.464    |
|    std                | 0.783    |
|    value_loss         | 0.00894  |
------------------------------------
Num timesteps: 672000
Best mean reward: 2188.17 - Last mean reward per episode: 2218.57
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0815   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.65e+03 |
|    ep_rew_mean        | 2.23e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 5300     |
|    time_elapsed       | 627      |
|    total_timesteps    | 678400   |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.984    |
|    learning_rate      | 0.001    |
|    n_updates          | 5299     |
|    policy_loss        | -0.584   |
|    std                | 0.787    |
|    value_loss         | 0.00978  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0812   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.76e+03 |
|    ep_rew_mean        | 2.3e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 5400     |
|    time_elapsed       | 638      |
|    total_timesteps    | 691200   |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.977    |
|    learning_rate      | 0.001    |
|    n_updates          | 5399     |
|    policy_loss        | -0.0686  |
|    std                | 0.79     |
|    value_loss         | 0.0171   |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0808   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.75e+03 |
|    ep_rew_mean        | 2.29e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 5500     |
|    time_elapsed       | 650      |
|    total_timesteps    | 704000   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.971    |
|    learning_rate      | 0.001    |
|    n_updates          | 5499     |
|    policy_loss        | 0.385    |
|    std                | 0.791    |
|    value_loss         | 0.00988  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0807   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.71e+03 |
|    ep_rew_mean        | 2.27e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 5600     |
|    time_elapsed       | 662      |
|    total_timesteps    | 716800   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.988    |
|    learning_rate      | 0.001    |
|    n_updates          | 5599     |
|    policy_loss        | -0.116   |
|    std                | 0.793    |
|    value_loss         | 0.0128   |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0811   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.73e+03 |
|    ep_rew_mean        | 2.28e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 5700     |
|    time_elapsed       | 674      |
|    total_timesteps    | 729600   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 5699     |
|    policy_loss        | 0.059    |
|    std                | 0.796    |
|    value_loss         | 0.00884  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0805   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.62e+03 |
|    ep_rew_mean        | 2.21e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 5800     |
|    time_elapsed       | 686      |
|    total_timesteps    | 742400   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | -0.0738  |
|    learning_rate      | 0.001    |
|    n_updates          | 5799     |
|    policy_loss        | 0.282    |
|    std                | 0.795    |
|    value_loss         | 0.0112   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0797   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.65e+03 |
|    ep_rew_mean        | 2.22e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 5900     |
|    time_elapsed       | 698      |
|    total_timesteps    | 755200   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.952    |
|    learning_rate      | 0.001    |
|    n_updates          | 5899     |
|    policy_loss        | -0.235   |
|    std                | 0.796    |
|    value_loss         | 0.0189   |
------------------------------------
Num timesteps: 768000
Best mean reward: 2218.57 - Last mean reward per episode: 2233.14
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0792   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.67e+03 |
|    ep_rew_mean        | 2.23e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6000     |
|    time_elapsed       | 709      |
|    total_timesteps    | 768000   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 5999     |
|    policy_loss        | -0.0354  |
|    std                | 0.8      |
|    value_loss         | 0.014    |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.079    |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6100     |
|    time_elapsed       | 721      |
|    total_timesteps    | 780800   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.979    |
|    learning_rate      | 0.001    |
|    n_updates          | 6099     |
|    policy_loss        | -0.715   |
|    std                | 0.799    |
|    value_loss         | 0.00492  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6200     |
|    time_elapsed       | 733      |
|    total_timesteps    | 793600   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 6199     |
|    policy_loss        | -0.976   |
|    std                | 0.795    |
|    value_loss         | 0.00811  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.39e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6300     |
|    time_elapsed       | 745      |
|    total_timesteps    | 806400   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | -0.00841 |
|    learning_rate      | 0.001    |
|    n_updates          | 6299     |
|    policy_loss        | 0.0345   |
|    std                | 0.797    |
|    value_loss         | 106      |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6400     |
|    time_elapsed       | 757      |
|    total_timesteps    | 819200   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 6399     |
|    policy_loss        | -0.509   |
|    std                | 0.796    |
|    value_loss         | 0.0164   |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0798   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6500     |
|    time_elapsed       | 768      |
|    total_timesteps    | 832000   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 6499     |
|    policy_loss        | -0.368   |
|    std                | 0.794    |
|    value_loss         | 0.0108   |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0794   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6600     |
|    time_elapsed       | 780      |
|    total_timesteps    | 844800   |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.987    |
|    learning_rate      | 0.001    |
|    n_updates          | 6599     |
|    policy_loss        | -0.0944  |
|    std                | 0.792    |
|    value_loss         | 0.0365   |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0794   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6700     |
|    time_elapsed       | 792      |
|    total_timesteps    | 857600   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.983    |
|    learning_rate      | 0.001    |
|    n_updates          | 6699     |
|    policy_loss        | -0.391   |
|    std                | 0.798    |
|    value_loss         | 0.00771  |
------------------------------------
Num timesteps: 864000
Best mean reward: 2233.14 - Last mean reward per episode: 2146.03
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.08     |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.53e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6800     |
|    time_elapsed       | 804      |
|    total_timesteps    | 870400   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 6799     |
|    policy_loss        | 0.412    |
|    std                | 0.799    |
|    value_loss         | 0.00384  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0801   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 6900     |
|    time_elapsed       | 816      |
|    total_timesteps    | 883200   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.986    |
|    learning_rate      | 0.001    |
|    n_updates          | 6899     |
|    policy_loss        | 0.316    |
|    std                | 0.801    |
|    value_loss         | 0.00498  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0805   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.65e+03 |
|    ep_rew_mean        | 2.22e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7000     |
|    time_elapsed       | 828      |
|    total_timesteps    | 896000   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 6999     |
|    policy_loss        | 0.123    |
|    std                | 0.803    |
|    value_loss         | 0.00251  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0951   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0951   |
| reward_position       | 0.0805   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.67e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7100     |
|    time_elapsed       | 839      |
|    total_timesteps    | 908800   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.99     |
|    learning_rate      | 0.001    |
|    n_updates          | 7099     |
|    policy_loss        | -0.126   |
|    std                | 0.804    |
|    value_loss         | 0.00324  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0806   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.66e+03 |
|    ep_rew_mean        | 2.23e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7200     |
|    time_elapsed       | 851      |
|    total_timesteps    | 921600   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 7199     |
|    policy_loss        | -0.928   |
|    std                | 0.802    |
|    value_loss         | 0.00136  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0806   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.6e+03  |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7300     |
|    time_elapsed       | 863      |
|    total_timesteps    | 934400   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.985    |
|    learning_rate      | 0.001    |
|    n_updates          | 7299     |
|    policy_loss        | -0.0843  |
|    std                | 0.802    |
|    value_loss         | 0.00784  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0804   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.6e+03  |
|    ep_rew_mean        | 2.19e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7400     |
|    time_elapsed       | 875      |
|    total_timesteps    | 947200   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 7399     |
|    policy_loss        | -0.741   |
|    std                | 0.803    |
|    value_loss         | 0.00397  |
------------------------------------
Num timesteps: 960000
Best mean reward: 2233.14 - Last mean reward per episode: 2091.54
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0799   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7500     |
|    time_elapsed       | 887      |
|    total_timesteps    | 960000   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 7499     |
|    policy_loss        | -0.0357  |
|    std                | 0.804    |
|    value_loss         | 0.00112  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.0798   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7600     |
|    time_elapsed       | 898      |
|    total_timesteps    | 972800   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 7599     |
|    policy_loss        | -0.21    |
|    std                | 0.801    |
|    value_loss         | 0.00368  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7700     |
|    time_elapsed       | 910      |
|    total_timesteps    | 985600   |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 7699     |
|    policy_loss        | -0.128   |
|    std                | 0.803    |
|    value_loss         | 0.00377  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7800     |
|    time_elapsed       | 922      |
|    total_timesteps    | 998400   |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 7799     |
|    policy_loss        | -0.355   |
|    std                | 0.806    |
|    value_loss         | 0.00582  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0952   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0952   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 7900     |
|    time_elapsed       | 934      |
|    total_timesteps    | 1011200  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 7899     |
|    policy_loss        | -0.0691  |
|    std                | 0.805    |
|    value_loss         | 0.00495  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.63e+03 |
|    ep_rew_mean        | 2.21e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8000     |
|    time_elapsed       | 946      |
|    total_timesteps    | 1024000  |
| train/                |          |
|    entropy_loss       | -47.7    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 7999     |
|    policy_loss        | -0.471   |
|    std                | 0.807    |
|    value_loss         | 0.00366  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.58e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8100     |
|    time_elapsed       | 958      |
|    total_timesteps    | 1036800  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 8099     |
|    policy_loss        | -0.291   |
|    std                | 0.803    |
|    value_loss         | 0.00339  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.078    |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.54e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8200     |
|    time_elapsed       | 969      |
|    total_timesteps    | 1049600  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.986    |
|    learning_rate      | 0.001    |
|    n_updates          | 8199     |
|    policy_loss        | 0.81     |
|    std                | 0.802    |
|    value_loss         | 0.00251  |
------------------------------------
Num timesteps: 1056000
Best mean reward: 2233.14 - Last mean reward per episode: 2141.77
------------------------------------
| reward                | 0.597    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0778   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.53e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8300     |
|    time_elapsed       | 981      |
|    total_timesteps    | 1062400  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 8299     |
|    policy_loss        | 0.167    |
|    std                | 0.799    |
|    value_loss         | 0.00602  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.53e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8400     |
|    time_elapsed       | 993      |
|    total_timesteps    | 1075200  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 8399     |
|    policy_loss        | -0.56    |
|    std                | 0.798    |
|    value_loss         | 0.0147   |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.65e+03 |
|    ep_rew_mean        | 2.22e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8500     |
|    time_elapsed       | 1005     |
|    total_timesteps    | 1088000  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 8499     |
|    policy_loss        | 0.0939   |
|    std                | 0.799    |
|    value_loss         | 0.00972  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0958   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0958   |
| reward_position       | 0.0778   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.69e+03 |
|    ep_rew_mean        | 2.25e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8600     |
|    time_elapsed       | 1017     |
|    total_timesteps    | 1100800  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 8599     |
|    policy_loss        | 0.0487   |
|    std                | 0.801    |
|    value_loss         | 0.00583  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0958   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0958   |
| reward_position       | 0.0781   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.7e+03  |
|    ep_rew_mean        | 2.25e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8700     |
|    time_elapsed       | 1029     |
|    total_timesteps    | 1113600  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 8699     |
|    policy_loss        | -0.101   |
|    std                | 0.8      |
|    value_loss         | 0.00285  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0958   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0958   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.6e+03  |
|    ep_rew_mean        | 2.19e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8800     |
|    time_elapsed       | 1040     |
|    total_timesteps    | 1126400  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 8799     |
|    policy_loss        | 0.494    |
|    std                | 0.797    |
|    value_loss         | 0.00402  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0794   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 8900     |
|    time_elapsed       | 1052     |
|    total_timesteps    | 1139200  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.959    |
|    learning_rate      | 0.001    |
|    n_updates          | 8899     |
|    policy_loss        | -0.0138  |
|    std                | 0.794    |
|    value_loss         | 0.00597  |
------------------------------------
Num timesteps: 1152000
Best mean reward: 2233.14 - Last mean reward per episode: 2179.82
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.58e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 9000     |
|    time_elapsed       | 1064     |
|    total_timesteps    | 1152000  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.988    |
|    learning_rate      | 0.001    |
|    n_updates          | 8999     |
|    policy_loss        | 0.202    |
|    std                | 0.796    |
|    value_loss         | 0.00955  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0957   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0957   |
| reward_position       | 0.079    |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 9100     |
|    time_elapsed       | 1076     |
|    total_timesteps    | 1164800  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 9099     |
|    policy_loss        | -0.582   |
|    std                | 0.794    |
|    value_loss         | 0.00642  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0958   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0958   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.66e+03 |
|    ep_rew_mean        | 2.23e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 9200     |
|    time_elapsed       | 1088     |
|    total_timesteps    | 1177600  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 9199     |
|    policy_loss        | -0.147   |
|    std                | 0.792    |
|    value_loss         | 0.00424  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0955   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0955   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.69e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 9300     |
|    time_elapsed       | 1100     |
|    total_timesteps    | 1190400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.982    |
|    learning_rate      | 0.001    |
|    n_updates          | 9299     |
|    policy_loss        | -0.153   |
|    std                | 0.793    |
|    value_loss         | 0.00524  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0958   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0958   |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.73e+03 |
|    ep_rew_mean        | 2.27e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 9400     |
|    time_elapsed       | 1111     |
|    total_timesteps    | 1203200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.843    |
|    learning_rate      | 0.001    |
|    n_updates          | 9399     |
|    policy_loss        | -0.63    |
|    std                | 0.793    |
|    value_loss         | 0.00431  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.71e+03 |
|    ep_rew_mean        | 2.26e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 9500     |
|    time_elapsed       | 1123     |
|    total_timesteps    | 1216000  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 9499     |
|    policy_loss        | -0.00118 |
|    std                | 0.793    |
|    value_loss         | 0.00378  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.68e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 9600     |
|    time_elapsed       | 1135     |
|    total_timesteps    | 1228800  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 9599     |
|    policy_loss        | 0.144    |
|    std                | 0.795    |
|    value_loss         | 0.00626  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0952   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0952   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 2.19e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 9700     |
|    time_elapsed       | 1147     |
|    total_timesteps    | 1241600  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 9699     |
|    policy_loss        | -0.193   |
|    std                | 0.795    |
|    value_loss         | 0.00318  |
------------------------------------
Num timesteps: 1248000
Best mean reward: 2233.14 - Last mean reward per episode: 2181.42
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 9800     |
|    time_elapsed       | 1159     |
|    total_timesteps    | 1254400  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 9799     |
|    policy_loss        | -1.15    |
|    std                | 0.795    |
|    value_loss         | 0.00363  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 9900     |
|    time_elapsed       | 1171     |
|    total_timesteps    | 1267200  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.965    |
|    learning_rate      | 0.001    |
|    n_updates          | 9899     |
|    policy_loss        | 0.622    |
|    std                | 0.798    |
|    value_loss         | 0.00554  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.62e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 10000    |
|    time_elapsed       | 1183     |
|    total_timesteps    | 1280000  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 9999     |
|    policy_loss        | 0.0731   |
|    std                | 0.797    |
|    value_loss         | 0.00545  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.6e+03  |
|    ep_rew_mean        | 2.19e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 10100    |
|    time_elapsed       | 1194     |
|    total_timesteps    | 1292800  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 1        |
|    learning_rate      | 0.001    |
|    n_updates          | 10099    |
|    policy_loss        | -0.304   |
|    std                | 0.798    |
|    value_loss         | 0.00236  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.7e+03  |
|    ep_rew_mean        | 2.25e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 10200    |
|    time_elapsed       | 1206     |
|    total_timesteps    | 1305600  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 10199    |
|    policy_loss        | 0.0403   |
|    std                | 0.796    |
|    value_loss         | 0.00469  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.079    |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.74e+03 |
|    ep_rew_mean        | 2.27e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 10300    |
|    time_elapsed       | 1218     |
|    total_timesteps    | 1318400  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 10299    |
|    policy_loss        | -1.08    |
|    std                | 0.792    |
|    value_loss         | 0.0076   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.66e+03 |
|    ep_rew_mean        | 2.23e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 10400    |
|    time_elapsed       | 1230     |
|    total_timesteps    | 1331200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 10399    |
|    policy_loss        | 0.0165   |
|    std                | 0.791    |
|    value_loss         | 0.0108   |
------------------------------------
Num timesteps: 1344000
Best mean reward: 2233.14 - Last mean reward per episode: 2190.78
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.19e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 10500    |
|    time_elapsed       | 1242     |
|    total_timesteps    | 1344000  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.927    |
|    learning_rate      | 0.001    |
|    n_updates          | 10499    |
|    policy_loss        | 0.258    |
|    std                | 0.793    |
|    value_loss         | 0.0112   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.54e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 10600    |
|    time_elapsed       | 1254     |
|    total_timesteps    | 1356800  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 10599    |
|    policy_loss        | -0.263   |
|    std                | 0.794    |
|    value_loss         | 0.0118   |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0776   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.54e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 10700    |
|    time_elapsed       | 1266     |
|    total_timesteps    | 1369600  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 10699    |
|    policy_loss        | 0.171    |
|    std                | 0.795    |
|    value_loss         | 0.00474  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 10800    |
|    time_elapsed       | 1277     |
|    total_timesteps    | 1382400  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 10799    |
|    policy_loss        | 0.415    |
|    std                | 0.798    |
|    value_loss         | 0.0056   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 10900    |
|    time_elapsed       | 1289     |
|    total_timesteps    | 1395200  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.773    |
|    learning_rate      | 0.001    |
|    n_updates          | 10899    |
|    policy_loss        | -0.661   |
|    std                | 0.799    |
|    value_loss         | 0.00638  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.51e+03 |
|    ep_rew_mean        | 2.13e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 11000    |
|    time_elapsed       | 1301     |
|    total_timesteps    | 1408000  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.975    |
|    learning_rate      | 0.001    |
|    n_updates          | 10999    |
|    policy_loss        | -0.453   |
|    std                | 0.801    |
|    value_loss         | 0.00649  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0929   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0929   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 11100    |
|    time_elapsed       | 1313     |
|    total_timesteps    | 1420800  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.976    |
|    learning_rate      | 0.001    |
|    n_updates          | 11099    |
|    policy_loss        | -0.462   |
|    std                | 0.805    |
|    value_loss         | 0.00553  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.093    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.093    |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 11200    |
|    time_elapsed       | 1325     |
|    total_timesteps    | 1433600  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.946    |
|    learning_rate      | 0.001    |
|    n_updates          | 11199    |
|    policy_loss        | 0.00814  |
|    std                | 0.803    |
|    value_loss         | 0.00655  |
------------------------------------
Num timesteps: 1440000
Best mean reward: 2233.14 - Last mean reward per episode: 2093.39
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0929   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0929   |
| reward_position       | 0.0792   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 11300    |
|    time_elapsed       | 1336     |
|    total_timesteps    | 1446400  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 11299    |
|    policy_loss        | 0.0329   |
|    std                | 0.806    |
|    value_loss         | 0.00362  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0926   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0926   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 11400    |
|    time_elapsed       | 1348     |
|    total_timesteps    | 1459200  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.985    |
|    learning_rate      | 0.001    |
|    n_updates          | 11399    |
|    policy_loss        | -0.0819  |
|    std                | 0.804    |
|    value_loss         | 0.016    |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.47e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 11500    |
|    time_elapsed       | 1360     |
|    total_timesteps    | 1472000  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 11499    |
|    policy_loss        | 0.816    |
|    std                | 0.806    |
|    value_loss         | 0.00634  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0929   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0929   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 11600    |
|    time_elapsed       | 1372     |
|    total_timesteps    | 1484800  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | -0.31    |
|    learning_rate      | 0.001    |
|    n_updates          | 11599    |
|    policy_loss        | -0.404   |
|    std                | 0.806    |
|    value_loss         | 0.00333  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0929   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0929   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 11700    |
|    time_elapsed       | 1384     |
|    total_timesteps    | 1497600  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.979    |
|    learning_rate      | 0.001    |
|    n_updates          | 11699    |
|    policy_loss        | -0.206   |
|    std                | 0.804    |
|    value_loss         | 0.00982  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 11800    |
|    time_elapsed       | 1395     |
|    total_timesteps    | 1510400  |
| train/                |          |
|    entropy_loss       | -47.6    |
|    explained_variance | 0.972    |
|    learning_rate      | 0.001    |
|    n_updates          | 11799    |
|    policy_loss        | 0.767    |
|    std                | 0.8      |
|    value_loss         | 0.00921  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.51e+03 |
|    ep_rew_mean        | 2.13e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 11900    |
|    time_elapsed       | 1407     |
|    total_timesteps    | 1523200  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.966    |
|    learning_rate      | 0.001    |
|    n_updates          | 11899    |
|    policy_loss        | 0.0778   |
|    std                | 0.8      |
|    value_loss         | 0.00918  |
------------------------------------
Num timesteps: 1536000
Best mean reward: 2233.14 - Last mean reward per episode: 2099.40
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.093    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.093    |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.46e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 12000    |
|    time_elapsed       | 1419     |
|    total_timesteps    | 1536000  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.977    |
|    learning_rate      | 0.001    |
|    n_updates          | 11999    |
|    policy_loss        | 0.781    |
|    std                | 0.799    |
|    value_loss         | 0.0134   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.52e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 12100    |
|    time_elapsed       | 1431     |
|    total_timesteps    | 1548800  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.987    |
|    learning_rate      | 0.001    |
|    n_updates          | 12099    |
|    policy_loss        | 0.3      |
|    std                | 0.795    |
|    value_loss         | 0.0071   |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.093    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.093    |
| reward_position       | 0.0786   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.54e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 12200    |
|    time_elapsed       | 1443     |
|    total_timesteps    | 1561600  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 12199    |
|    policy_loss        | 0.174    |
|    std                | 0.796    |
|    value_loss         | 0.00371  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0929   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0929   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 12300    |
|    time_elapsed       | 1455     |
|    total_timesteps    | 1574400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.594    |
|    learning_rate      | 0.001    |
|    n_updates          | 12299    |
|    policy_loss        | 0.1      |
|    std                | 0.793    |
|    value_loss         | 0.00878  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0934   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0934   |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.51e+03 |
|    ep_rew_mean        | 2.13e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 12400    |
|    time_elapsed       | 1466     |
|    total_timesteps    | 1587200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 12399    |
|    policy_loss        | -0.331   |
|    std                | 0.791    |
|    value_loss         | 0.00843  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 12500    |
|    time_elapsed       | 1478     |
|    total_timesteps    | 1600000  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | -0.00944 |
|    learning_rate      | 0.001    |
|    n_updates          | 12499    |
|    policy_loss        | -0.469   |
|    std                | 0.794    |
|    value_loss         | 113      |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 12600    |
|    time_elapsed       | 1490     |
|    total_timesteps    | 1612800  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 12599    |
|    policy_loss        | -0.00737 |
|    std                | 0.794    |
|    value_loss         | 0.00332  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 12700    |
|    time_elapsed       | 1502     |
|    total_timesteps    | 1625600  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 12699    |
|    policy_loss        | -0.361   |
|    std                | 0.794    |
|    value_loss         | 0.00303  |
------------------------------------
Num timesteps: 1632000
Best mean reward: 2233.14 - Last mean reward per episode: 2104.75
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.46e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 12800    |
|    time_elapsed       | 1514     |
|    total_timesteps    | 1638400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.986    |
|    learning_rate      | 0.001    |
|    n_updates          | 12799    |
|    policy_loss        | 0.261    |
|    std                | 0.794    |
|    value_loss         | 0.00295  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 12900    |
|    time_elapsed       | 1525     |
|    total_timesteps    | 1651200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.952    |
|    learning_rate      | 0.001    |
|    n_updates          | 12899    |
|    policy_loss        | 1.07     |
|    std                | 0.794    |
|    value_loss         | 0.0383   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.078    |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.35e+03 |
|    ep_rew_mean        | 2.03e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13000    |
|    time_elapsed       | 1537     |
|    total_timesteps    | 1664000  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 12999    |
|    policy_loss        | 0.425    |
|    std                | 0.795    |
|    value_loss         | 0.00401  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0794   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13100    |
|    time_elapsed       | 1549     |
|    total_timesteps    | 1676800  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.034    |
|    learning_rate      | 0.001    |
|    n_updates          | 13099    |
|    policy_loss        | 0.0314   |
|    std                | 0.792    |
|    value_loss         | 99.3     |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13200    |
|    time_elapsed       | 1561     |
|    total_timesteps    | 1689600  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 13199    |
|    policy_loss        | 0.642    |
|    std                | 0.794    |
|    value_loss         | 0.00259  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.31e+03 |
|    ep_rew_mean        | 2.02e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13300    |
|    time_elapsed       | 1573     |
|    total_timesteps    | 1702400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 13299    |
|    policy_loss        | 0.364    |
|    std                | 0.794    |
|    value_loss         | 0.0135   |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.0799   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.32e+03 |
|    ep_rew_mean        | 2.02e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13400    |
|    time_elapsed       | 1585     |
|    total_timesteps    | 1715200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 13399    |
|    policy_loss        | 0.491    |
|    std                | 0.793    |
|    value_loss         | 0.00128  |
------------------------------------
Num timesteps: 1728000
Best mean reward: 2233.14 - Last mean reward per episode: 2062.90
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.095    |
| reward_position       | 0.08     |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.39e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13500    |
|    time_elapsed       | 1596     |
|    total_timesteps    | 1728000  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 13499    |
|    policy_loss        | -0.371   |
|    std                | 0.795    |
|    value_loss         | 0.00351  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0797   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13600    |
|    time_elapsed       | 1608     |
|    total_timesteps    | 1740800  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 13599    |
|    policy_loss        | 0.33     |
|    std                | 0.796    |
|    value_loss         | 0.00481  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0961   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0961   |
| reward_position       | 0.0801   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.47e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13700    |
|    time_elapsed       | 1620     |
|    total_timesteps    | 1753600  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.984    |
|    learning_rate      | 0.001    |
|    n_updates          | 13699    |
|    policy_loss        | 0.0821   |
|    std                | 0.794    |
|    value_loss         | 0.0291   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0961   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0961   |
| reward_position       | 0.0792   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.46e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13800    |
|    time_elapsed       | 1632     |
|    total_timesteps    | 1766400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.958    |
|    learning_rate      | 0.001    |
|    n_updates          | 13799    |
|    policy_loss        | 0.076    |
|    std                | 0.793    |
|    value_loss         | 0.00803  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.096    |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.096    |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 13900    |
|    time_elapsed       | 1644     |
|    total_timesteps    | 1779200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.964    |
|    learning_rate      | 0.001    |
|    n_updates          | 13899    |
|    policy_loss        | -0.319   |
|    std                | 0.794    |
|    value_loss         | 0.006    |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0959   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0959   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14000    |
|    time_elapsed       | 1655     |
|    total_timesteps    | 1792000  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 13999    |
|    policy_loss        | 0.341    |
|    std                | 0.795    |
|    value_loss         | 0.011    |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0963   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0963   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.55e+03 |
|    ep_rew_mean        | 2.16e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14100    |
|    time_elapsed       | 1667     |
|    total_timesteps    | 1804800  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 14099    |
|    policy_loss        | 0.247    |
|    std                | 0.794    |
|    value_loss         | 0.00492  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0964   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0964   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14200    |
|    time_elapsed       | 1679     |
|    total_timesteps    | 1817600  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 14199    |
|    policy_loss        | -0.302   |
|    std                | 0.795    |
|    value_loss         | 0.0081   |
------------------------------------
Num timesteps: 1824000
Best mean reward: 2233.14 - Last mean reward per episode: 2158.69
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0963   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0963   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14300    |
|    time_elapsed       | 1691     |
|    total_timesteps    | 1830400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.99     |
|    learning_rate      | 0.001    |
|    n_updates          | 14299    |
|    policy_loss        | 0.144    |
|    std                | 0.794    |
|    value_loss         | 0.0116   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0962   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0962   |
| reward_position       | 0.079    |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.5e+03  |
|    ep_rew_mean        | 2.13e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14400    |
|    time_elapsed       | 1703     |
|    total_timesteps    | 1843200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 14399    |
|    policy_loss        | 0.515    |
|    std                | 0.791    |
|    value_loss         | 0.00697  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.096    |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.096    |
| reward_position       | 0.079    |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.52e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14500    |
|    time_elapsed       | 1715     |
|    total_timesteps    | 1856000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 14499    |
|    policy_loss        | 0.65     |
|    std                | 0.79     |
|    value_loss         | 0.00501  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0959   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0959   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.5e+03  |
|    ep_rew_mean        | 2.13e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14600    |
|    time_elapsed       | 1727     |
|    total_timesteps    | 1868800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 14599    |
|    policy_loss        | -0.443   |
|    std                | 0.79     |
|    value_loss         | 0.0064   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0958   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0958   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14700    |
|    time_elapsed       | 1738     |
|    total_timesteps    | 1881600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.985    |
|    learning_rate      | 0.001    |
|    n_updates          | 14699    |
|    policy_loss        | 0.073    |
|    std                | 0.789    |
|    value_loss         | 0.00533  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0958   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0958   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.16e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14800    |
|    time_elapsed       | 1750     |
|    total_timesteps    | 1894400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 14799    |
|    policy_loss        | -0.363   |
|    std                | 0.788    |
|    value_loss         | 0.00805  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0955   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0955   |
| reward_position       | 0.0779   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.5e+03  |
|    ep_rew_mean        | 2.13e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 14900    |
|    time_elapsed       | 1762     |
|    total_timesteps    | 1907200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 14899    |
|    policy_loss        | -0.162   |
|    std                | 0.791    |
|    value_loss         | 0.0222   |
------------------------------------
Num timesteps: 1920000
Best mean reward: 2233.14 - Last mean reward per episode: 2151.59
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0957   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0957   |
| reward_position       | 0.0777   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.54e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 15000    |
|    time_elapsed       | 1774     |
|    total_timesteps    | 1920000  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 14999    |
|    policy_loss        | -0.203   |
|    std                | 0.792    |
|    value_loss         | 0.00371  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0956   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0956   |
| reward_position       | 0.0776   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.52e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 15100    |
|    time_elapsed       | 1786     |
|    total_timesteps    | 1932800  |
| train/                |          |
|    entropy_loss       | -47.5    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 15099    |
|    policy_loss        | -0.451   |
|    std                | 0.797    |
|    value_loss         | 0.0031   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0956   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0956   |
| reward_position       | 0.0773   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.5e+03  |
|    ep_rew_mean        | 2.13e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 15200    |
|    time_elapsed       | 1798     |
|    total_timesteps    | 1945600  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 15199    |
|    policy_loss        | -0.528   |
|    std                | 0.792    |
|    value_loss         | 0.00294  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0955   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0955   |
| reward_position       | 0.0773   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.46e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 15300    |
|    time_elapsed       | 1809     |
|    total_timesteps    | 1958400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 15299    |
|    policy_loss        | -0.0808  |
|    std                | 0.793    |
|    value_loss         | 0.00495  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0956   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0763   |
| reward_orientation    | 0.0956   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.46e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 15400    |
|    time_elapsed       | 1821     |
|    total_timesteps    | 1971200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 15399    |
|    policy_loss        | 0.149    |
|    std                | 0.794    |
|    value_loss         | 0.00573  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0955   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0955   |
| reward_position       | 0.0777   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 15500    |
|    time_elapsed       | 1833     |
|    total_timesteps    | 1984000  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.905    |
|    learning_rate      | 0.001    |
|    n_updates          | 15499    |
|    policy_loss        | -0.823   |
|    std                | 0.796    |
|    value_loss         | 0.00696  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0955   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0955   |
| reward_position       | 0.078    |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.55e+03 |
|    ep_rew_mean        | 2.16e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 15600    |
|    time_elapsed       | 1845     |
|    total_timesteps    | 1996800  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.953    |
|    learning_rate      | 0.001    |
|    n_updates          | 15599    |
|    policy_loss        | -0.0139  |
|    std                | 0.796    |
|    value_loss         | 0.00992  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0955   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0955   |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.64e+03 |
|    ep_rew_mean        | 2.21e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 15700    |
|    time_elapsed       | 1857     |
|    total_timesteps    | 2009600  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.764    |
|    learning_rate      | 0.001    |
|    n_updates          | 15699    |
|    policy_loss        | -0.334   |
|    std                | 0.796    |
|    value_loss         | 0.00673  |
------------------------------------
Num timesteps: 2016000
Best mean reward: 2233.14 - Last mean reward per episode: 2223.92
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0958   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0958   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.67e+03 |
|    ep_rew_mean        | 2.23e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 15800    |
|    time_elapsed       | 1869     |
|    total_timesteps    | 2022400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.985    |
|    learning_rate      | 0.001    |
|    n_updates          | 15799    |
|    policy_loss        | 0.166    |
|    std                | 0.795    |
|    value_loss         | 0.00339  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.64e+03 |
|    ep_rew_mean        | 2.22e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 15900    |
|    time_elapsed       | 1880     |
|    total_timesteps    | 2035200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.991    |
|    learning_rate      | 0.001    |
|    n_updates          | 15899    |
|    policy_loss        | 0.164    |
|    std                | 0.796    |
|    value_loss         | 0.00776  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.65e+03 |
|    ep_rew_mean        | 2.22e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 16000    |
|    time_elapsed       | 1892     |
|    total_timesteps    | 2048000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.927    |
|    learning_rate      | 0.001    |
|    n_updates          | 15999    |
|    policy_loss        | -0.16    |
|    std                | 0.792    |
|    value_loss         | 0.00689  |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.73e+03 |
|    ep_rew_mean        | 2.27e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 16100    |
|    time_elapsed       | 1904     |
|    total_timesteps    | 2060800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 16099    |
|    policy_loss        | -0.146   |
|    std                | 0.791    |
|    value_loss         | 0.00394  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.79e+03 |
|    ep_rew_mean        | 2.31e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 16200    |
|    time_elapsed       | 1916     |
|    total_timesteps    | 2073600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 16199    |
|    policy_loss        | -0.239   |
|    std                | 0.792    |
|    value_loss         | 0.00746  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0952   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0952   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.78e+03 |
|    ep_rew_mean        | 2.3e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 16300    |
|    time_elapsed       | 1928     |
|    total_timesteps    | 2086400  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.928    |
|    learning_rate      | 0.001    |
|    n_updates          | 16299    |
|    policy_loss        | 0.0199   |
|    std                | 0.795    |
|    value_loss         | 0.00549  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.79e+03 |
|    ep_rew_mean        | 2.31e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 16400    |
|    time_elapsed       | 1940     |
|    total_timesteps    | 2099200  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.978    |
|    learning_rate      | 0.001    |
|    n_updates          | 16399    |
|    policy_loss        | -0.0414  |
|    std                | 0.797    |
|    value_loss         | 0.00525  |
------------------------------------
Num timesteps: 2112000
Best mean reward: 2233.14 - Last mean reward per episode: 2260.12
Saving new best model to assets/out/models/exp2/best_model.zip
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.71e+03 |
|    ep_rew_mean        | 2.26e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 16500    |
|    time_elapsed       | 1952     |
|    total_timesteps    | 2112000  |
| train/                |          |
|    entropy_loss       | -47.4    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 16499    |
|    policy_loss        | -0.12    |
|    std                | 0.796    |
|    value_loss         | 0.00667  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.7e+03  |
|    ep_rew_mean        | 2.25e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 16600    |
|    time_elapsed       | 1963     |
|    total_timesteps    | 2124800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 16599    |
|    policy_loss        | 0.078    |
|    std                | 0.793    |
|    value_loss         | 0.00797  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.69e+03 |
|    ep_rew_mean        | 2.25e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 16700    |
|    time_elapsed       | 1975     |
|    total_timesteps    | 2137600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 16699    |
|    policy_loss        | -0.29    |
|    std                | 0.79     |
|    value_loss         | 0.00714  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0792   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.68e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 16800    |
|    time_elapsed       | 1987     |
|    total_timesteps    | 2150400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 16799    |
|    policy_loss        | 0.197    |
|    std                | 0.788    |
|    value_loss         | 0.00466  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.7e+03  |
|    ep_rew_mean        | 2.25e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 16900    |
|    time_elapsed       | 1999     |
|    total_timesteps    | 2163200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 16899    |
|    policy_loss        | 0.527    |
|    std                | 0.79     |
|    value_loss         | 0.00526  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0792   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.62e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 17000    |
|    time_elapsed       | 2011     |
|    total_timesteps    | 2176000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 16999    |
|    policy_loss        | 0.372    |
|    std                | 0.79     |
|    value_loss         | 0.00666  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0799   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.67e+03 |
|    ep_rew_mean        | 2.23e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 17100    |
|    time_elapsed       | 2022     |
|    total_timesteps    | 2188800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 17099    |
|    policy_loss        | -0.783   |
|    std                | 0.787    |
|    value_loss         | 0.00519  |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0935   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0935   |
| reward_position       | 0.0811   |
| reward_velocity       | 0.0984   |
| rollout/              |          |
|    ep_len_mean        | 3.68e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 17200    |
|    time_elapsed       | 2034     |
|    total_timesteps    | 2201600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 17199    |
|    policy_loss        | 0.193    |
|    std                | 0.787    |
|    value_loss         | 0.00693  |
------------------------------------
Num timesteps: 2208000
Best mean reward: 2260.12 - Last mean reward per episode: 2233.65
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.093    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.093    |
| reward_position       | 0.0818   |
| reward_velocity       | 0.0984   |
| rollout/              |          |
|    ep_len_mean        | 3.79e+03 |
|    ep_rew_mean        | 2.31e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 17300    |
|    time_elapsed       | 2046     |
|    total_timesteps    | 2214400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 17299    |
|    policy_loss        | 0.25     |
|    std                | 0.79     |
|    value_loss         | 0.0106   |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0934   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0934   |
| reward_position       | 0.0816   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.76e+03 |
|    ep_rew_mean        | 2.29e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 17400    |
|    time_elapsed       | 2058     |
|    total_timesteps    | 2227200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 17399    |
|    policy_loss        | 0.324    |
|    std                | 0.791    |
|    value_loss         | 0.0107   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0934   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0934   |
| reward_position       | 0.081    |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.69e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 17500    |
|    time_elapsed       | 2070     |
|    total_timesteps    | 2240000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 17499    |
|    policy_loss        | -0.0755  |
|    std                | 0.787    |
|    value_loss         | 0.00311  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0934   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0934   |
| reward_position       | 0.081    |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.69e+03 |
|    ep_rew_mean        | 2.25e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 17600    |
|    time_elapsed       | 2082     |
|    total_timesteps    | 2252800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 17599    |
|    policy_loss        | 0.686    |
|    std                | 0.786    |
|    value_loss         | 0.011    |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0812   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.63e+03 |
|    ep_rew_mean        | 2.21e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 17700    |
|    time_elapsed       | 2094     |
|    total_timesteps    | 2265600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 17699    |
|    policy_loss        | -0.294   |
|    std                | 0.789    |
|    value_loss         | 0.008    |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0816   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.58e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 17800    |
|    time_elapsed       | 2105     |
|    total_timesteps    | 2278400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 17799    |
|    policy_loss        | -0.175   |
|    std                | 0.788    |
|    value_loss         | 0.00645  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0816   |
| reward_velocity       | 0.0984   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 17900    |
|    time_elapsed       | 2117     |
|    total_timesteps    | 2291200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 17899    |
|    policy_loss        | 0.535    |
|    std                | 0.79     |
|    value_loss         | 0.00115  |
------------------------------------
Num timesteps: 2304000
Best mean reward: 2260.12 - Last mean reward per episode: 2204.15
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0823   |
| reward_velocity       | 0.0984   |
| rollout/              |          |
|    ep_len_mean        | 3.62e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18000    |
|    time_elapsed       | 2129     |
|    total_timesteps    | 2304000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 17999    |
|    policy_loss        | -0.676   |
|    std                | 0.788    |
|    value_loss         | 0.00696  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.082    |
| reward_velocity       | 0.0984   |
| rollout/              |          |
|    ep_len_mean        | 3.6e+03  |
|    ep_rew_mean        | 2.19e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18100    |
|    time_elapsed       | 2141     |
|    total_timesteps    | 2316800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 18099    |
|    policy_loss        | -0.135   |
|    std                | 0.788    |
|    value_loss         | 0.00762  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0934   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0934   |
| reward_position       | 0.0824   |
| reward_velocity       | 0.0985   |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18200    |
|    time_elapsed       | 2153     |
|    total_timesteps    | 2329600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.973    |
|    learning_rate      | 0.001    |
|    n_updates          | 18199    |
|    policy_loss        | 0.262    |
|    std                | 0.79     |
|    value_loss         | 0.00427  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0931   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0931   |
| reward_position       | 0.0815   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18300    |
|    time_elapsed       | 2164     |
|    total_timesteps    | 2342400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.965    |
|    learning_rate      | 0.001    |
|    n_updates          | 18299    |
|    policy_loss        | 0.27     |
|    std                | 0.793    |
|    value_loss         | 0.00432  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0934   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0934   |
| reward_position       | 0.0808   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18400    |
|    time_elapsed       | 2176     |
|    total_timesteps    | 2355200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 18399    |
|    policy_loss        | -0.146   |
|    std                | 0.79     |
|    value_loss         | 0.0102   |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0936   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0936   |
| reward_position       | 0.08     |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18500    |
|    time_elapsed       | 2188     |
|    total_timesteps    | 2368000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.975    |
|    learning_rate      | 0.001    |
|    n_updates          | 18499    |
|    policy_loss        | 0.18     |
|    std                | 0.792    |
|    value_loss         | 0.00371  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0802   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.24e+03 |
|    ep_rew_mean        | 1.96e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18600    |
|    time_elapsed       | 2200     |
|    total_timesteps    | 2380800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.963    |
|    learning_rate      | 0.001    |
|    n_updates          | 18599    |
|    policy_loss        | 0.743    |
|    std                | 0.792    |
|    value_loss         | 0.00411  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0798   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.23e+03 |
|    ep_rew_mean        | 1.96e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18700    |
|    time_elapsed       | 2212     |
|    total_timesteps    | 2393600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.941    |
|    learning_rate      | 0.001    |
|    n_updates          | 18699    |
|    policy_loss        | -0.832   |
|    std                | 0.787    |
|    value_loss         | 0.00529  |
------------------------------------
Num timesteps: 2400000
Best mean reward: 2260.12 - Last mean reward per episode: 1957.69
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.08     |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.21e+03 |
|    ep_rew_mean        | 1.95e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18800    |
|    time_elapsed       | 2224     |
|    total_timesteps    | 2406400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.915    |
|    learning_rate      | 0.001    |
|    n_updates          | 18799    |
|    policy_loss        | -0.445   |
|    std                | 0.79     |
|    value_loss         | 0.00186  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0808   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.2e+03  |
|    ep_rew_mean        | 1.94e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 18900    |
|    time_elapsed       | 2235     |
|    total_timesteps    | 2419200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 18899    |
|    policy_loss        | -0.245   |
|    std                | 0.789    |
|    value_loss         | 0.00357  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0804   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.21e+03 |
|    ep_rew_mean        | 1.95e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 19000    |
|    time_elapsed       | 2247     |
|    total_timesteps    | 2432000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 18999    |
|    policy_loss        | 0.305    |
|    std                | 0.791    |
|    value_loss         | 0.00711  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0798   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.2e+03  |
|    ep_rew_mean        | 1.94e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 19100    |
|    time_elapsed       | 2259     |
|    total_timesteps    | 2444800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 19099    |
|    policy_loss        | -0.212   |
|    std                | 0.789    |
|    value_loss         | 0.00294  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.19e+03 |
|    ep_rew_mean        | 1.93e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 19200    |
|    time_elapsed       | 2271     |
|    total_timesteps    | 2457600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.119    |
|    learning_rate      | 0.001    |
|    n_updates          | 19199    |
|    policy_loss        | -0.179   |
|    std                | 0.79     |
|    value_loss         | 94.6     |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0946   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0946   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.14e+03 |
|    ep_rew_mean        | 1.9e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 19300    |
|    time_elapsed       | 2283     |
|    total_timesteps    | 2470400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 19299    |
|    policy_loss        | -0.0356  |
|    std                | 0.788    |
|    value_loss         | 0.00339  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.13e+03 |
|    ep_rew_mean        | 1.9e+03  |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 19400    |
|    time_elapsed       | 2294     |
|    total_timesteps    | 2483200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 19399    |
|    policy_loss        | -0.187   |
|    std                | 0.791    |
|    value_loss         | 0.00279  |
------------------------------------
Num timesteps: 2496000
Best mean reward: 2260.12 - Last mean reward per episode: 1880.84
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0946   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0946   |
| reward_position       | 0.079    |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.1e+03  |
|    ep_rew_mean        | 1.88e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 19500    |
|    time_elapsed       | 2306     |
|    total_timesteps    | 2496000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 19499    |
|    policy_loss        | -0.0459  |
|    std                | 0.792    |
|    value_loss         | 0.00302  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.095    |
| reward_position       | 0.079    |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.16e+03 |
|    ep_rew_mean        | 1.92e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 19600    |
|    time_elapsed       | 2318     |
|    total_timesteps    | 2508800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 19599    |
|    policy_loss        | 0.0152   |
|    std                | 0.792    |
|    value_loss         | 0.00965  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.095    |
| reward_position       | 0.079    |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.16e+03 |
|    ep_rew_mean        | 1.91e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 19700    |
|    time_elapsed       | 2330     |
|    total_timesteps    | 2521600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 19699    |
|    policy_loss        | 0.123    |
|    std                | 0.794    |
|    value_loss         | 0.00381  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0952   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0952   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.2e+03  |
|    ep_rew_mean        | 1.94e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 19800    |
|    time_elapsed       | 2342     |
|    total_timesteps    | 2534400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 19799    |
|    policy_loss        | -0.535   |
|    std                | 0.793    |
|    value_loss         | 0.0041   |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0778   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.04e+03 |
|    ep_rew_mean        | 1.84e+03 |
| time/                 |          |
|    fps                | 1082     |
|    iterations         | 19900    |
|    time_elapsed       | 2354     |
|    total_timesteps    | 2547200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 19899    |
|    policy_loss        | 0.107    |
|    std                | 0.794    |
|    value_loss         | 0.00571  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.16e+03 |
|    ep_rew_mean        | 1.92e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20000    |
|    time_elapsed       | 2366     |
|    total_timesteps    | 2560000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 19999    |
|    policy_loss        | 0.592    |
|    std                | 0.792    |
|    value_loss         | 0.00462  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0794   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.16e+03 |
|    ep_rew_mean        | 1.92e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20100    |
|    time_elapsed       | 2377     |
|    total_timesteps    | 2572800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 20099    |
|    policy_loss        | 0.556    |
|    std                | 0.793    |
|    value_loss         | 0.00488  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.2e+03  |
|    ep_rew_mean        | 1.94e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20200    |
|    time_elapsed       | 2389     |
|    total_timesteps    | 2585600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 20199    |
|    policy_loss        | 0.138    |
|    std                | 0.793    |
|    value_loss         | 0.0147   |
------------------------------------
Num timesteps: 2592000
Best mean reward: 2260.12 - Last mean reward per episode: 1945.60
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0804   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.19e+03 |
|    ep_rew_mean        | 1.94e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20300    |
|    time_elapsed       | 2401     |
|    total_timesteps    | 2598400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.953    |
|    learning_rate      | 0.001    |
|    n_updates          | 20299    |
|    policy_loss        | -0.683   |
|    std                | 0.793    |
|    value_loss         | 0.00761  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0803   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.26e+03 |
|    ep_rew_mean        | 1.98e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20400    |
|    time_elapsed       | 2413     |
|    total_timesteps    | 2611200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.966    |
|    learning_rate      | 0.001    |
|    n_updates          | 20399    |
|    policy_loss        | -0.894   |
|    std                | 0.791    |
|    value_loss         | 0.003    |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.081    |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.31e+03 |
|    ep_rew_mean        | 2.02e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20500    |
|    time_elapsed       | 2425     |
|    total_timesteps    | 2624000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.846    |
|    learning_rate      | 0.001    |
|    n_updates          | 20499    |
|    policy_loss        | 0.489    |
|    std                | 0.793    |
|    value_loss         | 0.0043   |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0766   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0808   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.33e+03 |
|    ep_rew_mean        | 2.03e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20600    |
|    time_elapsed       | 2437     |
|    total_timesteps    | 2636800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 20599    |
|    policy_loss        | -0.2     |
|    std                | 0.791    |
|    value_loss         | 0.00217  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0807   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.35e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20700    |
|    time_elapsed       | 2449     |
|    total_timesteps    | 2649600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.985    |
|    learning_rate      | 0.001    |
|    n_updates          | 20699    |
|    policy_loss        | -0.305   |
|    std                | 0.792    |
|    value_loss         | 0.00344  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.081    |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20800    |
|    time_elapsed       | 2461     |
|    total_timesteps    | 2662400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.99     |
|    learning_rate      | 0.001    |
|    n_updates          | 20799    |
|    policy_loss        | -0.125   |
|    std                | 0.792    |
|    value_loss         | 0.00669  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.081    |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 20900    |
|    time_elapsed       | 2472     |
|    total_timesteps    | 2675200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.99     |
|    learning_rate      | 0.001    |
|    n_updates          | 20899    |
|    policy_loss        | -0.128   |
|    std                | 0.795    |
|    value_loss         | 0.00593  |
------------------------------------
Num timesteps: 2688000
Best mean reward: 2260.12 - Last mean reward per episode: 2050.18
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0814   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21000    |
|    time_elapsed       | 2484     |
|    total_timesteps    | 2688000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.964    |
|    learning_rate      | 0.001    |
|    n_updates          | 20999    |
|    policy_loss        | -0.335   |
|    std                | 0.799    |
|    value_loss         | 0.0761   |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0818   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.33e+03 |
|    ep_rew_mean        | 2.03e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21100    |
|    time_elapsed       | 2496     |
|    total_timesteps    | 2700800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 21099    |
|    policy_loss        | 0.422    |
|    std                | 0.8      |
|    value_loss         | 0.00538  |
------------------------------------
------------------------------------
| reward                | 0.607    |
| reward_ang_vel        | 0.0936   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0764   |
| reward_orientation    | 0.0936   |
| reward_position       | 0.0824   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21200    |
|    time_elapsed       | 2508     |
|    total_timesteps    | 2713600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.983    |
|    learning_rate      | 0.001    |
|    n_updates          | 21199    |
|    policy_loss        | 0.239    |
|    std                | 0.802    |
|    value_loss         | 0.0435   |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0933   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0754   |
| reward_orientation    | 0.0933   |
| reward_position       | 0.0821   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21300    |
|    time_elapsed       | 2520     |
|    total_timesteps    | 2726400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 21299    |
|    policy_loss        | -0.127   |
|    std                | 0.8      |
|    value_loss         | 0.00145  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0932   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0754   |
| reward_orientation    | 0.0932   |
| reward_position       | 0.082    |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21400    |
|    time_elapsed       | 2531     |
|    total_timesteps    | 2739200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.982    |
|    learning_rate      | 0.001    |
|    n_updates          | 21399    |
|    policy_loss        | 0.0834   |
|    std                | 0.802    |
|    value_loss         | 0.0772   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0927   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0927   |
| reward_position       | 0.0814   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21500    |
|    time_elapsed       | 2543     |
|    total_timesteps    | 2752000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 21499    |
|    policy_loss        | -0.576   |
|    std                | 0.8      |
|    value_loss         | 0.0105   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0926   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0926   |
| reward_position       | 0.0812   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21600    |
|    time_elapsed       | 2555     |
|    total_timesteps    | 2764800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.99     |
|    learning_rate      | 0.001    |
|    n_updates          | 21599    |
|    policy_loss        | -0.0548  |
|    std                | 0.799    |
|    value_loss         | 0.0472   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0926   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0926   |
| reward_position       | 0.0805   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21700    |
|    time_elapsed       | 2567     |
|    total_timesteps    | 2777600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.959    |
|    learning_rate      | 0.001    |
|    n_updates          | 21699    |
|    policy_loss        | -0.041   |
|    std                | 0.795    |
|    value_loss         | 0.0667   |
------------------------------------
Num timesteps: 2784000
Best mean reward: 2260.12 - Last mean reward per episode: 2107.31
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0926   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0926   |
| reward_position       | 0.0797   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.46e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21800    |
|    time_elapsed       | 2579     |
|    total_timesteps    | 2790400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.978    |
|    learning_rate      | 0.001    |
|    n_updates          | 21799    |
|    policy_loss        | -0.188   |
|    std                | 0.796    |
|    value_loss         | 0.0104   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0927   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0927   |
| reward_position       | 0.0797   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 21900    |
|    time_elapsed       | 2591     |
|    total_timesteps    | 2803200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.977    |
|    learning_rate      | 0.001    |
|    n_updates          | 21899    |
|    policy_loss        | 0.166    |
|    std                | 0.795    |
|    value_loss         | 0.0215   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0925   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0925   |
| reward_position       | 0.0807   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.6e+03  |
|    ep_rew_mean        | 2.19e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22000    |
|    time_elapsed       | 2603     |
|    total_timesteps    | 2816000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.991    |
|    learning_rate      | 0.001    |
|    n_updates          | 21999    |
|    policy_loss        | 0.406    |
|    std                | 0.795    |
|    value_loss         | 0.0147   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0926   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0926   |
| reward_position       | 0.0802   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.61e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22100    |
|    time_elapsed       | 2614     |
|    total_timesteps    | 2828800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 22099    |
|    policy_loss        | 0.0971   |
|    std                | 0.797    |
|    value_loss         | 0.00577  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0935   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0935   |
| reward_position       | 0.0804   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.67e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22200    |
|    time_elapsed       | 2626     |
|    total_timesteps    | 2841600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 22199    |
|    policy_loss        | 0.207    |
|    std                | 0.796    |
|    value_loss         | 0.00982  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0798   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.59e+03 |
|    ep_rew_mean        | 2.19e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22300    |
|    time_elapsed       | 2638     |
|    total_timesteps    | 2854400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.983    |
|    learning_rate      | 0.001    |
|    n_updates          | 22299    |
|    policy_loss        | 0.254    |
|    std                | 0.792    |
|    value_loss         | 0.00968  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.54e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22400    |
|    time_elapsed       | 2650     |
|    total_timesteps    | 2867200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 22399    |
|    policy_loss        | -0.303   |
|    std                | 0.793    |
|    value_loss         | 0.00361  |
------------------------------------
Num timesteps: 2880000
Best mean reward: 2260.12 - Last mean reward per episode: 2157.95
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0929   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0929   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.54e+03 |
|    ep_rew_mean        | 2.16e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22500    |
|    time_elapsed       | 2662     |
|    total_timesteps    | 2880000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 22499    |
|    policy_loss        | 0.754    |
|    std                | 0.794    |
|    value_loss         | 0.0106   |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0931   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0931   |
| reward_position       | 0.079    |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.58e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22600    |
|    time_elapsed       | 2674     |
|    total_timesteps    | 2892800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.975    |
|    learning_rate      | 0.001    |
|    n_updates          | 22599    |
|    policy_loss        | -0.297   |
|    std                | 0.79     |
|    value_loss         | 0.00458  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0935   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0935   |
| reward_position       | 0.0792   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.65e+03 |
|    ep_rew_mean        | 2.22e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22700    |
|    time_elapsed       | 2685     |
|    total_timesteps    | 2905600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.587    |
|    learning_rate      | 0.001    |
|    n_updates          | 22699    |
|    policy_loss        | -0.12    |
|    std                | 0.791    |
|    value_loss         | 0.00586  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22800    |
|    time_elapsed       | 2697     |
|    total_timesteps    | 2918400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 22799    |
|    policy_loss        | -0.0328  |
|    std                | 0.793    |
|    value_loss         | 0.00353  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 22900    |
|    time_elapsed       | 2709     |
|    total_timesteps    | 2931200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 22899    |
|    policy_loss        | -0.349   |
|    std                | 0.793    |
|    value_loss         | 0.0114   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.51e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23000    |
|    time_elapsed       | 2721     |
|    total_timesteps    | 2944000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 22999    |
|    policy_loss        | -0.0269  |
|    std                | 0.796    |
|    value_loss         | 0.00331  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23100    |
|    time_elapsed       | 2733     |
|    total_timesteps    | 2956800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 23099    |
|    policy_loss        | 0.823    |
|    std                | 0.801    |
|    value_loss         | 0.0116   |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0781   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23200    |
|    time_elapsed       | 2745     |
|    total_timesteps    | 2969600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 23199    |
|    policy_loss        | -0.299   |
|    std                | 0.8      |
|    value_loss         | 0.00461  |
------------------------------------
Num timesteps: 2976000
Best mean reward: 2260.12 - Last mean reward per episode: 2050.59
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0981   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0752   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0769   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23300    |
|    time_elapsed       | 2756     |
|    total_timesteps    | 2982400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 23299    |
|    policy_loss        | -0.119   |
|    std                | 0.801    |
|    value_loss         | 0.018    |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0754   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0766   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23400    |
|    time_elapsed       | 2768     |
|    total_timesteps    | 2995200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 23399    |
|    policy_loss        | 0.48     |
|    std                | 0.799    |
|    value_loss         | 0.0144   |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0754   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0767   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23500    |
|    time_elapsed       | 2780     |
|    total_timesteps    | 3008000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 23499    |
|    policy_loss        | 0.328    |
|    std                | 0.798    |
|    value_loss         | 0.00992  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0754   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0765   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23600    |
|    time_elapsed       | 2792     |
|    total_timesteps    | 3020800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.916    |
|    learning_rate      | 0.001    |
|    n_updates          | 23599    |
|    policy_loss        | 0.408    |
|    std                | 0.799    |
|    value_loss         | 0.261    |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0754   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0764   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23700    |
|    time_elapsed       | 2804     |
|    total_timesteps    | 3033600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 23699    |
|    policy_loss        | -0.57    |
|    std                | 0.801    |
|    value_loss         | 0.0042   |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0758   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0759   |
| reward_velocity       | 0.0974   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23800    |
|    time_elapsed       | 2816     |
|    total_timesteps    | 3046400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.986    |
|    learning_rate      | 0.001    |
|    n_updates          | 23799    |
|    policy_loss        | -0.355   |
|    std                | 0.802    |
|    value_loss         | 0.00792  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.076    |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0764   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 23900    |
|    time_elapsed       | 2827     |
|    total_timesteps    | 3059200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 23899    |
|    policy_loss        | 0.44     |
|    std                | 0.804    |
|    value_loss         | 0.00588  |
------------------------------------
Num timesteps: 3072000
Best mean reward: 2260.12 - Last mean reward per episode: 2054.77
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0767   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24000    |
|    time_elapsed       | 2839     |
|    total_timesteps    | 3072000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 23999    |
|    policy_loss        | -0.105   |
|    std                | 0.801    |
|    value_loss         | 0.0064   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0983   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0766   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24100    |
|    time_elapsed       | 2851     |
|    total_timesteps    | 3084800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 24099    |
|    policy_loss        | 0.306    |
|    std                | 0.8      |
|    value_loss         | 0.00729  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0774   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24200    |
|    time_elapsed       | 2863     |
|    total_timesteps    | 3097600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 24199    |
|    policy_loss        | 0.51     |
|    std                | 0.801    |
|    value_loss         | 0.00379  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0952   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0952   |
| reward_position       | 0.0772   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24300    |
|    time_elapsed       | 2875     |
|    total_timesteps    | 3110400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.987    |
|    learning_rate      | 0.001    |
|    n_updates          | 24299    |
|    policy_loss        | -0.0596  |
|    std                | 0.8      |
|    value_loss         | 0.0071   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.078    |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24400    |
|    time_elapsed       | 2886     |
|    total_timesteps    | 3123200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 24399    |
|    policy_loss        | 0.875    |
|    std                | 0.801    |
|    value_loss         | 0.00756  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0774   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.34e+03 |
|    ep_rew_mean        | 2.03e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24500    |
|    time_elapsed       | 2898     |
|    total_timesteps    | 3136000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 24499    |
|    policy_loss        | 0.786    |
|    std                | 0.8      |
|    value_loss         | 0.0112   |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0777   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24600    |
|    time_elapsed       | 2910     |
|    total_timesteps    | 3148800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 24599    |
|    policy_loss        | -0.00264 |
|    std                | 0.798    |
|    value_loss         | 0.00484  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.33e+03 |
|    ep_rew_mean        | 2.02e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24700    |
|    time_elapsed       | 2922     |
|    total_timesteps    | 3161600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 24699    |
|    policy_loss        | 0.378    |
|    std                | 0.796    |
|    value_loss         | 0.00458  |
------------------------------------
Num timesteps: 3168000
Best mean reward: 2260.12 - Last mean reward per episode: 1990.84
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0775   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.29e+03 |
|    ep_rew_mean        | 1.99e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24800    |
|    time_elapsed       | 2934     |
|    total_timesteps    | 3174400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 24799    |
|    policy_loss        | 0.539    |
|    std                | 0.796    |
|    value_loss         | 0.00153  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0777   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.29e+03 |
|    ep_rew_mean        | 2e+03    |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 24900    |
|    time_elapsed       | 2946     |
|    total_timesteps    | 3187200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 24899    |
|    policy_loss        | -0.522   |
|    std                | 0.796    |
|    value_loss         | 0.00645  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0779   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.34e+03 |
|    ep_rew_mean        | 2.03e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25000    |
|    time_elapsed       | 2957     |
|    total_timesteps    | 3200000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 24999    |
|    policy_loss        | 0.0914   |
|    std                | 0.792    |
|    value_loss         | 0.00222  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25100    |
|    time_elapsed       | 2969     |
|    total_timesteps    | 3212800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 25099    |
|    policy_loss        | -0.195   |
|    std                | 0.792    |
|    value_loss         | 0.00314  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.31e+03 |
|    ep_rew_mean        | 2.01e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25200    |
|    time_elapsed       | 2981     |
|    total_timesteps    | 3225600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.222    |
|    learning_rate      | 0.001    |
|    n_updates          | 25199    |
|    policy_loss        | -0.661   |
|    std                | 0.791    |
|    value_loss         | 164      |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.39e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25300    |
|    time_elapsed       | 2993     |
|    total_timesteps    | 3238400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.991    |
|    learning_rate      | 0.001    |
|    n_updates          | 25299    |
|    policy_loss        | -0.886   |
|    std                | 0.793    |
|    value_loss         | 0.0103   |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25400    |
|    time_elapsed       | 3005     |
|    total_timesteps    | 3251200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | -0.014   |
|    learning_rate      | 0.001    |
|    n_updates          | 25399    |
|    policy_loss        | 0.114    |
|    std                | 0.795    |
|    value_loss         | 95.5     |
------------------------------------
Num timesteps: 3264000
Best mean reward: 2260.12 - Last mean reward per episode: 2045.25
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25500    |
|    time_elapsed       | 3017     |
|    total_timesteps    | 3264000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 25499    |
|    policy_loss        | -0.0693  |
|    std                | 0.795    |
|    value_loss         | 0.00928  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25600    |
|    time_elapsed       | 3028     |
|    total_timesteps    | 3276800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 25599    |
|    policy_loss        | 0.591    |
|    std                | 0.799    |
|    value_loss         | 0.00248  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0778   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25700    |
|    time_elapsed       | 3040     |
|    total_timesteps    | 3289600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.072    |
|    learning_rate      | 0.001    |
|    n_updates          | 25699    |
|    policy_loss        | 0.209    |
|    std                | 0.799    |
|    value_loss         | 106      |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0777   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25800    |
|    time_elapsed       | 3052     |
|    total_timesteps    | 3302400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.0495   |
|    learning_rate      | 0.001    |
|    n_updates          | 25799    |
|    policy_loss        | 0.219    |
|    std                | 0.8      |
|    value_loss         | 29.5     |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 25900    |
|    time_elapsed       | 3064     |
|    total_timesteps    | 3315200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 25899    |
|    policy_loss        | -0.366   |
|    std                | 0.798    |
|    value_loss         | 0.00523  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0794   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26000    |
|    time_elapsed       | 3076     |
|    total_timesteps    | 3328000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 25999    |
|    policy_loss        | 0.336    |
|    std                | 0.798    |
|    value_loss         | 0.00532  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26100    |
|    time_elapsed       | 3088     |
|    total_timesteps    | 3340800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 26099    |
|    policy_loss        | -0.244   |
|    std                | 0.797    |
|    value_loss         | 0.00602  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.47e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26200    |
|    time_elapsed       | 3099     |
|    total_timesteps    | 3353600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 26199    |
|    policy_loss        | 0.137    |
|    std                | 0.802    |
|    value_loss         | 0.00343  |
------------------------------------
Num timesteps: 3360000
Best mean reward: 2260.12 - Last mean reward per episode: 2100.83
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.078    |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.47e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26300    |
|    time_elapsed       | 3111     |
|    total_timesteps    | 3366400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 26299    |
|    policy_loss        | -0.102   |
|    std                | 0.801    |
|    value_loss         | 0.00207  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.53e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26400    |
|    time_elapsed       | 3123     |
|    total_timesteps    | 3379200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.953    |
|    learning_rate      | 0.001    |
|    n_updates          | 26399    |
|    policy_loss        | -0.156   |
|    std                | 0.802    |
|    value_loss         | 0.00188  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0946   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0946   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.52e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26500    |
|    time_elapsed       | 3135     |
|    total_timesteps    | 3392000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 26499    |
|    policy_loss        | 0.0917   |
|    std                | 0.804    |
|    value_loss         | 0.00542  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.58e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26600    |
|    time_elapsed       | 3147     |
|    total_timesteps    | 3404800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.952    |
|    learning_rate      | 0.001    |
|    n_updates          | 26599    |
|    policy_loss        | -0.0773  |
|    std                | 0.805    |
|    value_loss         | 0.00513  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26700    |
|    time_elapsed       | 3158     |
|    total_timesteps    | 3417600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 26699    |
|    policy_loss        | -0.371   |
|    std                | 0.804    |
|    value_loss         | 0.00399  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.16e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26800    |
|    time_elapsed       | 3170     |
|    total_timesteps    | 3430400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 26799    |
|    policy_loss        | 0.428    |
|    std                | 0.802    |
|    value_loss         | 0.00286  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0784   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 26900    |
|    time_elapsed       | 3182     |
|    total_timesteps    | 3443200  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.948    |
|    learning_rate      | 0.001    |
|    n_updates          | 26899    |
|    policy_loss        | 0.163    |
|    std                | 0.8      |
|    value_loss         | 0.0849   |
------------------------------------
Num timesteps: 3456000
Best mean reward: 2260.12 - Last mean reward per episode: 2126.01
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0786   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.5e+03  |
|    ep_rew_mean        | 2.13e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27000    |
|    time_elapsed       | 3194     |
|    total_timesteps    | 3456000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.985    |
|    learning_rate      | 0.001    |
|    n_updates          | 26999    |
|    policy_loss        | -0.202   |
|    std                | 0.8      |
|    value_loss         | 0.00292  |
------------------------------------
------------------------------------
| reward                | 0.597    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27100    |
|    time_elapsed       | 3206     |
|    total_timesteps    | 3468800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.981    |
|    learning_rate      | 0.001    |
|    n_updates          | 27099    |
|    policy_loss        | -0.485   |
|    std                | 0.801    |
|    value_loss         | 0.00476  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.079    |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27200    |
|    time_elapsed       | 3218     |
|    total_timesteps    | 3481600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.985    |
|    learning_rate      | 0.001    |
|    n_updates          | 27199    |
|    policy_loss        | 0.856    |
|    std                | 0.802    |
|    value_loss         | 0.00779  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27300    |
|    time_elapsed       | 3230     |
|    total_timesteps    | 3494400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 27299    |
|    policy_loss        | -0.14    |
|    std                | 0.799    |
|    value_loss         | 0.00508  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.54e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27400    |
|    time_elapsed       | 3241     |
|    total_timesteps    | 3507200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 27399    |
|    policy_loss        | -0.454   |
|    std                | 0.795    |
|    value_loss         | 0.00513  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0803   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.63e+03 |
|    ep_rew_mean        | 2.21e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27500    |
|    time_elapsed       | 3253     |
|    total_timesteps    | 3520000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 27499    |
|    policy_loss        | 0.424    |
|    std                | 0.793    |
|    value_loss         | 0.00508  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.08     |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.16e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27600    |
|    time_elapsed       | 3265     |
|    total_timesteps    | 3532800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 27599    |
|    policy_loss        | 0.0868   |
|    std                | 0.797    |
|    value_loss         | 0.00348  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0798   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27700    |
|    time_elapsed       | 3277     |
|    total_timesteps    | 3545600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 27699    |
|    policy_loss        | -0.149   |
|    std                | 0.797    |
|    value_loss         | 0.00422  |
------------------------------------
Num timesteps: 3552000
Best mean reward: 2260.12 - Last mean reward per episode: 2154.24
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.53e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27800    |
|    time_elapsed       | 3289     |
|    total_timesteps    | 3558400  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 27799    |
|    policy_loss        | 0.025    |
|    std                | 0.799    |
|    value_loss         | 0.0129   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0804   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 27900    |
|    time_elapsed       | 3301     |
|    total_timesteps    | 3571200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 27899    |
|    policy_loss        | -0.662   |
|    std                | 0.797    |
|    value_loss         | 0.00884  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.08     |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28000    |
|    time_elapsed       | 3312     |
|    total_timesteps    | 3584000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 27999    |
|    policy_loss        | 0.109    |
|    std                | 0.797    |
|    value_loss         | 0.00526  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0952   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0952   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28100    |
|    time_elapsed       | 3324     |
|    total_timesteps    | 3596800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | -0.00566 |
|    learning_rate      | 0.001    |
|    n_updates          | 28099    |
|    policy_loss        | -0.319   |
|    std                | 0.8      |
|    value_loss         | 83       |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.35e+03 |
|    ep_rew_mean        | 2.03e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28200    |
|    time_elapsed       | 3336     |
|    total_timesteps    | 3609600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 28199    |
|    policy_loss        | -0.223   |
|    std                | 0.798    |
|    value_loss         | 0.00289  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0946   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0946   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28300    |
|    time_elapsed       | 3348     |
|    total_timesteps    | 3622400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 28299    |
|    policy_loss        | 0.678    |
|    std                | 0.797    |
|    value_loss         | 0.00959  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.33e+03 |
|    ep_rew_mean        | 2.02e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28400    |
|    time_elapsed       | 3360     |
|    total_timesteps    | 3635200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.924    |
|    learning_rate      | 0.001    |
|    n_updates          | 28399    |
|    policy_loss        | 0.121    |
|    std                | 0.796    |
|    value_loss         | 0.00588  |
------------------------------------
Num timesteps: 3648000
Best mean reward: 2260.12 - Last mean reward per episode: 2067.29
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28500    |
|    time_elapsed       | 3372     |
|    total_timesteps    | 3648000  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 28499    |
|    policy_loss        | 1.35     |
|    std                | 0.8      |
|    value_loss         | 0.00587  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.31e+03 |
|    ep_rew_mean        | 2.01e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28600    |
|    time_elapsed       | 3384     |
|    total_timesteps    | 3660800  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.978    |
|    learning_rate      | 0.001    |
|    n_updates          | 28599    |
|    policy_loss        | 0.0723   |
|    std                | 0.802    |
|    value_loss         | 0.00655  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28700    |
|    time_elapsed       | 3395     |
|    total_timesteps    | 3673600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 28699    |
|    policy_loss        | 0.172    |
|    std                | 0.803    |
|    value_loss         | 0.00345  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0796   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28800    |
|    time_elapsed       | 3407     |
|    total_timesteps    | 3686400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 28799    |
|    policy_loss        | 0.246    |
|    std                | 0.797    |
|    value_loss         | 0.00559  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0799   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.35e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 28900    |
|    time_elapsed       | 3419     |
|    total_timesteps    | 3699200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 28899    |
|    policy_loss        | 0.606    |
|    std                | 0.797    |
|    value_loss         | 0.0305   |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.0792   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.28e+03 |
|    ep_rew_mean        | 1.99e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29000    |
|    time_elapsed       | 3431     |
|    total_timesteps    | 3712000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 28999    |
|    policy_loss        | -0.136   |
|    std                | 0.796    |
|    value_loss         | 0.00785  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.28e+03 |
|    ep_rew_mean        | 1.99e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29100    |
|    time_elapsed       | 3443     |
|    total_timesteps    | 3724800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 29099    |
|    policy_loss        | -0.219   |
|    std                | 0.795    |
|    value_loss         | 0.00355  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.26e+03 |
|    ep_rew_mean        | 1.98e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29200    |
|    time_elapsed       | 3454     |
|    total_timesteps    | 3737600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 29199    |
|    policy_loss        | 1.06     |
|    std                | 0.796    |
|    value_loss         | 0.00411  |
------------------------------------
Num timesteps: 3744000
Best mean reward: 2260.12 - Last mean reward per episode: 1978.68
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.28e+03 |
|    ep_rew_mean        | 1.99e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29300    |
|    time_elapsed       | 3466     |
|    total_timesteps    | 3750400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 29299    |
|    policy_loss        | 0.0072   |
|    std                | 0.796    |
|    value_loss         | 0.00479  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0936   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0936   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.21e+03 |
|    ep_rew_mean        | 1.95e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29400    |
|    time_elapsed       | 3478     |
|    total_timesteps    | 3763200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.987    |
|    learning_rate      | 0.001    |
|    n_updates          | 29399    |
|    policy_loss        | 0.0842   |
|    std                | 0.794    |
|    value_loss         | 0.00285  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0935   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0935   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.21e+03 |
|    ep_rew_mean        | 1.94e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29500    |
|    time_elapsed       | 3490     |
|    total_timesteps    | 3776000  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 29499    |
|    policy_loss        | -0.107   |
|    std                | 0.789    |
|    value_loss         | 0.00551  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0938   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0938   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.29e+03 |
|    ep_rew_mean        | 1.99e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29600    |
|    time_elapsed       | 3502     |
|    total_timesteps    | 3788800  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 29599    |
|    policy_loss        | -0.566   |
|    std                | 0.784    |
|    value_loss         | 0.00455  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29700    |
|    time_elapsed       | 3514     |
|    total_timesteps    | 3801600  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 29699    |
|    policy_loss        | 0.0478   |
|    std                | 0.788    |
|    value_loss         | 0.00485  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29800    |
|    time_elapsed       | 3526     |
|    total_timesteps    | 3814400  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 29799    |
|    policy_loss        | 0.0606   |
|    std                | 0.789    |
|    value_loss         | 0.00501  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0781   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 29900    |
|    time_elapsed       | 3538     |
|    total_timesteps    | 3827200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 29899    |
|    policy_loss        | -0.355   |
|    std                | 0.791    |
|    value_loss         | 0.0112   |
------------------------------------
Num timesteps: 3840000
Best mean reward: 2260.12 - Last mean reward per episode: 2038.04
------------------------------------
| reward                | 0.597    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0773   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30000    |
|    time_elapsed       | 3549     |
|    total_timesteps    | 3840000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 29999    |
|    policy_loss        | 0.655    |
|    std                | 0.792    |
|    value_loss         | 0.00423  |
------------------------------------
------------------------------------
| reward                | 0.596    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0764   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.32e+03 |
|    ep_rew_mean        | 2.01e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30100    |
|    time_elapsed       | 3561     |
|    total_timesteps    | 3852800  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 30099    |
|    policy_loss        | 0.0815   |
|    std                | 0.788    |
|    value_loss         | 0.0133   |
------------------------------------
------------------------------------
| reward                | 0.594    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.108    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0757   |
| reward_velocity       | 0.0974   |
| rollout/              |          |
|    ep_len_mean        | 3.31e+03 |
|    ep_rew_mean        | 2e+03    |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30200    |
|    time_elapsed       | 3573     |
|    total_timesteps    | 3865600  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 30199    |
|    policy_loss        | -0.0545  |
|    std                | 0.784    |
|    value_loss         | 0.00344  |
------------------------------------
------------------------------------
| reward                | 0.594    |
| reward_ang_vel        | 0.094    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.108    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.094    |
| reward_position       | 0.0764   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.28e+03 |
|    ep_rew_mean        | 1.99e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30300    |
|    time_elapsed       | 3585     |
|    total_timesteps    | 3878400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.979    |
|    learning_rate      | 0.001    |
|    n_updates          | 30299    |
|    policy_loss        | 0.781    |
|    std                | 0.784    |
|    value_loss         | 0.00582  |
------------------------------------
------------------------------------
| reward                | 0.595    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.108    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0771   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30400    |
|    time_elapsed       | 3597     |
|    total_timesteps    | 3891200  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.954    |
|    learning_rate      | 0.001    |
|    n_updates          | 30399    |
|    policy_loss        | -0.119   |
|    std                | 0.783    |
|    value_loss         | 0.0136   |
------------------------------------
------------------------------------
| reward                | 0.593    |
| reward_ang_vel        | 0.0936   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.108    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0936   |
| reward_position       | 0.0769   |
| reward_velocity       | 0.0973   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30500    |
|    time_elapsed       | 3609     |
|    total_timesteps    | 3904000  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 30499    |
|    policy_loss        | 0.114    |
|    std                | 0.786    |
|    value_loss         | 0.00929  |
------------------------------------
------------------------------------
| reward                | 0.593    |
| reward_ang_vel        | 0.0936   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.108    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0936   |
| reward_position       | 0.0766   |
| reward_velocity       | 0.0973   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30600    |
|    time_elapsed       | 3621     |
|    total_timesteps    | 3916800  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 30599    |
|    policy_loss        | 0.103    |
|    std                | 0.784    |
|    value_loss         | 0.00832  |
------------------------------------
------------------------------------
| reward                | 0.592    |
| reward_ang_vel        | 0.0935   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.107    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0935   |
| reward_position       | 0.0763   |
| reward_velocity       | 0.0973   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30700    |
|    time_elapsed       | 3632     |
|    total_timesteps    | 3929600  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 30699    |
|    policy_loss        | -0.16    |
|    std                | 0.782    |
|    value_loss         | 0.00496  |
------------------------------------
Num timesteps: 3936000
Best mean reward: 2260.12 - Last mean reward per episode: 2061.34
------------------------------------
| reward                | 0.593    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.108    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0775   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30800    |
|    time_elapsed       | 3644     |
|    total_timesteps    | 3942400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 30799    |
|    policy_loss        | 0.611    |
|    std                | 0.783    |
|    value_loss         | 0.00389  |
------------------------------------
------------------------------------
| reward                | 0.593    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.108    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0775   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 30900    |
|    time_elapsed       | 3656     |
|    total_timesteps    | 3955200  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 30899    |
|    policy_loss        | -0.505   |
|    std                | 0.787    |
|    value_loss         | 0.00712  |
------------------------------------
------------------------------------
| reward                | 0.593    |
| reward_ang_vel        | 0.0936   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.107    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0936   |
| reward_position       | 0.0777   |
| reward_velocity       | 0.0975   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31000    |
|    time_elapsed       | 3668     |
|    total_timesteps    | 3968000  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 30999    |
|    policy_loss        | -0.0114  |
|    std                | 0.788    |
|    value_loss         | 0.00775  |
------------------------------------
------------------------------------
| reward                | 0.594    |
| reward_ang_vel        | 0.0934   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.107    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0934   |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31100    |
|    time_elapsed       | 3680     |
|    total_timesteps    | 3980800  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 31099    |
|    policy_loss        | -0.215   |
|    std                | 0.788    |
|    value_loss         | 0.00987  |
------------------------------------
------------------------------------
| reward                | 0.595    |
| reward_ang_vel        | 0.0935   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.108    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0935   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31200    |
|    time_elapsed       | 3691     |
|    total_timesteps    | 3993600  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 31199    |
|    policy_loss        | 0.141    |
|    std                | 0.783    |
|    value_loss         | 0.00335  |
------------------------------------
------------------------------------
| reward                | 0.595    |
| reward_ang_vel        | 0.0933   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.107    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0933   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.51e+03 |
|    ep_rew_mean        | 2.13e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31300    |
|    time_elapsed       | 3703     |
|    total_timesteps    | 4006400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 31299    |
|    policy_loss        | -0.395   |
|    std                | 0.781    |
|    value_loss         | 0.012    |
------------------------------------
------------------------------------
| reward                | 0.594    |
| reward_ang_vel        | 0.0931   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.107    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0931   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31400    |
|    time_elapsed       | 3715     |
|    total_timesteps    | 4019200  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 31399    |
|    policy_loss        | -0.00969 |
|    std                | 0.783    |
|    value_loss         | 0.00462  |
------------------------------------
Num timesteps: 4032000
Best mean reward: 2260.12 - Last mean reward per episode: 2121.92
------------------------------------
| reward                | 0.594    |
| reward_ang_vel        | 0.0932   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.107    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0932   |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.49e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31500    |
|    time_elapsed       | 3727     |
|    total_timesteps    | 4032000  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 31499    |
|    policy_loss        | 0.191    |
|    std                | 0.787    |
|    value_loss         | 0.00434  |
------------------------------------
------------------------------------
| reward                | 0.593    |
| reward_ang_vel        | 0.0923   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.107    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0923   |
| reward_position       | 0.0786   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.58e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31600    |
|    time_elapsed       | 3739     |
|    total_timesteps    | 4044800  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 31599    |
|    policy_loss        | -0.292   |
|    std                | 0.79     |
|    value_loss         | 0.00731  |
------------------------------------
------------------------------------
| reward                | 0.593    |
| reward_ang_vel        | 0.0926   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.106    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0926   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.6e+03  |
|    ep_rew_mean        | 2.19e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31700    |
|    time_elapsed       | 3751     |
|    total_timesteps    | 4057600  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 31699    |
|    policy_loss        | -0.0909  |
|    std                | 0.792    |
|    value_loss         | 0.00737  |
------------------------------------
------------------------------------
| reward                | 0.592    |
| reward_ang_vel        | 0.0924   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.106    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0924   |
| reward_position       | 0.0783   |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.55e+03 |
|    ep_rew_mean        | 2.15e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31800    |
|    time_elapsed       | 3762     |
|    total_timesteps    | 4070400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 31799    |
|    policy_loss        | -0.328   |
|    std                | 0.793    |
|    value_loss         | 0.0054   |
------------------------------------
------------------------------------
| reward                | 0.592    |
| reward_ang_vel        | 0.0924   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.107    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0924   |
| reward_position       | 0.078    |
| reward_velocity       | 0.0976   |
| rollout/              |          |
|    ep_len_mean        | 3.62e+03 |
|    ep_rew_mean        | 2.2e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 31900    |
|    time_elapsed       | 3774     |
|    total_timesteps    | 4083200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 31899    |
|    policy_loss        | -0.634   |
|    std                | 0.797    |
|    value_loss         | 0.00356  |
------------------------------------
------------------------------------
| reward                | 0.595    |
| reward_ang_vel        | 0.0923   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.108    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0923   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.0977   |
| rollout/              |          |
|    ep_len_mean        | 3.58e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32000    |
|    time_elapsed       | 3786     |
|    total_timesteps    | 4096000  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 31999    |
|    policy_loss        | -0.412   |
|    std                | 0.792    |
|    value_loss         | 0.00685  |
------------------------------------
------------------------------------
| reward                | 0.596    |
| reward_ang_vel        | 0.0929   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.109    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0929   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.63e+03 |
|    ep_rew_mean        | 2.21e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32100    |
|    time_elapsed       | 3798     |
|    total_timesteps    | 4108800  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 32099    |
|    policy_loss        | 0.469    |
|    std                | 0.794    |
|    value_loss         | 0.0045   |
------------------------------------
------------------------------------
| reward                | 0.597    |
| reward_ang_vel        | 0.093    |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.11     |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.093    |
| reward_position       | 0.0789   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.56e+03 |
|    ep_rew_mean        | 2.17e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32200    |
|    time_elapsed       | 3810     |
|    total_timesteps    | 4121600  |
| train/                |          |
|    entropy_loss       | -47.3    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 32199    |
|    policy_loss        | -0.174   |
|    std                | 0.798    |
|    value_loss         | 0.00414  |
------------------------------------
Num timesteps: 4128000
Best mean reward: 2260.12 - Last mean reward per episode: 2114.14
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0925   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0925   |
| reward_position       | 0.079    |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.46e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32300    |
|    time_elapsed       | 3822     |
|    total_timesteps    | 4134400  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.139    |
|    learning_rate      | 0.001    |
|    n_updates          | 32299    |
|    policy_loss        | -0.538   |
|    std                | 0.794    |
|    value_loss         | 106      |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0922   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0922   |
| reward_position       | 0.0797   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32400    |
|    time_elapsed       | 3834     |
|    total_timesteps    | 4147200  |
| train/                |          |
|    entropy_loss       | -47.2    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 32399    |
|    policy_loss        | -0.0797  |
|    std                | 0.793    |
|    value_loss         | 0.00243  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0923   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0923   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32500    |
|    time_elapsed       | 3845     |
|    total_timesteps    | 4160000  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 32499    |
|    policy_loss        | 0.136    |
|    std                | 0.789    |
|    value_loss         | 0.00524  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.093    |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.093    |
| reward_position       | 0.08     |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.42e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32600    |
|    time_elapsed       | 3857     |
|    total_timesteps    | 4172800  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 32599    |
|    policy_loss        | 0.812    |
|    std                | 0.783    |
|    value_loss         | 0.0117   |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0929   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0929   |
| reward_position       | 0.0805   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32700    |
|    time_elapsed       | 3869     |
|    total_timesteps    | 4185600  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.938    |
|    learning_rate      | 0.001    |
|    n_updates          | 32699    |
|    policy_loss        | -0.245   |
|    std                | 0.785    |
|    value_loss         | 0.00554  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0813   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32800    |
|    time_elapsed       | 3881     |
|    total_timesteps    | 4198400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 32799    |
|    policy_loss        | -0.389   |
|    std                | 0.782    |
|    value_loss         | 0.00815  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0926   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0926   |
| reward_position       | 0.0816   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 32900    |
|    time_elapsed       | 3893     |
|    total_timesteps    | 4211200  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | -0.0743  |
|    learning_rate      | 0.001    |
|    n_updates          | 32899    |
|    policy_loss        | -1.24    |
|    std                | 0.781    |
|    value_loss         | 94.5     |
------------------------------------
Num timesteps: 4224000
Best mean reward: 2260.12 - Last mean reward per episode: 2117.51
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0928   |
| reward_contact        | 0.0979   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0928   |
| reward_position       | 0.0813   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33000    |
|    time_elapsed       | 3905     |
|    total_timesteps    | 4224000  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 32999    |
|    policy_loss        | -0.183   |
|    std                | 0.781    |
|    value_loss         | 0.00588  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.0804   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.46e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33100    |
|    time_elapsed       | 3917     |
|    total_timesteps    | 4236800  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 33099    |
|    policy_loss        | -0.165   |
|    std                | 0.777    |
|    value_loss         | 0.00612  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0935   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0935   |
| reward_position       | 0.0797   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33200    |
|    time_elapsed       | 3928     |
|    total_timesteps    | 4249600  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 33199    |
|    policy_loss        | -0.00408 |
|    std                | 0.781    |
|    value_loss         | 0.0087   |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0932   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.111    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0932   |
| reward_position       | 0.0806   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.4e+03  |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33300    |
|    time_elapsed       | 3940     |
|    total_timesteps    | 4262400  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 33299    |
|    policy_loss        | 0.0019   |
|    std                | 0.784    |
|    value_loss         | 0.00255  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0932   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0932   |
| reward_position       | 0.0811   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33400    |
|    time_elapsed       | 3952     |
|    total_timesteps    | 4275200  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 33399    |
|    policy_loss        | -0.751   |
|    std                | 0.785    |
|    value_loss         | 0.00476  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0937   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0937   |
| reward_position       | 0.08     |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.47e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33500    |
|    time_elapsed       | 3964     |
|    total_timesteps    | 4288000  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.993    |
|    learning_rate      | 0.001    |
|    n_updates          | 33499    |
|    policy_loss        | -0.0944  |
|    std                | 0.78     |
|    value_loss         | 0.00273  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0939   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0939   |
| reward_position       | 0.0799   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.47e+03 |
|    ep_rew_mean        | 2.11e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33600    |
|    time_elapsed       | 3976     |
|    total_timesteps    | 4300800  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.98     |
|    learning_rate      | 0.001    |
|    n_updates          | 33599    |
|    policy_loss        | -0.219   |
|    std                | 0.779    |
|    value_loss         | 0.00477  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0802   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.08e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33700    |
|    time_elapsed       | 3988     |
|    total_timesteps    | 4313600  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.984    |
|    learning_rate      | 0.001    |
|    n_updates          | 33699    |
|    policy_loss        | 0.0447   |
|    std                | 0.782    |
|    value_loss         | 0.00723  |
------------------------------------
Num timesteps: 4320000
Best mean reward: 2260.12 - Last mean reward per episode: 2031.37
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0803   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.34e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33800    |
|    time_elapsed       | 4000     |
|    total_timesteps    | 4326400  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 33799    |
|    policy_loss        | -0.451   |
|    std                | 0.783    |
|    value_loss         | 0.00233  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.08     |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.22e+03 |
|    ep_rew_mean        | 1.96e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 33900    |
|    time_elapsed       | 4011     |
|    total_timesteps    | 4339200  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.983    |
|    learning_rate      | 0.001    |
|    n_updates          | 33899    |
|    policy_loss        | -0.542   |
|    std                | 0.785    |
|    value_loss         | 0.0131   |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0801   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.14e+03 |
|    ep_rew_mean        | 1.91e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34000    |
|    time_elapsed       | 4023     |
|    total_timesteps    | 4352000  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 33999    |
|    policy_loss        | -0.616   |
|    std                | 0.784    |
|    value_loss         | 0.00924  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0804   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.13e+03 |
|    ep_rew_mean        | 1.9e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34100    |
|    time_elapsed       | 4035     |
|    total_timesteps    | 4364800  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 34099    |
|    policy_loss        | -0.209   |
|    std                | 0.785    |
|    value_loss         | 0.00769  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0805   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.13e+03 |
|    ep_rew_mean        | 1.9e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34200    |
|    time_elapsed       | 4047     |
|    total_timesteps    | 4377600  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 34199    |
|    policy_loss        | 0.35     |
|    std                | 0.782    |
|    value_loss         | 0.0145   |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0946   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0946   |
| reward_position       | 0.0799   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.07e+03 |
|    ep_rew_mean        | 1.86e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34300    |
|    time_elapsed       | 4059     |
|    total_timesteps    | 4390400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 34299    |
|    policy_loss        | 0.0307   |
|    std                | 0.78     |
|    value_loss         | 0.00716  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.05e+03 |
|    ep_rew_mean        | 1.85e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34400    |
|    time_elapsed       | 4070     |
|    total_timesteps    | 4403200  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.995    |
|    learning_rate      | 0.001    |
|    n_updates          | 34399    |
|    policy_loss        | -0.0203  |
|    std                | 0.779    |
|    value_loss         | 0.00831  |
------------------------------------
Num timesteps: 4416000
Best mean reward: 2260.12 - Last mean reward per episode: 1842.84
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.079    |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.04e+03 |
|    ep_rew_mean        | 1.84e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34500    |
|    time_elapsed       | 4082     |
|    total_timesteps    | 4416000  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 1        |
|    learning_rate      | 0.001    |
|    n_updates          | 34499    |
|    policy_loss        | -0.0249  |
|    std                | 0.782    |
|    value_loss         | 0.00668  |
------------------------------------
------------------------------------
| reward                | 0.602    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0785   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.01e+03 |
|    ep_rew_mean        | 1.82e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34600    |
|    time_elapsed       | 4094     |
|    total_timesteps    | 4428800  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 34599    |
|    policy_loss        | -0.471   |
|    std                | 0.782    |
|    value_loss         | 0.00571  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0777   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.06e+03 |
|    ep_rew_mean        | 1.85e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34700    |
|    time_elapsed       | 4106     |
|    total_timesteps    | 4441600  |
| train/                |          |
|    entropy_loss       | -47.1    |
|    explained_variance | 0.991    |
|    learning_rate      | 0.001    |
|    n_updates          | 34699    |
|    policy_loss        | 0.0903   |
|    std                | 0.782    |
|    value_loss         | 0.00838  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0781   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.1e+03  |
|    ep_rew_mean        | 1.88e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34800    |
|    time_elapsed       | 4118     |
|    total_timesteps    | 4454400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 34799    |
|    policy_loss        | 0.599    |
|    std                | 0.778    |
|    value_loss         | 0.00514  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.08e+03 |
|    ep_rew_mean        | 1.87e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 34900    |
|    time_elapsed       | 4129     |
|    total_timesteps    | 4467200  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 34899    |
|    policy_loss        | 0.087    |
|    std                | 0.778    |
|    value_loss         | 0.00462  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0775   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.19e+03 |
|    ep_rew_mean        | 1.93e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35000    |
|    time_elapsed       | 4141     |
|    total_timesteps    | 4480000  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.976    |
|    learning_rate      | 0.001    |
|    n_updates          | 34999    |
|    policy_loss        | 0.187    |
|    std                | 0.775    |
|    value_loss         | 0.00294  |
------------------------------------
------------------------------------
| reward                | 0.598    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0775   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.27e+03 |
|    ep_rew_mean        | 1.98e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35100    |
|    time_elapsed       | 4153     |
|    total_timesteps    | 4492800  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.964    |
|    learning_rate      | 0.001    |
|    n_updates          | 35099    |
|    policy_loss        | -0.323   |
|    std                | 0.774    |
|    value_loss         | 0.00523  |
------------------------------------
------------------------------------
| reward                | 0.599    |
| reward_ang_vel        | 0.0952   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0952   |
| reward_position       | 0.0784   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.41e+03 |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35200    |
|    time_elapsed       | 4165     |
|    total_timesteps    | 4505600  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 35199    |
|    policy_loss        | 0.355    |
|    std                | 0.779    |
|    value_loss         | 0.00256  |
------------------------------------
Num timesteps: 4512000
Best mean reward: 2260.12 - Last mean reward per episode: 2100.76
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0952   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0952   |
| reward_position       | 0.0781   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35300    |
|    time_elapsed       | 4177     |
|    total_timesteps    | 4518400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 35299    |
|    policy_loss        | 0.327    |
|    std                | 0.778    |
|    value_loss         | 0.00154  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.078    |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.39e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35400    |
|    time_elapsed       | 4189     |
|    total_timesteps    | 4531200  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 35399    |
|    policy_loss        | -0.0398  |
|    std                | 0.777    |
|    value_loss         | 0.00204  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0779   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.39e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35500    |
|    time_elapsed       | 4201     |
|    total_timesteps    | 4544000  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 35499    |
|    policy_loss        | 0.445    |
|    std                | 0.777    |
|    value_loss         | 0.00182  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0782   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.37e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35600    |
|    time_elapsed       | 4212     |
|    total_timesteps    | 4556800  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.996    |
|    learning_rate      | 0.001    |
|    n_updates          | 35599    |
|    policy_loss        | -0.277   |
|    std                | 0.777    |
|    value_loss         | 0.00171  |
------------------------------------
------------------------------------
| reward                | 0.6      |
| reward_ang_vel        | 0.0946   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.112    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0946   |
| reward_position       | 0.0788   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.34e+03 |
|    ep_rew_mean        | 2.03e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35700    |
|    time_elapsed       | 4224     |
|    total_timesteps    | 4569600  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 35699    |
|    policy_loss        | 0.199    |
|    std                | 0.778    |
|    value_loss         | 0.00444  |
------------------------------------
------------------------------------
| reward                | 0.601    |
| reward_ang_vel        | 0.0945   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.113    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0945   |
| reward_position       | 0.0787   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.32e+03 |
|    ep_rew_mean        | 2.02e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35800    |
|    time_elapsed       | 4236     |
|    total_timesteps    | 4582400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 35799    |
|    policy_loss        | -0.453   |
|    std                | 0.777    |
|    value_loss         | 0.00214  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0792   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.35e+03 |
|    ep_rew_mean        | 2.03e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 35900    |
|    time_elapsed       | 4248     |
|    total_timesteps    | 4595200  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 35899    |
|    policy_loss        | 0.077    |
|    std                | 0.773    |
|    value_loss         | 0.00242  |
------------------------------------
Num timesteps: 4608000
Best mean reward: 2260.12 - Last mean reward per episode: 2087.47
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.114    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36000    |
|    time_elapsed       | 4260     |
|    total_timesteps    | 4608000  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 35999    |
|    policy_loss        | -0.282   |
|    std                | 0.772    |
|    value_loss         | 0.000656 |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.08     |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36100    |
|    time_elapsed       | 4272     |
|    total_timesteps    | 4620800  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 36099    |
|    policy_loss        | 0.157    |
|    std                | 0.774    |
|    value_loss         | 0.00707  |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.08     |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36200    |
|    time_elapsed       | 4283     |
|    total_timesteps    | 4633600  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 36199    |
|    policy_loss        | 0.348    |
|    std                | 0.777    |
|    value_loss         | 0.0047   |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0767   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0798   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.43e+03 |
|    ep_rew_mean        | 2.09e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36300    |
|    time_elapsed       | 4295     |
|    total_timesteps    | 4646400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 36299    |
|    policy_loss        | -0.153   |
|    std                | 0.777    |
|    value_loss         | 0.00546  |
------------------------------------
------------------------------------
| reward                | 0.603    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.079    |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.33e+03 |
|    ep_rew_mean        | 2.02e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36400    |
|    time_elapsed       | 4307     |
|    total_timesteps    | 4659200  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.0137   |
|    learning_rate      | 0.001    |
|    n_updates          | 36399    |
|    policy_loss        | 0.325    |
|    std                | 0.773    |
|    value_loss         | 70.6     |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.095    |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.095    |
| reward_position       | 0.0791   |
| reward_velocity       | 0.0978   |
| rollout/              |          |
|    ep_len_mean        | 3.34e+03 |
|    ep_rew_mean        | 2.03e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36500    |
|    time_elapsed       | 4319     |
|    total_timesteps    | 4672000  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 36499    |
|    policy_loss        | -0.55    |
|    std                | 0.775    |
|    value_loss         | 0.00733  |
------------------------------------
------------------------------------
| reward                | 0.604    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36600    |
|    time_elapsed       | 4331     |
|    total_timesteps    | 4684800  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.991    |
|    learning_rate      | 0.001    |
|    n_updates          | 36599    |
|    policy_loss        | 0.0369   |
|    std                | 0.777    |
|    value_loss         | 0.00483  |
------------------------------------
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0793   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36700    |
|    time_elapsed       | 4343     |
|    total_timesteps    | 4697600  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | -0.0131  |
|    learning_rate      | 0.001    |
|    n_updates          | 36699    |
|    policy_loss        | 0.455    |
|    std                | 0.773    |
|    value_loss         | 114      |
------------------------------------
Num timesteps: 4704000
Best mean reward: 2260.12 - Last mean reward per episode: 2102.22
------------------------------------
| reward                | 0.605    |
| reward_ang_vel        | 0.0951   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0951   |
| reward_position       | 0.0795   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.45e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36800    |
|    time_elapsed       | 4354     |
|    total_timesteps    | 4710400  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 36799    |
|    policy_loss        | 0.0149   |
|    std                | 0.772    |
|    value_loss         | 0.00272  |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0951   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0951   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.44e+03 |
|    ep_rew_mean        | 2.1e+03  |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 36900    |
|    time_elapsed       | 4366     |
|    total_timesteps    | 4723200  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.989    |
|    learning_rate      | 0.001    |
|    n_updates          | 36899    |
|    policy_loss        | 0.277    |
|    std                | 0.771    |
|    value_loss         | 0.00245  |
------------------------------------
------------------------------------
| reward                | 0.606    |
| reward_ang_vel        | 0.0951   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0951   |
| reward_position       | 0.0796   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37000    |
|    time_elapsed       | 4378     |
|    total_timesteps    | 4736000  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 36999    |
|    policy_loss        | 0.139    |
|    std                | 0.771    |
|    value_loss         | 0.00564  |
------------------------------------
------------------------------------
| reward                | 0.607    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0797   |
| reward_velocity       | 0.0979   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37100    |
|    time_elapsed       | 4390     |
|    total_timesteps    | 4748800  |
| train/                |          |
|    entropy_loss       | -46.8    |
|    explained_variance | 0.992    |
|    learning_rate      | 0.001    |
|    n_updates          | 37099    |
|    policy_loss        | 0.166    |
|    std                | 0.769    |
|    value_loss         | 0.00547  |
------------------------------------
------------------------------------
| reward                | 0.609    |
| reward_ang_vel        | 0.0953   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0953   |
| reward_position       | 0.0817   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.66e+03 |
|    ep_rew_mean        | 2.23e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37200    |
|    time_elapsed       | 4402     |
|    total_timesteps    | 4761600  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 37199    |
|    policy_loss        | 0.21     |
|    std                | 0.772    |
|    value_loss         | 0.00239  |
------------------------------------
------------------------------------
| reward                | 0.609    |
| reward_ang_vel        | 0.0952   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0952   |
| reward_position       | 0.0818   |
| reward_velocity       | 0.0982   |
| rollout/              |          |
|    ep_len_mean        | 3.68e+03 |
|    ep_rew_mean        | 2.25e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37300    |
|    time_elapsed       | 4414     |
|    total_timesteps    | 4774400  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 37299    |
|    policy_loss        | 0.24     |
|    std                | 0.771    |
|    value_loss         | 0.0041   |
------------------------------------
------------------------------------
| reward                | 0.609    |
| reward_ang_vel        | 0.0951   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0951   |
| reward_position       | 0.0822   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.74e+03 |
|    ep_rew_mean        | 2.28e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37400    |
|    time_elapsed       | 4425     |
|    total_timesteps    | 4787200  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 37399    |
|    policy_loss        | 0.224    |
|    std                | 0.77     |
|    value_loss         | 0.0027   |
------------------------------------
Num timesteps: 4800000
Best mean reward: 2260.12 - Last mean reward per episode: 2235.01
------------------------------------
| reward                | 0.611    |
| reward_ang_vel        | 0.0956   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0956   |
| reward_position       | 0.0828   |
| reward_velocity       | 0.0984   |
| rollout/              |          |
|    ep_len_mean        | 3.66e+03 |
|    ep_rew_mean        | 2.24e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37500    |
|    time_elapsed       | 4437     |
|    total_timesteps    | 4800000  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 37499    |
|    policy_loss        | 0.267    |
|    std                | 0.772    |
|    value_loss         | 0.00534  |
------------------------------------
------------------------------------
| reward                | 0.611    |
| reward_ang_vel        | 0.0954   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0954   |
| reward_position       | 0.0826   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.62e+03 |
|    ep_rew_mean        | 2.21e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37600    |
|    time_elapsed       | 4449     |
|    total_timesteps    | 4812800  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 37599    |
|    policy_loss        | 0.0396   |
|    std                | 0.778    |
|    value_loss         | 0.00661  |
------------------------------------
------------------------------------
| reward                | 0.611    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0828   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.57e+03 |
|    ep_rew_mean        | 2.18e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37700    |
|    time_elapsed       | 4461     |
|    total_timesteps    | 4825600  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 37699    |
|    policy_loss        | 0.0273   |
|    std                | 0.779    |
|    value_loss         | 0.00318  |
------------------------------------
------------------------------------
| reward                | 0.61     |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0827   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.51e+03 |
|    ep_rew_mean        | 2.14e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37800    |
|    time_elapsed       | 4473     |
|    total_timesteps    | 4838400  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.999    |
|    learning_rate      | 0.001    |
|    n_updates          | 37799    |
|    policy_loss        | -0.414   |
|    std                | 0.778    |
|    value_loss         | 0.00368  |
------------------------------------
------------------------------------
| reward                | 0.61     |
| reward_ang_vel        | 0.0944   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0944   |
| reward_position       | 0.0823   |
| reward_velocity       | 0.0983   |
| rollout/              |          |
|    ep_len_mean        | 3.39e+03 |
|    ep_rew_mean        | 2.07e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 37900    |
|    time_elapsed       | 4485     |
|    total_timesteps    | 4851200  |
| train/                |          |
|    entropy_loss       | -47      |
|    explained_variance | 0.0256   |
|    learning_rate      | 0.001    |
|    n_updates          | 37899    |
|    policy_loss        | 0.685    |
|    std                | 0.776    |
|    value_loss         | 68.4     |
------------------------------------
------------------------------------
| reward                | 0.609    |
| reward_ang_vel        | 0.0942   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0942   |
| reward_position       | 0.0819   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 38000    |
|    time_elapsed       | 4497     |
|    total_timesteps    | 4864000  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.988    |
|    learning_rate      | 0.001    |
|    n_updates          | 37999    |
|    policy_loss        | 0.272    |
|    std                | 0.776    |
|    value_loss         | 0.00356  |
------------------------------------
------------------------------------
| reward                | 0.608    |
| reward_ang_vel        | 0.0941   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0941   |
| reward_position       | 0.0818   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.36e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 38100    |
|    time_elapsed       | 4508     |
|    total_timesteps    | 4876800  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.982    |
|    learning_rate      | 0.001    |
|    n_updates          | 38099    |
|    policy_loss        | 0.254    |
|    std                | 0.773    |
|    value_loss         | 0.0078   |
------------------------------------
------------------------------------
| reward                | 0.608    |
| reward_ang_vel        | 0.0943   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.115    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0943   |
| reward_position       | 0.0816   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.35e+03 |
|    ep_rew_mean        | 2.04e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 38200    |
|    time_elapsed       | 4520     |
|    total_timesteps    | 4889600  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 38199    |
|    policy_loss        | -0.634   |
|    std                | 0.77     |
|    value_loss         | 0.00343  |
------------------------------------
Num timesteps: 4896000
Best mean reward: 2260.12 - Last mean reward per episode: 2059.49
------------------------------------
| reward                | 0.609    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.117    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0812   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 38300    |
|    time_elapsed       | 4532     |
|    total_timesteps    | 4902400  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 38299    |
|    policy_loss        | -0.732   |
|    std                | 0.768    |
|    value_loss         | 0.00436  |
------------------------------------
------------------------------------
| reward                | 0.608    |
| reward_ang_vel        | 0.0949   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0949   |
| reward_position       | 0.0805   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.05e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 38400    |
|    time_elapsed       | 4544     |
|    total_timesteps    | 4915200  |
| train/                |          |
|    entropy_loss       | -46.8    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 38399    |
|    policy_loss        | -0.0544  |
|    std                | 0.767    |
|    value_loss         | 0.00499  |
------------------------------------
------------------------------------
| reward                | 0.607    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0804   |
| reward_velocity       | 0.098    |
| rollout/              |          |
|    ep_len_mean        | 3.38e+03 |
|    ep_rew_mean        | 2.06e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 38500    |
|    time_elapsed       | 4556     |
|    total_timesteps    | 4928000  |
| train/                |          |
|    entropy_loss       | -46.9    |
|    explained_variance | 0.994    |
|    learning_rate      | 0.001    |
|    n_updates          | 38499    |
|    policy_loss        | -0.333   |
|    std                | 0.768    |
|    value_loss         | 0.00527  |
------------------------------------
------------------------------------
| reward                | 0.608    |
| reward_ang_vel        | 0.0947   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0947   |
| reward_position       | 0.0811   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.48e+03 |
|    ep_rew_mean        | 2.12e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 38600    |
|    time_elapsed       | 4568     |
|    total_timesteps    | 4940800  |
| train/                |          |
|    entropy_loss       | -46.8    |
|    explained_variance | 0.997    |
|    learning_rate      | 0.001    |
|    n_updates          | 38599    |
|    policy_loss        | -0.0881  |
|    std                | 0.765    |
|    value_loss         | 0.00485  |
------------------------------------
------------------------------------
| reward                | 0.607    |
| reward_ang_vel        | 0.0948   |
| reward_contact        | 0.0984   |
| reward_ctrl           | 0.116    |
| reward_energy         | 0.0765   |
| reward_orientation    | 0.0948   |
| reward_position       | 0.0804   |
| reward_velocity       | 0.0981   |
| rollout/              |          |
|    ep_len_mean        | 3.55e+03 |
|    ep_rew_mean        | 2.16e+03 |
| time/                 |          |
|    fps                | 1081     |
|    iterations         | 38700    |
|    time_elapsed       | 4579     |
|    total_timesteps    | 4953600  |
| train/                |          |
|    entropy_loss       | -46.8    |
|    explained_variance | 0.998    |
|    learning_rate      | 0.001    |
|    n_updates          | 38699    |
|    policy_loss        | 0.181    |
|    std                | 0.763    |
|    value_loss         | 0.0101   |
------------------------------------
